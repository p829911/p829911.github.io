{"meta":{"title":"data science study blog","subtitle":"Data Science Study Blog","description":null,"author":"Seungwoo Hyun","url":"https://p829911.github.io"},"pages":[],"posts":[{"title":"K-means","slug":"K-means","date":"2019-01-10T11:31:21.000Z","updated":"2019-01-10T11:34:35.975Z","comments":true,"path":"2019/01/10/K-means/","link":"","permalink":"https://p829911.github.io/2019/01/10/K-means/","excerpt":"","text":"K-MeansK-Means 클러스터링 알고리즘은 가장 단순하고 빠른 클러스터링 알고리즘의 하나이다. 다음과 같은 목적함수 값이 최소화될 때까지 클러스터의 중심(centroid)의 위치와 각 데이터가 소속될 클러스터를 반복해서 찾는다. 이 값을 inertia라고도 한다. J = \\sum_{k=1}^K\\sum_{i \\in C_k} d(x_i, u_k)이 식에서 $K$ 는 클러스터의 갯수이고 $C_k$ 는 $k$ 번째 클러스터에 속하는 데이터의 집합, $u_k$ 는 $k$ 번째 클러스터의 중심위치, $d$ 는 $x_i, u_k$ 두 데이터 사이의 거리(distance) 혹은 비유사도(dissimilarity)로 정의한다. 만약 유클리드 거리를 사용한다면 다음과 같다. d(x_i, u_k) = || x_i - u_k || ^ 2세부 알고리즘은 다음과 같다. 임의의 중심값 $u_k$ 를 고른다. 보통 데이터 샘플 중에서 $K$개를 선택한다. 중심에서 각 데이터까지의 거리를 계산 각 데이터에서 가장 가까운 중심을 선택하여 클러스터 갱신 다시 만들어진 클러스터에 대해 중심을 다시 계산하고 1~4를 반복한다. scikit-learn의 cluster 서브패키지는 Means 클러스터링을 위한 KMeans 클래스를 제공한다. 다음과 같은 인수를 받을 수 있다. n_clusters: 클러스터의 갯수 init: 초기화 방법. &quot;random&quot; 이면 무작위, &quot;K-means++&quot; 이면 K-means++ 방법. 또는 각 데이터의 클러스터 라벨 n_init: 초기 중심값 시도 횟수. 디폴트는 10이고 10개의 무작위 중심값 목록 중 가장 좋은 값을 선택한다. max_iter: 최대 반복 횟수. random_state: 시드값. 다음은 make_blobs 명령으로 만든 데이터를 2개로 K-means 클러스터링하는 과정을 나타낸 것이다. 마커(marker)의 모양은 클러스터를 나타내고 크기가 큰 마커가 중심값 위치이다. 각 단계에서 중심값은 전단계의 클러스터의 평균으로 다시 계산된다. 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeansX, _ = make_blobs(n_samples=20, random_state=4)def plot_KMeans(n): model = KMeans(n_clusters=2, init=\"random\", n_init=1, max_iter=n, random_state=8).fit(X) c0, c1 = model.cluster_centers_ plt.scatter(X[model.labels_ == 0, 0], X[model.labels_ == 0, 1], marker='v', facecolor='r', edgecolors='k') plt.scatter(X[model.labels_ == 1, 0], X[model.labels_ == 1, 1], marker='^', facecolor='y', edgecolors='k') plt.scatter(c0[0], c0[1], marker='v', c=\"r\", s=200) plt.scatter(c1[0], c1[1], marker='^', c=\"y\", s=200) plt.grid(False) plt.title(\"iteration=&#123;&#125;, score=&#123;:5.2f&#125;\".format(n, model.score(X)))plt.figure(figsize=(8, 8))plt.subplot(321)plot_KMeans(1)plt.subplot(322)plot_KMeans(2)plt.subplot(323)plot_KMeans(3)plt.subplot(324)plot_KMeans(4)plt.tight_layout()plt.show() K-Means++K-Means++ 알고리즘은 초기 중심값을 설정하기 위한 알고리즘이다. 다음과 같은 방법을 통해 되도록 멀리 떨어진 중심값 집합을 나타낸다. 중심값을 저장할 집합 $M$ 준비 일단 하나의 중심 $\\mu_0$ 을 랜덤하게 선택하여 $M$ 에 넣는다. $M$ 에 속하지 않는 모든 샘플 $x_i$ 에 대해 거리 $d(M,x_i)$ 를 계산. $d(M, x_i)$ 는 $M$ 안의 모든 샘플 $\\mu_k$ 에 대해 $d(u_k, x_i)$ 를 계산하여 가장 작은 값 선택 $d(M, x_i)$ 에 비례한 확률로 다음 중심 $\\mu$ 를 선택. $K$ 개의 중심을 선택할 때까지 반복 K-Means 알고리즘 사용 다음은 KMean 방법을 사용하여 MNIST Digit 이미지 데이터를 클러스터링한 결과이다. 각 클러스터에서 10개씩의 데이터만 표시하였다. 1234567891011121314151617181920212223242526272829from sklearn.datasets import load_digitsdigits = load_digits()model = KMeans(init=\"k-means++\", n_clusters=10, random_state=0)model.fit(digits.data)y_pred = model.labels_def show_digits(images, labels): f = plt.figure(figsize=(8, 2)) i = 0 while (i &lt; 10 and i &lt; images.shape[0]): ax = f.add_subplot(1, 10, i + 1) ax.imshow(images[i], cmap=plt.cm.bone) ax.grid(False) ax.set_title(labels[i]) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks([]) plt.tight_layout() i += 1 def show_cluster(images, y_pred, cluster_number): images = images[y_pred == cluster_number] y_pred = y_pred[y_pred == cluster_number] show_digits(images, y_pred) for i in range(10): show_cluster(digits.images, y_pred, i) 이미지의 제목에 있는 숫자는 클러스터 번호에 지나지 않으므로 원래 숫자의 번호와 일치하지 않는다. 하지만 이를 예측 문제라고 가정하고 분류 결과 행렬을 만들면 다음과 같다. 1234567891011121314from sklearn.metrics import confusion_matrixconfusion_matrix(digits.target, y_pred)array([[ 1, 0, 0, 0, 0, 177, 0, 0, 0, 0], [ 0, 1, 1, 0, 0, 0, 55, 99, 24, 2], [ 0, 13, 0, 2, 3, 1, 2, 8, 148, 0], [ 0, 154, 2, 13, 7, 0, 0, 7, 0, 0], [163, 0, 0, 0, 7, 0, 7, 4, 0, 0], [ 2, 0, 136, 43, 0, 0, 0, 0, 0, 1], [ 0, 0, 0, 0, 0, 1, 1, 2, 0, 177], [ 0, 0, 0, 0, 177, 0, 0, 2, 0, 0], [ 0, 2, 4, 53, 5, 0, 5, 100, 3, 2], [ 0, 6, 6, 139, 7, 0, 20, 2, 0, 0]]) 이 클러스터링 결과의 adjusted Rand index와 adjusted mutual info값은 다음과 같다. 123456789101112131415from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_scoreprint(adjusted_rand_score(digits.target, y_pred))print(adjusted_mutual_info_score(digits.target, y_pred))array([[ 1, 0, 0, 0, 0, 177, 0, 0, 0, 0], [ 0, 1, 1, 0, 0, 0, 55, 99, 24, 2], [ 0, 13, 0, 2, 3, 1, 2, 8, 148, 0], [ 0, 154, 2, 13, 7, 0, 0, 7, 0, 0], [163, 0, 0, 0, 7, 0, 7, 4, 0, 0], [ 2, 0, 136, 43, 0, 0, 0, 0, 0, 1], [ 0, 0, 0, 0, 0, 1, 1, 2, 0, 177], [ 0, 0, 0, 0, 177, 0, 0, 2, 0, 0], [ 0, 2, 4, 53, 5, 0, 5, 100, 3, 2], [ 0, 6, 6, 139, 7, 0, 20, 2, 0, 0]])","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"}]},{"title":"clustering","slug":"clustering","date":"2019-01-05T11:30:02.000Z","updated":"2019-01-10T11:43:08.374Z","comments":true,"path":"2019/01/05/clustering/","link":"","permalink":"https://p829911.github.io/2019/01/05/clustering/","excerpt":"","text":"주어진 데이터 집합을 유사한 데이터들의 그룹으로 나누는 것을 클러스터링(clustering)이라고 하고 이렇게 나누어진 유사한 데이터의 그룹을 클러스터(cluster)라 한다. 클러스터링은 예측(prediction) 문제와 달리 특정한 독립변수와 종속변수의 구분도 없고 학습을 위한 목푯값(target value)도 필요로 하지 않는 비지도학습(unsupervised learning)의 일종이다. 클러스터링 방법대부분의 클러스터링 방법들도 예측모형처럼 특정한 목표함수의 값을 최소화 혹은 최대화하지만 예측모형과 달리 다만 목표함수가 명확히 주어지지 않았기 때문에 목표함수의 정의 및 최적화 방법들이 각기 다른 다양한 클러스터링 방법이 존재한다. 다음과 같은 클러스터링 방법이 많이 쓰인다. K-means DBSCAN Spectral Clustering Affinity Propagation 계층적 클러스터링(Hierarchical Clustering) 클러스터링 방법은 사용법과 모수 등이 서로 다르다. 예를 들어 K-means, Spectral Clustering 등은 클러스터의 갯수를 미리 지정해 주어야 하지만 DBSCAN이나 Affinity Propagation 등은 클러스터의 갯수를 지정할 필요가 없다. 다만 이 경우에는 다른 종류의 모수를 지정해주어야 하는데 이 모수의 값에 따라 클러스터링 갯수가 달라질 수 있다. 클러스터링 성능기준클러스터링 성능의 경우에는 분류문제와 달리 성능기준을 만들기 어렵다. 심지어는 원래 데이터가 어떻게 클러스터링되어 있었는지를 보여주는 정답(groundtruth)이 있는 경우도 마찬가지이다. 따라서 다양한 성능기준이 사용되고 있다. 다음은 클러스터링 성능기준의 예이다. Adjusted Rand Index Adjusted Mutual Information Silhouette Coefficient Incidence Matrix(adjusted) Rand index를 구하려면 데이터가 원래 어떻게 클러스터링되어 있어야 하는지를 알려주는 정답(groundtruth)이 있어야 한다. $N$개의 데이터 집합에서 $i, j$ 두 개의 데이터를 선택하였을 때 그 두 데이터가 같은 클러스터에 속하면 1 다른 데이터에 속하면 0이라고 하자. 이 값을 $N \\times N$ 행렬 $T$로 나타내자. T_{ij} = \\begin{cases} 1 &amp; \\text{$i$와 $j$가 같은 클러스터} \\\\ 0 &amp; \\text{$i$와 $j$가 다른 클러스터} \\end{cases}예를 들어 $\\{0, 1, 2, 3, 4\\}$라는 5개의 데이터 집합에서 $\\{0, 1, 2\\}$과 $\\{3, 4\\}$이 같은 클러스터라면 다음과 같아진다. 1234567groundtruth = np.array([ [1, 1, 1, 0, 0], [1, 1, 1, 0, 0], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1],]) 이제 클러스터링을 적용한 결과를 같은 방법으로 행렬 $C$로 표시하자. 만약 클러스터링이 정확하다면 이 행렬은 정답을 이용해서 만든 행렬과 거의 같은 값을 가져야 한다 만약 클러스터링 결과 $\\{0,1\\}$ 과 $\\{2, 3, 4\\}$ 이 같은 클러스터라면 다음과 같아진다. 1234567clustering = np.array([ [1, 1, 0, 0, 0], [1, 1, 0, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 1],]) 이 두 행렬의 모든 원소에 대해 값이 같으면 1 다르면 0으로 계산한 행렬을 incidence matrix라고 한다. 즉 데이터 집합에서 만들 수 있는 모든 데이터 쌍에 대해 정답과 클러스터링 결과에서 동일한 값을 나타내면 1, 다르면 0이 된다. R_{ij} = \\begin{cases} 1 &amp; \\text{if $T_{ij} = C_{ij}$} \\\\ 0 &amp; \\text{if $T_{ij} \\neq C_{ij}$} \\end{cases}즉, 원래 정답에서 1번 데이터와 2번 데이터가 다른 클러스터인데 클러스터링 결과에서도 다른 클러스터라고 하면 $R_{12} = 0$이다. 위 예제에서 incidence matrix를 구하면 다음과 같다. 1234567incidence = 1 * (groundtruth == clustering)incidencearray([[1, 1, 0, 1, 1], [1, 1, 0, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 1]]) Adjusted Rand IndexRand index는 가능한 모든 쌍의 경우에 대해 정답인 쌍의 갯수의 비율로 정의한다. 이 값은 groundtruth를 목표값, 클러스터링 결과를 예측값으로 하는 이진분류문제의 정확도(accuracy)에 해당한다. 123rand_index = np.sum(incidence) / np.prod(incidence.shape)rand_index0.68 Rand index는 0부터 1까지의 값을 가지고 1이 가장 좋은 성능을 뜻한다. Rand index의 문제점은 무작위로 클러스터링을 한 경우에도 어느 정도 좋은 값이 나올 가능성이 높다는 점이다. 즉 무작위 클러스터링 에서 생기는 Rand index의 기댓값이 너무 크다. 이를 해결하기 위해 무작위 클러스터링에서 생기는 Rand index의 기댓값을 원래의 값에서 빼서 기댓값과 분산을 재조정한 것이 adjusted Rand index다. adjusted Rand index는 무작위 클러스터링의 경우에 0이 나올 확률이 높다. 하지만 경우에 따라서는 음수가 나올 수도 있다. adjusted Rand index를 계산하려면 다음과 같은 contigency table을 만들어야 한다. contingency table은 정답과 클러스터링 결과에서 각각 같은 클러스터에 속하는 데이터의 갯수를 나타낸 것이다. 정답은 T = \\{T_1, T_2, \\dots, T_r \\}의 $r$ 개의 클러스터이고 클러스터링 결과는 C = \\{C_1, C_2, \\cdots, C_3 \\}의 $s$ 개의 클러스터라고 가정한다. \\begin{array}{c|cccc|c} T \\; \\backslash \\; C &amp; C_1&amp; C_2&amp; \\ldots&amp; C_s&amp; \\text{소계} \\\\ \\hline T_1&amp; n_{11}&amp; n_{12}&amp; \\ldots&amp; n_{1s}&amp; a_1 \\\\ T_2&amp; n_{21}&amp; n_{22}&amp; \\ldots&amp; n_{2s}&amp; a_2 \\\\ \\vdots&amp; \\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots&amp; \\vdots \\\\ T_r&amp; n_{r1}&amp; n_{r2}&amp; \\ldots&amp; n_{rs}&amp; a_r \\\\ \\hline \\text{소계}&amp; b_1&amp; b_2&amp; \\ldots&amp; b_s&amp; \\end{array} $n_{ij}$: 정답에서는 클러스터 $T_i$ 에 속하고 클러스터링 결과에서는 클러스터 $C_j$ 에 속하는 데이터의 수 $a_i = \\sum_{j=1}^s n_{ij}$ $b_j = \\sum_{i=1}^r n_{ij}$ 무작위 클러스터링의 rand index 기댓값을 구하는 공식을 적용하면 adjusted Rand index값이 다음처럼 정의된다. \\text{ARI} = \\frac{ \\overbrace{\\sum_{ij} \\binom{n_{ij}}{2}}^\\text{Index} - \\overbrace{[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}^\\text{기댓값} }{ \\underbrace{\\frac{1}{2} [\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}]}_\\text{최댓값} - \\underbrace{[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}_\\text{기댓값} }위에서 예로 들었던 타원형 데이터 예제에 대해 여러가지 클러스터링 방법을 적용하였을때 adjusted Rand index 값을 계산해보면 DBSCAN과 Spectral Clustering의 값이 높게 나오는 것을 확인할 수 있다. scikit-learn 패키지의 metrics.cluster 서브패키지는 adjusted_rand_score 명령을 제공한다. 123456789101112131415161718from sklearn.metrics.cluster import adjusted_rand_scoreX, y_true = anisotropicX = StandardScaler().fit_transform(X)for name, algorithm in clustering_algorithms: with ignore_warnings(category=UserWarning): algorithm.fit(X) if hasattr(algorithm, 'labels_'): y_pred = algorithm.labels_.astype(np.int) else: y_pred = algorithm.predict(X) print(\"&#123;:25s&#125;: ARI=&#123;:5.3f&#125;\".format(name, adjusted_rand_score(y_true, y_pred))) K-Means : ARI=0.607DBSCAN : ARI=0.975Spectral Clustering : ARI=0.959Hierarchical Clustering : ARI=0.685Affinity Propagation : ARI=0.617 Adjusted Mutual Informationmutual information은 두 확률변수간의 상호 의존성을 측정한 값이다. 클러스터링 결과를 이산확률변수라고 가정한다. 정답은 T = \\{T_1, T_2, \\cdots, T_r \\}의 $r$ 개의 값을 가질 수 있는 이산확률변수이고 클러스터링 결과는 C = \\{C_1, C_2, \\cdots, C_s \\}의 $s$ 개의 값을 가질 수 있는 이산확률변수라고 하자. 전체 데이터의 갯수를 $N$ 이라고 하면 이산확률변수 $T$ 의 분포는 P(i) = \\dfrac{|T_i|}{N}로 추정할 수 있다. 이 식에서 $|T_i|$ 는 클러스터 $T_i$ 에 속하는 데이터의 갯수를 나타낸다. 비슷하게 이산확률변수 $C$ 의 분포는 P&#39;(j) = \\dfrac{|C_i|}{N}라고 추정하고 $T$ 와 $C$ 의 결합확률분포는 P(i, j) = \\dfrac{|\\;T_i \\;\\cap\\; C_j\\;|}{N}라고 추정한다. 여기에서 $|T_i \\cap C_j|$ 는 클러스터 $T_i$ 에도 속하고 클러스터 $C_j$ 에도 속하는 데이터의 갯수를 나타낸다. 확률변수 $T, C$ 의 mutual information은 MI(T, C) = \\sum_{i=1}^r\\sum_{j=1}^s P(i,j)로 정의한다. 만약 두 확률변수가 서로 독립이면 mutual information의 값은 0이며 이 값이 mutual information이 가질 수 있는 최소값이다. 두 확률변수가 의존성이 강할수록 mutual information은 증가한다. 또한 클러스터의 갯수가 많아질수록 mutual information이 증가하므로 올바른 비교가 어렵다. 따라서 adjusted Rand index의 경우와 마찬가지로 각 경우에 따른 mutual information의 기댓값을 빼서 재조정한 것이 adjusted mutual information이다. 다음은 위에서 예로 들었던 타원형 데이터 예제에 대해 여러가지 클러스터링 방법을 적용하였을때 adjusted mutual information 값을 계산한 결과이다. scikit-learn 패키지의 metrics.cluster 서브패키지는 adjusted_mutual_info_score 명령을 제공한다. 123456789101112131415161718from sklearn.metrics.cluster import adjusted_mutual_info_scoreX, y_true = anisotropicX = StandardScaler().fit_transform(X)for name, algorithm in clustering_algorithms: with ignore_warnings(category=UserWarning): algorithm.fit(X) if hasattr(algorithm, 'labels_'): y_pred = algorithm.labels_.astype(np.int) else: y_pred = algorithm.predict(X) print(\"&#123;:25s&#125;: ARI=&#123;:5.3f&#125;\".format(name, adjusted_mutual_info_score(y_true, y_pred)))array([[1, 1, 0, 1, 1], [1, 1, 0, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 1]]) 실루엣 계수지금까지는 각각의 데이터가 원래 어떤 클러스터에 속해있었는지 정답(groudtruth)를 알고 있는 경우를 다루었다. 하지만 이러한 정답 정보가 없다면 어떻게 클러스터링이 잘되었는지 판단할 수 있을까? 실루엣 계수(Silhouette coeffient)는 이러한 경우에 클러스터링의 성능을 판단하기 위한 기준의 하나이다. 우선 모든 데이터 쌍 $(i, j)$ 에 대해 거리(distance) 혹은 비유사도(dissimilarity)을 구한다. 이 결과를 이용하여 모든 데이터 $i$에 대해 다음 값을 구한다. $a_i$: $i$ 와 같은 클러스터에 속한 원소들의 평균 거리 $b_i$: $i$ 와 다른 클러스터 중 가장 가까운 클러스터까지의 평균 거리 실루엣 계수는 s = \\dfrac{b - a}{\\text{max}(a,b)}로 정의한다. 만약 데이터 $i$ 에 대해 같은 클러스터의 데이터가 다른 클러스터의 데이터보다 더 가깝다면 실루엣 계수는 양수가 된다. 하지만 만약 다른 클러스터의 데이터가 같은 클러스터의 데이터보다 더 가깝다면 클러스터링이 잘못된 경우라고 볼 수 있는데 이 때는 실루엣 계수가 음수가 된다. 실루엣 계수가 클수록 보다 좋은 클러스터링이라고 이야기 할 수 있다. 클러스터링 방법 중에는 클러스터의 갯수를 사용자가 정해주어야 하는 것들이 있는데 실루엣 계수는 이 경우 클러스터의 갯수를 정하는데 큰 도움이 된다. 앞에서 예로 들었던 3개의 원형 데이터에 대해 KMean 방법으로 클러스터 갯수를 바꾸어가면서 클러스터링 결과를 살펴보자. scikit-learn 패키지의 metrics 서브패키지는 실루엣 계수를 계산하는 silhouette_samples 명령을 제공한다. 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.metrics import silhouette_samplesX = StandardScaler().fit_transform(blobs[0])colors = plt.cm.tab10(np.arange(20, dtype=int))plt.figure(figsize=(6, 8))for i in range(4): model = KMeans(n_clusters=i + 2, random_state=0) cluster_labels = model.fit_predict(X) sample_silhouette_values = silhouette_samples(X, cluster_labels) silhouette_avg = sample_silhouette_values.mean() plt.subplot(4, 2, 2 * i + 1) y_lower = 10 for j in range(i + 2): jth_cluster_silhouette_values = sample_silhouette_values[cluster_labels == j] jth_cluster_silhouette_values.sort() size_cluster_j = jth_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_j plt.fill_betweenx(np.arange(y_lower, y_upper), 0, jth_cluster_silhouette_values, facecolor=colors[j], edgecolor=colors[j]) plt.text(-0.05, y_lower + 0.5 * size_cluster_j, str(j + 1)) plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") plt.xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]) plt.yticks([]) plt.title(\"실루엣 계수 평균: &#123;:5.2f&#125;\".format(silhouette_avg)) y_lower = y_upper + 10 plt.subplot(4, 2, 2 * i + 2) plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[cluster_labels]) plt.xlim(-2.5, 2.5) plt.ylim(-2.5, 2.5) plt.xticks(()) plt.yticks(()) plt.title(\"클러스터 수: &#123;&#125;\".format(i + 2))plt.tight_layout()plt.show()","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"}]},{"title":"dark sky 사이트를 이용한 날씨 크롤링","slug":"dark-sky-사이트를-이용한-날씨-크롤링","date":"2019-01-01T09:45:00.000Z","updated":"2019-01-01T09:46:24.182Z","comments":true,"path":"2019/01/01/dark-sky-사이트를-이용한-날씨-크롤링/","link":"","permalink":"https://p829911.github.io/2019/01/01/dark-sky-사이트를-이용한-날씨-크롤링/","excerpt":"","text":"https://darksky.net/dev 다음 사이트를 이용해 api로 날씨 데이터를 크롤링 해 볼 것이다. dark sky api는 전 세계 현재/과거/미래에 관련된 날씨와 관련된 많은 데이터들을 제공한다. 사이트에 가입 후 console로 들어가면 다음과 같은 창이 나타난다. Your Secret Key에 있는 key를 사용하여 dark sky로 쿼리를 날려야 한다. dark sky api 는 하루에 1000건의 무료 call을 제공하고 있으며 1건이 추가 될 때마다 $0.0001를 받고 있다. 1000건이 넘는 요청을 해야 할 경우 account setting에서 카드 등록을 하고 하루 최대 call수를 늘려주면 요청이 정상적으로 된다. 이 포스트 에서는 구글 맵을 이용하여 지역 명으로 위도 경도를 얻은 후 그 위도 경도를 이용하여 dark sky에 요청하여 날씨 정보를 받는 실습을 진행해 보았다. 자세한 코드는 아래 링크에서 확인 가능하다. https://github.com/p829911/python_study/blob/master/crawling/dark_sky_weather_crawling.ipynb","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Crawling","slug":"Crawling","permalink":"https://p829911.github.io/tags/Crawling/"}]},{"title":"모형 최적화","slug":"모형-최적화","date":"2019-01-01T08:13:29.000Z","updated":"2019-01-01T08:21:50.409Z","comments":true,"path":"2019/01/01/모형-최적화/","link":"","permalink":"https://p829911.github.io/2019/01/01/모형-최적화/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 머신 러닝 모형이 완성된 후에는 최적화 과정을 통해 예측 성능을 향상시킨다. Scikit-Learn 의 모형 하이퍼 파라미터 튜닝 도구Scikit-Learn에서는 다음과 같은 모형 최적화 도구를 지원한다. validation_curve 단일 하이퍼 파라미터 최적화 GridSearchCV 그리드를 사용한 복수 하이퍼 파라미터 최적화 ParameterGrid 복수 파라미터 최적화용 그리드 validation_curvevalidation_curve 함수는 최적화할 파라미터 이름과 범위, 그리고 성능 기준을 param_name, param_range, scoring 인수로 받아 파라미터 범위의 모든 경우에 대해 성능 기준을 계산한다. 12345678from sklearn.datasets import load_digitsfrom sklearn.svm import SVCfrom sklearn.model_selection import validation_curvedigits = load_digits()X, y = digits.data, digits.targetparam_range = np.logspace(-6, -1, 10) 12345%%timetrain_scores, test_scores = \\ validation_curve(SVC(), X, y, param_name=\"gamma\", param_range=param_range, cv=10, scoring=\"accuracy\", n_jobs=1) 123456789101112131415161718train_scores_mean = np.mean(train_scores, axis=1)train_scores_std = np.std(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1)test_scores_std = np.std(test_scores, axis=1)mpl.rcParams[\"font.family\"] = 'DejaVu Sans'plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")plt.legend(loc=\"best\")plt.title(\"Validation Curve with SVM\")plt.xlabel(\"$\\gamma$\")plt.ylabel(\"Score\")plt.ylim(0.0, 1.1)plt.show() GridSearchCVGridSearchCV 클래스는 validation_curve 함수와 달리 모형 래퍼(Wrapper) 성격의 클래스이다. 클래스 객체에 fit 메서드를 호출하면 grid search를 사용하여 자동으로 복수개의 내부 모형을 생성하고 이를 모두 실행시켜서 최적 파라미터를 찾아준다. 생성된 복수개와 내부 모형과 실행 결과는 다음 속성에 저장된다. grid_scores_ param_grid 의 모든 파라미터 조합에 대한 성능 결과. 각각의 원소는 다음 요소로 이루어진 튜플이다. parameters: 사용된 파라미터 mean_validation_score: 교차 검증(cross-validation) 결과의 평균값 cv_validation_scores: 모든 교차 검증(cross-validation) 결과 best_score_ 최고 점수 best_params_ 최고 점수를 낸 파라미터 best_estimator_ 최고 점수를 낸 파라미터를 가진 모형 \u0002\u0002 1234567891011121314from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import SVCpipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]param_grid = [ &#123;'clf__C': param_range, 'clf__kernel': ['linear']&#125;, &#123;'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']&#125;]gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=1)%time gs = gs.fit(X, y) 1gs.cv_results_[\"params\"] 1gs.cv_results_[\"mean_test_score\"] 12print(gs.best_score_)print(gs.best_params_) ParameterGrid때로는 scikit-learn 이 제공하는 GridSearchCV 이외의 방법으로 그리드 탐색을 해야하는 경우도 있다. 이 경우 파라미터를 조합하여 탐색 그리드를 생성해 주는 명령어가 ParameterGrid 이다. ParameterGrid 는 탐색을 위한 iterator 역할을 한다. 1from sklearn.model_selection import ParameterGrid 12param_grid = &#123;'a': [1, 2], 'b': [True, False]&#125;list(ParameterGrid(param_grid)) 12param_grid = [&#123;'kernel': ['linear']&#125;, &#123;'kernel': ['rbf'], 'gamma': [1, 10]&#125;]list(ParameterGrid(param_grid)) 병렬 처리GridSearchCV 명령에는 n_jobs 라는 인수가 있다. 디폴트 값은 1인데 이 값을 증가시키면 내부적으로 멀티 프로세서를 사용하여 그리드서치를 수행한다. 만약 CPU 코어의 수가 충분하다면 n_jobs 를 늘릴수록 속도가 증가한다. 123param_grid = &#123;\"gamma\": np.logspace(-6, -1, 10)&#125;gs1 = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=1)gs2 = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=2) 12%%timegs1.fit(X, y) 12%%timegs2.fit(X, y)","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://p829911.github.io/categories/Math/Classification/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Classification","slug":"Classification","permalink":"https://p829911.github.io/tags/Classification/"}]},{"title":"hexo 테마 변경하기","slug":"hexo-테마-변경하기","date":"2018-12-13T14:05:43.000Z","updated":"2018-12-13T15:23:42.000Z","comments":true,"path":"2018/12/13/hexo-테마-변경하기/","link":"","permalink":"https://p829911.github.io/2018/12/13/hexo-테마-변경하기/","excerpt":"","text":"hexo 폰트 변경 나눔스퀘어라운드 (NanumSquareRound) https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css head.ejs 수정하기1&lt;%- css(&apos;https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css&apos;) %&gt; 1vi blog/themes/hueman/layout/common/head.ejs head 부분에 추가 가운데 보면 위의 코드를 넣어 준 것을 볼 수 있다. css 적용1vi blog/themes/hueman/source/css/_variables.styl font-sans 본문 폰트 : NanumSquareRound 추가 font-mono 코드 폰트 : D2Coding을 추가 hexo theme color 변경 위와 같이 hueman 테마의 색상을 변경할 수 있다. 1vi blog/themes/hueman/source/css/_variables.styl color-default: hexo 폴더 안에 있는 _config.yml 파일에서 설정해 줄 수 있는 색상이다. 위에 사진에 보이는 follow 부분에 나타나는 색상을 적용해 줄 수 있다, 여기서 적용하지 말고 _config.yml파일에서 바꿔준다. color-header-background: 위에 보이는 파란색 영역에 색상을 지정해 줄 수 있다. color-border: 위에 보이는 노란색 영역에 색상을 지정해 줄 수 있다. color-nav-background: 검은색 영역에 색상 지정 color-footer-background: 맨 밑의 영역에 색상 지정 color-sidebar- background: sidebar의 배경색은 모바일에서는 바뀌지만, 데스크탑 사이트에서는 바뀌지 않는다 이 부분의 문제점은 좀 더 찾아봐야할 것 같다. 아래의 링크에서 색상 선택에 도움을 받을 수 있을 것이다.Hex Color Code","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Blog","slug":"Blog","permalink":"https://p829911.github.io/tags/Blog/"}]},{"title":"entropy","slug":"entropy","date":"2018-12-13T11:18:47.000Z","updated":"2018-12-13T11:29:43.000Z","comments":true,"path":"2018/12/13/entropy/","link":"","permalink":"https://p829911.github.io/2018/12/13/entropy/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 엔트로피의 정의$Y = 0$ 또는 $Y=1$인 두 가지 값을 가지는 확률 분포가 다음과 같이 세 종류가 있다고 하자 확률 분포 $Y_1$: $P(Y=0) = 0.5, P(Y=1) = 0.5$ 확률 분포 $Y_2$: $P(Y=0) = 0.8, P(Y=1) = 0.2$ 확률 분포 $Y_3$ : $P(Y=0) = 0.8, P(Y=1) = 0.2$ 이 확률 값이 베이지안 확률이라면 확률 분포 $Y_1$은 $y$값에 대해 아무것도 모르는 상태, $Y_3$은 $y$값이 0이라고 100% 확신하는 상태, $Y_2$은 $y$값이 0이라고 믿지만 아닐 수도 있다는 것을 아는 상태를 나타내고 있을 것이다. 확률 분포들이 가지는 확신의 정도를 수치로 표현하는 것을 엔트로피(entropy)라고 한다. 확률 변수의 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 높아진다. 반대로 특정한 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피가 작아진다. 물리학에서는 상태가 분산되어 있는 정도를 엔트로피로 정의한다. 여러가지로 고루 분산되어 있을 수 있으면 엔트로피가 높고 특정한 하나의 상태로 몰려있으면 엔트로피가 낮다. 확률분포의 엔트로피는 물리학의 엔트로피 용어를 빌려온 것이다. 엔트로피는 수학적으로 다음과 같이 정의한다 확률변수 $Y$가 베르누이나 카테고리 분포와 같은 이산 확률변수이면 H[Y] = -\\sum_{k=1}^K P(y_k) log_2 P(y_k)이 식에서 $K$는 $X$가 가질 수 있는 클래스의 수이고 $P(y)$는 확률질량함수이다. 확률변수 $Y$가 연속 확률변수이면 H[Y] = - \\int_{-\\infty}^{\\infty} p(y) log_2p(y)\\;dy이 식에서 $P(y)$는 확률밀도함수이다. 로그의 밑(base)이 2로 정의된 것은 정보통신과 관련을 가지는 역사적인 이유 때문이다. 위에서 예를 든 $Y_1$, $Y_2$, $Y_3$에 대해 엔트로피를 구하면 다음과 같다. \\begin{eqnarray} H[Y_1] &=& -\\frac{1}{2}log_2\\frac{1}{2} - \\frac{1}{2}log_2\\frac{1}{2} &=& 1 \\\\\\\\ H[Y_2] &=& -\\frac{8}{10}log_2\\frac{8}{10} - \\frac{2}{10}log_2\\frac{2}{10} &=& 0.72 \\\\\\\\ H[Y_3] &=& -1log_21 - 0log_20 &=& 0 \\end{eqnarray}엔트로피 계산에서 $p(y) = 0$인 경우에는 다음과 같은 극한값을 사용한다. 이 값은 로피탈의 정리에서 구할 수 있다. 엔트로피의 성질확률변수가 결정론적이면 확률분포에서 특정한 하나의 값이 나올 확률이 1이다. 이 때 엔트로피는 0이 되고 이 값은 엔트로피가 가질 수 있는 최솟값이다. 반대로 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산 확률변수가 가질 수 있는 클래스가 $2^K$개이고 이산 확률변수가 가질 수 있는 엔트로피의 최대값은 각 클래스가 모두 같은 확률을 가지는 때이다. 이 때 엔트로피의 값은 H = -\\frac{2^K}{2^K}log_2\\frac{1}{2^K} = K이다. 엔트로피와 정보량엔트로피는 확률변수가 담을 수 있는 정보의 양을 의미한다고 볼 수도 있다. 확률변수가 담을 수 있는 정보량(information)이란 확률변수의 표본값을 관측해서 얻을 수 있는 추가적인 정보의 종류를 말한다. 엔트로피가 0이면 확률변수는 결정론적이므로 확률 변수의 표본값은 항상 같다. 따라서 확률 변수의 표본값을 관측한다고 해도 우리가 얻을 수 있는 추가 정보는 없다. 반대로 엔트로피가 크다면 확률변수의 표본값이 가질 수 있는 경우의 수가 증가하므로 표본값을 실제로 관측하기 전까지는 아는 것이 거의 없다. 반대로 말하면 확률변수의 표본값이 우리에게 가져다 줄 수 있는 정보의 양이 많다. 엔트로피와 무손실 인코딩엔트로피는 원래 통신 분야에서 데이터가 가지고 있는 정보량을 계산하기 위해 고안되었다. 예를 들어 4개의 알파벳 A, B, C, D로 씌어진 문서가 있다고 하자. 이 문서를 0과 1로 이루어진 이진수로 변환할 때 일반적인 경우라면 다음과 같이 인코딩을 할 것이다. A = “00” B = “01” C = “10” D = “11” 이렇게 인코딩을 하면 1,000 글자로 이루어진 문서는 이진수 2,000개가 된다. 만약 문서에서 각 알파벳이 나올 확률이 동일하지 않고 다음과 같다고 가정하다. \\Big\\{ \\dfrac{1}{2}, \\dfrac{1}{4}, \\dfrac{1}{8}, \\dfrac{1}{8} \\Big\\}이 때는 다음과 같이 가변길이 인코딩(variable length encoding)을 하면 인코딩된 이진수의 수를 줄일 수 있다. A = “0” B = “10” C = “110” D = “111” 인코딩된 이진수의 숫자는 다음 계산에서 약 1,750개가 됨을 알 수 있다. \\left(1000 \\times \\frac{1}{2}\\right) \\cdot 1 + \\left(1000 \\times \\frac{1}{4}\\right) \\cdot 2 + \\left(1000 \\times \\frac{1}{8}\\right) \\cdot 3 + \\left(1000 \\times \\frac{1}{8}\\right) \\cdot 3 = 17501.75는 알파벳 한 글자를 인코딩하는데 필요한 평균 비트(bit)수이며 확률변수의 엔트로피 값과 같다. H = -\\frac{1}{2}log_2\\frac{1}{2}-\\frac{1}{4}log_2\\frac{1}{4}-\\frac{2}{8}log_2\\frac{1}{8} = 1.75엔트로피의 추정확률 변수 모형, 즉 이론적인 확률 밀도(질량) 함수가 아닌 실제 데이터가 주어진 경우에는 확률질량함수를 추정하여 엔트로피를 계산한다. 예를 들어 데이터가 모두 80개가 있고 그 중 $Y=0$인 데이터가 40개, $Y=1$인 데이터가 40개 있는 경우는 엔트로피가 1이다. P(y=0) = \\frac{40}{80} = \\frac{1}{2} \\\\ \\\\ P(y=1) = \\frac{40}{80} = \\frac{1}{2} \\\\ \\\\ H[Y] = -\\frac{1}{2}log_2\\left(\\frac{1}{2}\\right) -\\frac{1}{2}log_2\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1만약 데이터가 모두 60개가 있고 그 중 $Y=0$인 데이터가 20개, $Y=1$인 데이터가 40개 있는 경우는 엔트로피가 약 0.92이다. P(y=0) = \\frac{20}{60} = \\frac{1}{3}\\\\ P(y=1) = \\frac{40}{60} = \\frac{2}{3}\\\\ H[Y] = -\\frac{1}{3}log_2\\left(\\frac{1}{3}\\right)-\\frac{2}{3}log_2\\left(\\frac{2}{3}\\right) = 0.92지니불순도엔트로피와 유사한 개념으로 지니불순도(Gini Impurity)라는 것이 있다. 지니불순도는 엔트로피처럼 확률분포가 어느쪽에 치우쳐있는가를 재는 척도지만 로그를 사용하지 않으므로 계산량이 더 적어 엔트로피 대용으로 많이 사용된다. G[Y] = \\sum_{k=1}^K P(y_k)(1-P(y_k))1234567891011P0 = np.linspace(0.001, 1 - 0.001, 1000)P1 = 1 - P0H = - P0 * np.log2(P0) - P1 * np.log2(P1)G = (P0 * (1 - P0) + P1 * (1 - P1)) # 엔트로피와 높이를 맞춰주기 위해 스케일링 하는 경우도 있다. 여기서는 G 앞에 2를 곱해준다.plt.plot(P0, H, \"-\", label=\"엔트로피\")plt.plot(P0, G, \"--\", label=\"지니불순도\")plt.legend()plt.xlabel(\"P(0)\")plt.show() 결합 엔트로피두 이산확률변수 $X, Y$에 대해 결합 엔트로피(joint entropy)는 다음처럼 정의한다. H[X,Y] = -\\sum_{i=1}^{K_x}\\sum_{j=1}^{K_y}P(x_i,y_j)log_2P(x_i,y_j)연속확률변수의 경우에는 다음처럼 정의한다. H[X,Y] = - \\int_x\\int_y p(x,y)log_2p(x,y)\\; dxdy조건부 엔트로피조건부 엔트로피는 상관관계가 있는 두 확률변수 $X,Y$가 있고 $X$의 값을 안다면 $Y$의 확률변수가 가질 수 있는 정보의 양을 뜻한다. 수학적으로는 다음과 같이 정의한다. H[Y \\mid X] = - \\sum_{i=1}^{K_x} \\sum_{j=1}^{K_y} P(x_i,y_j)\\,log2\\, P(y_j \\mid x_i)연속확률변수의 경우에는 다음처럼 정의한다. H[Y \\mid X] = - \\int_x \\int_y p(x,y) \\, log_2\\,p(y \\mid x)\\; dxdy조건부 엔트로피는 조건부 확률 분포의 정의를 사용하여 다음과 같이 고칠 수 있다. H[Y \\mid X] = - \\sum_{i=1}^{K_x} \\,P(x_i)\\,H[Y \\mid X=x_i]연속확률변수의 경우에는 다음과 같다. H[Y \\mid X] = - \\int_{x} \\,p(x) \\,H[Y \\mid X=x] \\; dx크로스 엔트로피두 확률분포 $p(x), p(y)$의 크로스 엔트로피(cross entropy) $H[p,q]$는 다음과 같이 정의한다. 크로스 엔트로피의 경우에는 같은 확률변수에 대한 두 개의 추정 확률분포를 비교하는데 주로 쓰이기 때문에 표기를 할 땐 결합 엔트로피처럼 확률변수를 인수로 사용하지 않고 확률분포를 인수로 사용한다는 점에 주의해라 H[P,Q] = -\\sum_{k=1}^K P(y_k)\\, \\log_2\\, Q(y_k)또는 H[p,q] = - \\int_yp(y)\\log_2\\,q(y)\\,dy크로스 엔트로피는 확률분포의 차이를 정량화한 값이지만 기준이 되는 분포가 $p$로 고정되어 있다. 즉 $p$와 $q$가 바뀌면 값이 달라진다. H[p,q] \\neq H[q,p]크로스 엔트로피는 분류 모형의 성능을 측정하는데 사용된다. $Y$가 0 또는 1이라는 값만 가지는 이진 분류 문제를 예로 들어보자. $P_Y$는 $X$가 정해졌을 때 실제 $Y$가 가지는 분포를 뜻한다. $X$가 정해지면 $Y$는 확실히 0이거나 확실히 1이다. 즉, $P_Y$는 (0,1) 또는 (1,0)이 된다. 하지만 예측값 $\\hat{Y}$의 분포 $Q_{\\hat{Y}}$는 모수가 $\\mu$인 베르누이 분포이다. 즉 $Q_{\\hat{Y}}$는 $(1-\\mu, \\mu)$이다. 특정한 $X$에 대해 $P$와 $Q$의 크로스 엔트로피는 H[P,Q] = \\begin{cases} & -\\log_2 (1-\\mu) & Y=0 \\text{일 때} \\\\ & -\\log_2 \\mu & Y=1 \\text{일 때} \\end{cases}가 된다. $Y = 0$일 때 $\\mu$가 커질수록 즉, 예측이 틀릴수록 $-\\log_2 (1-\\mu)$의 값도 커진다. $Y = 1$일 때, $\\mu$가 작아질수록 즉, 예측이 틀릴수록 $-\\log_2(\\mu)$의 값도 커진다. 따라서 위 값은 예측의 틀린정도를 나타내는 오차 함수의 역할을 할 수 있다. 모든 데이터에 대해 이 값의 평균을 구하면 다음 식으로 표현할 수 있다. H[P,Q] = - \\frac{1}{N} \\sum_{i=1}^N(y_i \\log_2 \\mu_i + (1-y_i)\\log_2 (1-\\mu))같은 방법으로 이진 분류가 아닌 다중 분류에서도 크로스 엔트로피를 오차 함수로 사용할 수 있다. 쿨백-라이블러 발산쿨백-라이블러 발산(Kullback-Leibler divergence)은 두 확률분포 $p(y), q(y)$의 차이를 정량화하는 방법의 하나이다. 다음과 같이 정의한다. \\begin{eqnarray} KL(P || Q) &=& H[P,Q] - H[P] &=& \\sum_{i=1}^{K} P(y_i) \\log_2 \\left(\\dfrac{P(y_i)}{Q(y_i)}\\right) \\end{eqnarray}또는 \\begin{eqnarray} KL(p || q) &=& H[p, q] - H[p] &=& \\int p(y) \\log_2 \\left(\\dfrac{p(y)}{q(y)}\\right) dy \\end{eqnarray}쿨백-라이블러 발산은 크로스 엔트로피에서 기준이 되는 분포의 엔트로피 값을 뺀 값이므로 상대 엔트로피(relative entropy)라고도 한다. 그 값은 항상 양수이며 두 확률분포 $p(x), q(x)$가 완전히 같을 경우에만 0이 된다.","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://p829911.github.io/categories/Math/Classification/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Classification","slug":"Classification","permalink":"https://p829911.github.io/tags/Classification/"}]},{"title":"mysql install","slug":"mysql-install","date":"2018-12-06T12:52:38.000Z","updated":"2018-12-06T12:54:22.000Z","comments":true,"path":"2018/12/06/mysql-install/","link":"","permalink":"https://p829911.github.io/2018/12/06/mysql-install/","excerpt":"","text":"mysql install1sudo apt install -y mysql-server 1sudo mysql_secure_installaion press y|Y for Yes, any other key for No: n remove anonymous users? y disallow root login remotely? n remove test database and access to it? n reload privilege tables now? y success! 1sudo mysql passward 설정1234SELECT user,authentication_string,plugin,host FROM mysql.user;ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_passwordBY &apos;dss&apos;;FLUSH PRIVILEGES; root 사용자로 접속12mysql -u root -p# 패스워드 입력 상태확인bash 창에서 1sudo systemctl status mysql active 확인 외부접속 설정1sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf bind-address = 0.0.0.0으로 수정 외부접속이 허용되도록 mysql 설정 1mysql -u root -p 1grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;&lt;password&gt;&apos;; 포트 활성화AWS서버에서 3306포트 활성화 재시작으로 설정 적용 1sudo systemctl restart mysql.service mysql에서 접속 import sql file바로 데이터 베이스 넣기 1mysql -u root -p world &lt; world.sql mysql shell에서 넣기 1234create database world;use world;source world.sql;show tables; Mysql 삭제12345sudo apt remove --purge mysql-server mysql-clientsudo rm -rf /etc/mysql/var/lib/mysqlsudo apt autoremovesudo apt autocleansudo apt purge mysql*","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://p829911.github.io/categories/Mysql/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"},{"name":"Mysql","slug":"Mysql","permalink":"https://p829911.github.io/tags/Mysql/"}]},{"title":"다중 선형 회귀","slug":"regression","date":"2018-12-06T12:33:47.000Z","updated":"2018-12-13T11:22:47.000Z","comments":true,"path":"2018/12/06/regression/","link":"","permalink":"https://p829911.github.io/2018/12/06/regression/","excerpt":"","text":"다중 선형 회귀 설명변수들 $X_1, X_2, \\cdots, X_p$ 중 적어도 하나는 반응변수를 예측하는데 유용한가? $Y$를 설명하는 데 모든 설명변수들이 도움이 되는가? 또는 설명변수들의 일부만이 유용한가? 모델은 데이터에 얼마나 잘 맞는가? 주어진 설명변수 값들에 대해 어떤 반응변수 값을 예측해야 하고 그 예측은 얼마나 정확한가? 1. 반응변수와 설명변수 사이에 상관관계가 있는가? 단순선형회귀에서 반응변수와 설명변수 사이에 상관관계가 있는지는 단순히 $\\beta_1 = 0$인지 검사하면 결정할 수 있다. $p$개 설명변수가 있는 다중회귀에서는 모든 회귀계수들이 영인지, 즉 $\\beta_1 = \\beta_2 = \\cdots = \\beta_p =0$인지를 검사해야 한다. 단순선형회귀에서와 같이 이 질문에 답하기 위해 가설검정을 사용한다. 귀무가설 H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0대립가설 H_a: \\text{적어도 하나의 }\\beta_j\\text{는 영이 아니다.}이러한 가설 검정은 $F$-통계량을 계산함으로써 이루어진다. F = \\dfrac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n-p-1)}단순선형회귀에서와 같이 $\\text{TSS} = \\sum(y_i-\\bar{y})^2$ 이고 $\\text{RSS} = \\sum(y_i - \\hat{y_i})^2$이다. 만약 선형 모델의 가정이 같다면 다음이 성립함을 보여줄 것이다. E[\\,\\text{RSS}\\, / \\,(n-p-1)\\,] = \\sigma^2또한 귀무가설 $H_0$이 참이면 다음이 성립함을 보여줄 수도 있다. E[\\,(\\text{TSS} - \\text{RSS})\\,/\\,p\\,] = \\sigma^2그러므로, 반응변수와 설명변수들 사이에 상관관계가 없는 경우(RSS가 커질때 TSS와 거의 가까움) $F$- 통계량이 1에 매우 가까운 값이라고 기대할 수 있을 것이다. 반면에 만약 대립가설 $H_a$가 참이면 $E[\\,(\\text{TSS}-\\text{RSS})\\,/\\, p\\,] &gt; \\sigma^2$이고 그래서 $F$의 기대값은 1보다 크다. $H_0$을 기각하고 상관관계가 있다고 결론을 내릴 수 있으면 $F$-통계량이 얼마나 커야될까? $n\\text{과} p$값에 따라 다르다. $n$이 큰 경우에는 $F$-통계량이 1보다 약간만 크면 $H_0$에 반하는 증거가 된다. $n$이 작은 경우 $H_0$를 기각하려면 더 큰 $F$-통계량이 필요하다. 2. 중요 변수의 결정 모든 설명변수가 반응변수와 상관성이 있을 수도 있다. 하지만 대부분의 경우 설명변수들의 일부(서브셋)만이 반응변수와 상관관계가 있다. 상관성이 있는 설명변수만으로 모델 적합을 수행하기 위해 어느 설명변수가 반응변수와 상관성이 있는지 결정하는 것을 변수선택이라고 한다. 모델의 질 평가 맬로우즈(Mallows) $C_p$ AIC (Akaike information criterion) BIC (Bayesian information criterion) 수정된 $R^2$ $p$개 변수들의 일부를 포함하는 총 모델의 경우의 수는 $2^p$개에 이른다. 심지어 $p$가 크지 않더라도 모든 가능한 설명변수들의 부분집합을 다 시험해 보는 것은 현실적으로 어렵다. 예를 들어 $p = 2$인 경우 $2^2 = 4$모델을 고려하면 된다. 그러나 $p=30$이면 고려해야 하는 모델 수는 $2^{30} = 1,073,741,824$개로 늘어나 현실적으로 불가능에 가깝다. 그러므로 $p$가 아주 작은 경우가 아니면 $2^p$개 모델 모두를 고려할 수는 없고, 대신에 더 작은 수의 고려할 모델 집합을 선택하는 자동화되고 효과적인 기법이 필요하다. 이 목적을 위한 3가지 고전적인 기법은 아래와 같다. 전진선택: 이 방법은 절편만 포함하고 설명변수는 없는 영모델(null model)을 가지고 시작한다. $p$개의 단순 선형 회귀를 적합하여 가장 낮은 $RSS$가 발생되는 변수를 영모델에 추가한다. 그런 다음 새로운 새로운 2-변수 모델에 대해 가장 낮은 $RSS$가 생기는 변수를 모델에 추가한다. 이런 방식으로 어떤 정지규칙(stopping rule)을 만족할 때까지 계속된다. 후진선택(Backward selection): 이 방법은 모델의 모든 변수를 가지고 시작하여 가장 큰 $p$-값을 가지는 변수, 즉 통계적으로 중요도가 가장 낮은 변수를 제외한다. 그 다음에 새로운 (p-1)-변수의 모델을 적합하고 $p$-값이 가장 큰 변수를 제외한다. 이 과정을 정지 규칙이 만족될 때까지 계속한다. 예를 들어 모든 남아있는 변수들의 $p$-값이 어떤 임계치보다 작으면 이 과정을 중지한다. 혼합선택(Mixed selection): 이것은 전진선택과 후진선택을 결합한 것이다. 전진선택처럼 변수가 없는 모델로 시작하여 최상의 적합을 제공하는 변수를 하나씩 추가한다. 새로운 설명변수들이 모델에 추가됨에 따라 변수들에 대한 $p$-값이 커질 수 있다. 그러므로 모델의 변수들 중 어느 하나에 대한 $p$-값이 어떤 임계치보다 커지면 그 변수를 모델에서 제외한다. 이러한 전진선택 및 후진선택 단계를 계속하여 모델에 포함되는 모든 변수들은 충분히 작은 $p$-값을 가지고 모델에서 제외된 변수들은 만약 모델에 추가될 경우 $p$-값이 크게 될 때 중지한다. 후진 선택법은 만약 p &gt; n 이면 사용할 수 없지만 전진선택법은 항상 사용할 수 있다. 전진선택법은 그리디(greedy)방식이다. 그래서 초기에 포함된 변수들이 나중에는 유효하지 않을 수 있다. 이 문제는 혼합선택법으로 선택할 수 있다. 3. 모델 적합 모델 적합의 수치적 측도로 가장 흔히 사용되는 두가지는 $RSE$와 $R^2$(설명되는 분산의 비율)이다. 이 값들은 단순선형회귀에서와 같은 방식으로 계산되고 해석된다. 단순회귀에서 $R^2$은 반응변수와 설명변수의 상관계수의 곱이다. 다중 선형회귀에서 이것은 반응변수와 적합된 선형모델 사이의 상관계수의 제곱인 $Cor(Y,\\hat{Y})^2$과 동일하다. 사실 적합된 선형모델은 모든 가능한 선형모델 중에서 이 상관계수가 최대로 되는 것이다. 1에 가까운 $R^2$값은 모델이 반응변수 내 분산의 많은 부분을 설명한다는 것을 나타낸다. 모델에 더 많은 변수가 추가되면 비록 추가된 변수와 반응변수의 상관관계가 아주 약하더라도 $R^2$은 항상 증가할 것이다. 이것은 최소제곱 방정식에 변수를 추가하면 훈련 데이터(반드시 검정 데이터일 필요는 없다.)를 더 정확하게 적합할 수 있다는 사실 때문이다. 특정 독립 변수를 추가했을 때 $R^2$이 약간만 증가한다는 사실은 그 독립 변수가 모델에서 제외될 수 있다는 추가적인 증거가 된다. 모델에 그 독렵변수를 포함하는 것은 독립적인 검정표본에 대한 과적합으로 인해 좋지 않은 결과를 초래할 가능성이 높을 것이다. 4. 예측 다중선형모델 적합을 수행하였으면 $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\cdots + \\hat{\\beta_p}x_p$을 적용하여 설명변수 $X_1, X_2, \\cdots, X_p$의 값에 기초하여 반응 변수 $Y$를 예측하는 것은 어렵지 않다. 하지만, 이러한 예측에는 세 가지 명확하지 않은 것이 연관되어 있다. 계수추정 $\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_p}$는 $\\beta_0,\\beta_1,\\cdots, \\beta_p$에 대한 추정값이다. 즉 아래 최소제곱평면은 \\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X_1 + \\cdots + \\hat{\\beta_p}X_p다음의 실제 모회귀평면에 대한 추정값이다. f(X) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p계수추정의 부정확도는 2장의 축소가능 오차(reducible error)와 관련된다. 신뢰구간을 계산하여 $\\hat{Y}$가 $f(x)$에 얼마나 가까운지 결정할 수 있다. 물론, 실제로 $f(x)$에 대해 선형 모델을 가정하는 것은 거의 항상 현실에 대한 근사이다. 따라서 모델 편향(model bias)이라고 하는 잠재적으로 축소가능한 오차의 또 다른 출처가 있다. 그러므로 선형모델을 사용할 때 실제 표면에 대한 최상의 선형 근사를 추정하는 것이다. 하지만, 여기서는 이러한 차이를 무시하고 마치 선형 모델이 올바른 것으로 간주한다. 심지어 $f(x)$를 알아도 - $\\beta_0, \\beta_1, \\cdots, \\beta_p$에 대한 실제 값을 알아도 - 모델의 랜덤오차 때문에 반응변수 값을 완벽하게 예측할 수는 없다. 2장에서 이 오차를 축소불가능 오차(irreducible error)라고 하였다. 선형 모델의 확장표준선형회귀모델 $Y = \\beta_0 + \\beta_1X1 + \\beta_2X2 + \\cdots + \\beta_pX_p + \\epsilon$ 는 해석이 가능한 결과를 제공하며 많은 현실적인 문제에 대해서도 잘 동작한다. 하지만 이것은 실제로는 성립되지 않는 몇 가지 아주 제한적인 가정을 사용한다. 가장 중요한 가정 중 두 가지는 설명변수와 반응변수 사이의 관계는 가산적(additive)이고 선형적이라는 것이다. 가산성 가정: 설명변수 $X_j$의 변화가 반응변수 $Y$에 미치는 영향은 다른 설명변수 값에 독립적이다. 선형성 가정: $X_j$의 한 유닛 변화로 인한 $Y$의 변화는 $X_j$의 값에 관계없이 상수이다. 가산성 가정의 제거 Advertising 자료 분석에서 TV와 radio 둘 다 sales와 상관관계가 있다고 결론지었다. 이러한 결론의 근거가 되는 선형모델들은 한 광고매체의 지출 증가가 sales에 미치는 영향은 다른 매체에 대한 지출과 무관하다고(독립적이라고) 가정한다. 하지만, 이런 단순한 모델은 맞지 않을 수 있다. 라디오 광고 지출이 실제로 TV 광고의 효과를 증가시켜 TV에 대한 기울기 항이 라디오 지출이 늘어남에 따라 증가해야 한다고 해보자. 이러한 경우, 주어진 10만 달러의 고정 광고예산을 라디오와 TV에 절반씩 지출하는 것이 전체 예산을 TV 또는 라디오 어느 한쪽에 모두 사용하는 것보다 판매량 증가가 더 클 수 있다. 이것을 마케팅에서는 시너지 효과 라 하고 통계학에서는 상호작용 효과 라 한다. 두 개의 변수를 가지는 표준 선형회귀모델을 고려해보자 Y = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\epsilon이 모델에 따르면 $X_1$이 한 유닛 증가하면 $Y$는 평균 $\\beta_1$유닛만큼 증가할 것이다. $X_2$의 존재는 이 사실을 변경하지 않는다. 즉, $X_2$의 값에 관계없이 $X_1$이 한 유닛 증가하면 $Y$는 $\\beta_1$ 유닛 증가할 것이다. 상호작용 효과를 포함하도록 이 모델을 확장하는 한 가지 방법은 상호작용 항이라 불리는 세 번째 설명변수를 포함하는 것이다. 상호작용 항은 $X_1$과 $X_2$의 곱으로 구성된다. 그러면 결과 모델은 다음과 같이 표현 된다. Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon위의 식은 아래와 같이 다시 쓸 수 있다. \\begin{eqnarray} Y &=& \\beta_0 + (\\beta_1 + \\beta_3X_2)X_1 + \\beta_2X_2 + \\epsilon\\\\ &=& \\beta_0 + \\tilde{\\beta_1}X_1 + \\beta_2X_2 + \\epsilon \\end{eqnarray}여기서 $\\tilde{\\beta_1} = \\beta_1 + \\beta_3X_2$이다. $\\tilde{\\beta_1}$은 $X_2$에 따라 변하므로 $Y$에 대한 $X_1$의 효과는 더이상 상수가 아니다. $X_2$를 조정하면 $Y$에 대한 $X_1$의 효과가 변할 것이다. 이것은 실제 상관관계는 가산적이지 않다는 것이 명백하다. 가끔씩 상호작용 항은 매우 작은 $p$-값을 가지지만 관련된 주효과는 그렇지 않은 경우도 있다. 계층적 원리 에 의하면, 만약 모델에 상호작용을 포함하면 주효과는 그 계수와 연관된 $p$-값이 유의하지 않더라도 모델에 포함해야 한다. 다시 말해, 만약 $X_1$과 $X_2$사이의 상호작용이 중요한 것 같으면 $X_1$과 $X_2$의 계수 추정치가 큰 $p$-값을 가져도 모델에 $X_1$과 $X_2$를 포함해야 한다. 이유는 만약 $X_1 \\times X_2 $가 반응변수와 상관관계가 있으면 $X_1$또는 $X_2$의 계수가 영인지는 관심이 없다. 또한 $X_1 \\times X_2$는 보통 $X_1$ 및 $X_2$와 상관되어 있어 이들을 제외하는 것은 상호작용의 의미를 바꾸는 경향이 있다. 상호작용의 개념은 질적 변수 또는 양적 변수와 질적 변수의 조합에도 적용된다. 사실, 질적 변수와 양적 변수 사이의 상호작용을 해석하기는 특히 쉽다. 상호작용 항이 없을 경우 모델은 다음 형태를 가진다. \\begin{eqnarray} \\text{balance}_i &\\approx& \\beta_0 + \\beta_1 \\times \\text{income}_i + \\begin{cases}\\beta_2 & i\\text{번째 사람이 학생인 경우} \\\\ 0 & i\\text{번째 사람이 학생이 아닌 경우} \\end{cases} \\\\\\\\ &=& \\beta_1 \\times \\text{income}_i + \\begin{cases}\\beta_0 + \\beta_2 & i\\text{번째 사람이 학생인 경우} \\\\ \\beta_0 & i\\text{번째 사람이 학생이 아닌 경우}\\end{cases} \\end{eqnarray}이것은 두 개의 평행한 직선을 데이터에 적합하는 것이다. 학생과 학생이 아닌 사람에 대한 두 직선은 다른 절편 $\\beta_0 + \\beta_2$와 $\\beta_0$을 가지지만 동일한 기울기 $\\beta_1$을 가진다. 두 직선이 평행하다는 사실이 의미하는 것은 income의 한 유닛 증가가 balance에 미치는 평균 효과는 그 사람이 학생인지 아닌지에 의존적이지 않다는 것을 의미한다. 이것은 모델이 잠재적으로 심각한 한계가 있음을 나타낸다. 왜냐하면 소득의 변화는 학생과 학생이 아닌 사람의 신용카드 대금에 아주 다른 효과를 줄 수 있기 때문이다. 이 한계는 income을 student에 대한 가변수와 곱하여 얻은 상호작용 변수를 추가함으로써 해결할 수 있다. 그러면 모델은다음과 같이 표현된다. \\begin{eqnarray} \\text{balance}_i &\\approx& \\beta_0 + \\beta_1 \\times \\text{income}_i + \\begin{cases}\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{학생인 경우}\\\\ 0 & \\text{학생이 아닌 경우}\\end{cases}\\\\ \\\\ &=& \\begin{cases}(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & 학생인 경우 \\\\ \\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{학생이 아닌 경우}\\end{cases} \\end{eqnarray}이 경우에도 학생과 학생이 아닌 사람에 대한 회귀 직선이 다르다. 그러나 이번에는 두 직선의 절편 뿐만 아니라 기울기도 다르다. 학생인 경우, 회귀직선의 절편은 $\\beta_0 + \\beta_2$, 기울기는 $\\beta_1 + \\beta_3$이다. 학생이 아닌 경우에는 절편은 $\\beta_0$, 기울기는 $\\beta_1$이다. 이것은 소득 변화가 신용카드 대금에 미치는 영향이 학생인지의 여부에 따라 다를 수 있게 한다. 학생에 대한 기울의 기울기가 학생이 아닌 경우에 대한 것보다 작은데, 이것은 소득 증가에 따른 카드 대금의 증가가 학생인 경우 학생이 아닌 사람보다 낮다는 것을 시사한다. 비선형 상관관계","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Regression","slug":"Math/Regression","permalink":"https://p829911.github.io/categories/Math/Regression/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Regression","slug":"Regression","permalink":"https://p829911.github.io/tags/Regression/"}]},{"title":"git 설치 & 사용법","slug":"git-install-and-how-to-use","date":"2018-12-03T12:58:52.000Z","updated":"2018-12-19T05:45:28.000Z","comments":true,"path":"2018/12/03/git-install-and-how-to-use/","link":"","permalink":"https://p829911.github.io/2018/12/03/git-install-and-how-to-use/","excerpt":"","text":"git은 터미널에서 다음과 같은 명렁어로 설치 할 수 있다.1sudo apt-get install git 설치가 완료 되었으면 1git --version git config 설정 1234git config --global user.name \"username\"git config --global user.email \"github email address\"git config --global core.editor \"vim\"git config --list local에 있는 폴더와 git 저장소 연동시키기 폴더 생성 후 git init 해주기 123git initgit add.git commit -m \"contents\" git repository 생성 해 준 후 12git remote add origin https://github.com/username/repo.gitgit push origin master 위의 코드에서 origin이라는 부분은 사용자가 지정할 수 있지만, 보통 origin이라고 써줌 git 에서 저장소를 clone 할 때는 자동적으로 origin이라고 지정 된다. git 저장소 local 폴더로 복제하기1git clone https://github.com/username/repo.git 개인적으로는 git remote 보다 git clone해서 사용하는 것이 더 편하다. branch소프트웨어를 개발할 때 개발자들은 동일한 소스코드를 함께 공유한다. 동일한 소스코드 위에서 서로 다른 작업을 할 때는 각각 서로 다른 버전의 코드가 만들어 질 수 밖에 없다.이럴 때, 여러 개발자들이 동시에 다양한 작업을 할 수 있게 만들어 주는 기능이 바로 ‘브랜치(Branch)’이다.이렇게 분리된 작업 영역에서 변경된 내용은 나중에 원래의 버전과 비교해서 하나의 새로운 버전으로 만들어 낼 수 있다. 브랜치란 독립적으로 어떤 작업을 진행하기 위한 개념이다. 필요에 의해 만들어지는 각각의 브랜치는 다른 브랜치의 영향을 받지 않기 때문에, 여러 작업을 동시에 진행할 수 있다. 또한 이렇게 만들어진 브랜치는 다른 브랜치와 병합(Merge)함으로써, 작업한 내용을 다시 새로운 하나의 브랜치로 모을 수 있다. 저장소를 처음 만들면, Git은 바로 ‘master’라는 이름의 브랜치를 만들어 준다. 이 새로운 저장소에 새로운 파일을 추가한다거나 추가한 파일의 내용을 변경하여 그 내용을 저장(커밋, commit)하는 것은 모두 ‘master’라는 이름의 브랜치를 통해 처리할 수 있다. 12345678git branch &lt;branchname&gt; # 브랜치 생성git branch # 브랜치 확인git branch -r # remote 브랜치 확인git branch -a # 모든 사용가능한 브랜치 확인git checkout &lt;branchname&gt; # 브랜치 지정git checkout -b &lt;branchname&gt; # 브랜치 생성, 체크아웃git merge &lt;branchname&gt; # 브랜치 병합git branch -d &lt;branchname&gt; # 브랜치 삭제 참고 https://backlog.com/git-tutorial/kr/stepup/stepup1_1.html https://github.com/ulgoon/dss-linux-git","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"ubuntu permission","slug":"ubuntu-permission","date":"2018-11-27T08:44:04.000Z","updated":"2018-11-28T09:35:45.000Z","comments":true,"path":"2018/11/27/ubuntu-permission/","link":"","permalink":"https://p829911.github.io/2018/11/27/ubuntu-permission/","excerpt":"","text":"파일 정보 보기 ls -al: 현재 위치에 있는 파일들을 자세히 보여주는 명령 파일 Type 퍼미션정보 링크수 소유자 소유그룹 용량 생성날짜 파일이름 d rwxr-xr-x 28 p829911 p829911 4096 11월 27 20:30 .ipython 파일 Type: d (디렉토리), ㅣ (링크파일), - (일반파일) 퍼미션정보: 해당 파일에 어떠한 퍼미션이 부여되어 있는지 링크수: 해당 파일이 링크된 수, 윈도우의 바로가기와 같다. in [대상파일][링크파일]명령으로 만든다. 소유자: 해당 파일의 소유자 이름 소유그룹: 해당 파일을 소유한 그룹이름 용량: 파일의 용량 생성날짜: 파일이 생성된 날짜 파일이름: 파일의 이름 퍼미션 정보 앞에서 두번째부터 아홉번째까지 퍼미션 정보이다. rwxr-xr-x 퍼미션 종류 r : 파일의 읽기 권한 w : 파일의 쓰기 권한 x : 파일의 실행 권한 퍼미션의 사용자지정 소유자: 소유자에 대한 퍼미션 지정 rwx 그룹: 소유그룹에 대한 퍼미션 지정 r-x 공개: 모든 사용자들에 대한 퍼미션 지정 r-x -: 그 퍼미션은 없다 소유자는 읽기, 쓰기, 실행을 허용하고 파일의 소유그룹에 속하고 있는 사용자들은 읽기 실행만 허용하고 이외에 나머지 모든 사용자들도 읽기, 실행만 허용한다. 퍼미션 변경하기chmod [변경될 퍼미션값][변경할 파일] 각 퍼미션 기호를 숫자로 변환한다. ( r = 4, w = 2, x = 1)ex) r - x = 4 0 1 변환한 숫자를 합산한다.ex) 4 0 1 = 5 rwxr-xr-x = 755 chmod 775 [변경할 파일] : 변경할 파일이 755에 해당되는 퍼미션으로 변경된다. 디렉토리의 경우 -R 옵션을 사용하면 하위 디렉토리의 모든 디렉토리 및 파일의 퍼미션이 변경된다.ex) chmod -R 777 [변경할 디렉토리] 소유자 변경하기chown [변경할 소유자][변경할 파일] 이 명령으로 소유자뿐만 아니라 소유그룹도 변경할 수 있다. [변경할 소유자]에 .그룹이름 형식으로 입력하면 된다. ex) p829911.text 파일의 소유자를 p829911 소유그룹을 p829911로 동시에 변경할 경우chown p829911.p829911 p829911.text","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"ubuntu apt 명령어","slug":"ubuntu-apt-명령어","date":"2018-11-27T07:50:51.000Z","updated":"2018-11-27T08:05:17.000Z","comments":true,"path":"2018/11/27/ubuntu-apt-명령어/","link":"","permalink":"https://p829911.github.io/2018/11/27/ubuntu-apt-명령어/","excerpt":"","text":"우분투에서 패키지를 관리하는 명령어가 몇 가지 있다. 그 중 apt-get과 apt-cache를 결합한 apt에 관한 명령어를 알아보겠다. 패키지 목록 갱신 1apt update 모든 패키지를 최신 버전으로 업그레이드 12apt install upgradeapt full-upgrade # 의존성 고려한 패키지 업그레이드 패키지 설치 1apt install package_name 패키지 삭제 1apt remove package_name 패키지 삭제(설정 파일 포함) 1apt purge package_name 불필요한 패키지 제거 1apt autoremove 패키지 검색 1apt search package_name 패키지 상세 정보 출력 1apt show package_name apt 명령어 사용법 &amp; 옵션 1apt -h 패키지 리스트 출력 1apt list 권한 문제가 발생할 경우 sudo 명령을 붙여 root로 실행할 수 있다.","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"교차 검증","slug":"교차-검증","date":"2018-11-27T06:32:01.000Z","updated":"2018-12-13T11:23:22.000Z","comments":true,"path":"2018/11/27/교차-검증/","link":"","permalink":"https://p829911.github.io/2018/11/27/교차-검증/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 표본 내 성능과 표본 외 성능회귀분석 모형을 만들기 위해서는 모수 추정 즉 학습을 위한 데이터 집합이 필요하다. 보통 회귀분석 성능을 이야기할 때는 이 학습 데이터 집합의 종속 변수값을 얼마나 잘 예측하였는지를 결정 계수(codefficient of determination) 등을 이용하여 따진다. 이러한 성능을 표본 내 성능 검증(in-sample testing)이라고 한다. 그런데 회귀분석 모형을 만드는 목적 중 하나는 종속 변수의 값을 아직 알지 못하고 따라서 학습에 사용하지 않은 표본에 대해 종속 변수의 값을 알아내고자 하는 것 즉 예측(prediction)이다. 이렇게 학습에 쓰이지 않는 표본 데이터 집합의 종속 변수 값을 얼마나 잘 예측하는가를 검사하는 것을 표본 외 성능 검증(out-of-sample testing) 혹은 교차 검증(cross validation)이라고 한다. 과최적화일반적으로 표본 내 성능과 표본 외 성능은 비슷한 수준을 보이지만 경우에 따라서는 표본 내 성능은 좋으면서 표본 외 성능이 상대적으로 많이 떨어지는 수도 있다. 이러한 경우를 과최적화(overfitting)라고 한다. 과최적화가 발생하면 학습에 쓰였던 표본 데이터에 대해서는 종속변수의 값을 잘 추정하지만 새로운 데이터를 주었을 때 전혀 예측하지 못하기 때문에 예측 목적으로는 쓸모없는 모형이 된다. 검증용 데이터 집합교차 검증을 하려면 두 종류의 데이터 집합이 필요하다. 모형 추정 즉 학습을 위한 데이터 집합 (training data set) 성능 검증을 위한 데이터 집합 (test data set) 두 데이터 집합 모두 종속 변수값이 있어야 한다. 따라서 보통은 가지고 있는 데이터 집합을 학습용과 검증용으로 나누어 학습용 데이터만을 사용하여 회귀분석 모형을 만들고 검증용 데이터로 성능을 계산하는 학습/검증 데이터 분리(train-test split) 방법을 사용한다. statsmodels 패키지에서의 교차 검증사실 소수의 입력 변수와 소규모 데이터를 사용하는 전통적인 회귀분석에서는 다항 회귀 등의 방법으로 모형 차수를 증가시키지 않는 한 과최적화가 잘 발생하지 않는다. 따라서 statsmodels 패키지에는 교차 검증을 위한 기능이 별도로 준비되어 있지 않고 사용자가 직접 코드를 작성해야 한다. scikit-learn의 교차 검증 기능독립 변수의 개수가 많은 빅데이터에서는 과최적화가 쉽게 발생한다. 따라서 scikit-learn 의 model_selection 서브 패키지는 교차 검증을 위한 다양한 명령을 제공한다. 단순 데이터 분리train_test_split 명령은 데이터를 학습용 데이터와 검증용 데이터로 분리한다. 1train_test_split(data, data2, test_size, train_size, random_state) data: 독립 변수 데이터 배열 또는 pandas 데이터 프레임 data2: 종속 변수 데이터. data인수에 종속 변수 데이터가 같이 있으면 생략할 수 있다. test_size: 검증용 데이터 개수. 1보다 작은 실수이면 비율을 나타낸다. train_size: 학습용 데이터의 개수. 1보다 작은 실수이면 비율을 나타낸다. test_size와 train_size중 하나만 있어도 된다. random_state: 난수 시드 1234from sklearn.model_selection import train_test_splitdf_train, df_test = train_test_split(df, test_size=0.3, random_state=0)df_train.shape, df_test.shape 12dfx_train, dfx_test, dfy_train, dfy_test = train_test_split(dfx, dfy, test_size=0.3, random_state=0)dfx_train.shape, dfy_train.shape, dfx_test.shape, dfy_test.shape K- 폴드 교차 검증데이터의 수가 적은 경우에는 이 데이터 중의 일부인 검증 데이터의 수도 적기 때문에 검증 성능의 신뢰도가 떨어진다. 그렇다고 검증 데이터의 수를 증가시키면 학습용 데이터의 수가 적어지므로 정상적인 학습이 되지 않는다. 이러한 딜레마를 해결하기 위한 검증 방법이 K-폴드(K-fold) 교차 검증 방법이다. K-폴드 교차 검증에서는 전체 데이터를 K개의 부분집합($\\{1, 2, \\cdots , K\\}$)로 나눈 뒤 다음과 같이 학습과 검증을 반복한다. 데이터 $\\{1, 2, \\cdots, K - 1\\}$를 학습용 데이터로 사용하여 회귀분석 모형을 만들고 데이터 $\\{K\\}$ 로 교차 검증을 한다. 데이터 $\\{1, 2, \\cdots, K - 2, K\\}$를 학습용 데이터로 사용하여 회귀분석 모형을 만들고 데이터 $\\{K-1\\}$로 교차 검증을 한다. $\\vdots$ 데이터 $\\{2, \\cdots, K\\}$를 학습용 데이터로 사용하여 회귀분석 모형을 만들고 데이터 $\\{1\\}$로 교차 검증을 한다. 이렇게 하면 총 K개의 모형과 K개의 교차 검증 성능이 나온다. 이 K개의 교차 검증 성능을 평균하여 최종 교차 검증 성능을 계산한다. scikit-learn 패키지의 model_selection 서브 패키지는 KFold 클래스를 비롯한 다양한 교차 검증 생성기를 제공한다. 이 생성기의 split 메서드는 학습용과 검증용의 데이터 인덱스를 출력하는 파이썬 반복자(iterator)를 반환한다. 123456789101112131415161718from sklearn.model_selection import KFoldscores = np.zeros(5)cv = KFold(5, shuffle=True, random_state=0)for i, (idx_train, idx_test) in enumerate(cv.split(df)): df_train = df.iloc[idx_train] df_test = df.iloc[idx_test] model = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train) result = model.fit() pred = result.predict(df_test) rss = ((df_test.MEDV - pred) ** 2).sum() tss = ((df_test.MEDV - df_test.MEDV.mean()) ** 2).sum() rsquared = 1 - rss/tss scores[i] = rsquared print(\"train R2 = &#123;:.8f&#125;, test R2 = &#123;:.8f&#125;\".format(result.rsquared, rsquared))","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Regression","slug":"Math/Regression","permalink":"https://p829911.github.io/categories/Math/Regression/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Regression","slug":"Regression","permalink":"https://p829911.github.io/tags/Regression/"}]},{"title":"hexo mathjax","slug":"hexo-mathjax","date":"2018-11-26T14:36:53.000Z","updated":"2018-11-26T14:53:50.000Z","comments":true,"path":"2018/11/26/hexo-mathjax/","link":"","permalink":"https://p829911.github.io/2018/11/26/hexo-mathjax/","excerpt":"","text":"rendering engine changeHexo의 기본 renderer인 hexo-renderer-marked는 mathjax 문법을 지원하지 않는다. 따라서 mathjax를 지원하는 rendering engine으로 교체해준다. 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save &lt;blog dir&gt;/node_modules/hexo-renderer-kramed/lib/renderer.js를 열어 return값을 text로 수정한다. 12345function formatText(text) &#123; // Fit kramed's rule: $$ + \\1 + $$ // return text.replace(/`\\$(.*?)\\$`/g, '$$$$$1$$$$'); return text;&#125; install mathjaxmathjax plugin 설치 1npm install hexo-renderer-mathjax --save &lt;blog dir&gt;/node_modules/hexo-renderer-kramed/node_modules/hexo-renderer-mathjax/mathjax.html 을 열고 URL을 수정해준다. 12&lt;!-- &lt;script src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"&gt;&lt;/script&gt; --&gt;&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'&gt;&lt;/script&gt; LaTex와 markdown 문법 충돌 해결하기&lt;blog dir&gt;/node_modules/kramed/lib/rules/inline.js를 열고 다음과 같이 수정한다. 12escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, Mathjax 사용하기사용하고 있는 theme의 _config.yml파일을 열고 다음과 같이 수정한다. 12mathjax: enable: true markdown post 작성post 작성시 header 부분에 mathjax: true를 넣어주면 블로그에서 수식이 보이게 된다.","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Blog","slug":"Blog","permalink":"https://p829911.github.io/tags/Blog/"},{"name":"Mathjax","slug":"Mathjax","permalink":"https://p829911.github.io/tags/Mathjax/"}]},{"title":"분산 분석","slug":"분산-분석","date":"2018-11-26T14:21:57.000Z","updated":"2018-12-13T11:23:10.000Z","comments":true,"path":"2018/11/26/분산-분석/","link":"","permalink":"https://p829911.github.io/2018/11/26/분산-분석/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 선형회귀분석의 결과가 얼마나 좋은지는 단순히 잔차제곱합(RSS: Residual sum of square)으로 평가할 수 없다. 변수의 단위 즉, 스케일이 달라지면 회귀분석과 상관없이 잔차제곱합도 달라지기 때문이다. 분산 분석(ANOVA: Analysis of Variance)은 종속변수의 분산과 독립변수의 분산간의 관계를 사용하여 선형회귀분석의 성능을 평가하고자 하는 방법이다. 분산 분석은 서로 다른 두 개의 선형회귀분석의 성능 비교에 응용할 수 있으며 독립변수가 카테고리 변수인 경우 각 카테고리 값에 따른 영향을 정량적으로 분석하는데도 사용된다. $\\hat{y}$를 종속변수 $y$의 샘플 평균이라고 하자. \\hat{y} = \\frac{1}{N} \\sum_{i=1}^N y_iTSS (total sum of squares) \\text{TSS} = \\sum_{i=1}^N (y_i-\\bar{y})^2 = (y-\\bar{y})^T(y-\\bar{y})종속변수값의 움직임의 범위를 나타낸다. ESS(explained sum of squares) \\text{ESS} = \\sum_{i=1}^N (\\hat{y_i}-\\bar{\\hat{y}})^2 = (\\hat{y}-\\bar{\\hat{y}})^T(\\hat{y}-\\bar{\\hat{y}})회귀 분석에 의해 예측한 값 $\\hat{y}$의 분산을 나타낸다. 모형에서 나온 예측값의 움직임의 범위를 뜻한다. RSS (residual sum of squares) \\text{RSS} = \\sum_{i=1}^N(y_i-\\hat{y_i})^2 = e^Te잔차 $e$의 분산을 나타낸다. 잔차의 움직임의 범위, 즉 오차의 크기를 뜻한다. 만약 회귀모형이 상수항을 포함하여 올바르게 정의되었다면 잔차의 평균이 0이 된다. 즉 종속변수의 평균과 모형 예측값의 평균이 같아진다. \\bar{e} = \\bar{y} - \\bar{\\hat{y}} = 0 \\bar{y} = \\bar{\\hat{y}}그리고 이 분산값들 간에는 다음과 같은 관계가 성립한다. \\text{TSS} = \\text{ESS} + \\text{RSS}위 식이 말하는 바는 다음과 같다. 모형 예측치의 움직임의 크기(분산)은 종속변수의 움직임의 크기(분산)보다 클 수 없다. 모형의 성능이 좋을 수록 모형 예측치의 움직임의 크기는 종속변수의 움직임의 크기와 비슷해진다. example 123456789101112import numpyimport pandasfrom sklearn.datasets import make_regressionx0, y, coef = make_regression(n_samples=100, n_features=1, noise=30, coef=True, random_state=0)dfx0 = pd.DataFrame(x0, columns=[\"X\"])dfx = sm.add_constant(dfx0)dfy = pd.DataFrame(y, columns=[\"Y\"])df = pd.concat([dfx, dfy], axis=1)model = sm.OLS.from_formula(\"Y ~ X\", data=df)result = model.fit() 12345print(\"TSS = \", result.uncentered_tss)print(\"ESS = \", result.mse_model)print(\"RSS = \", result.ssr)print(\"ESS + RSS = \", result.mse_model + result.ssr)print(\"R squared = \", result.rsquared) 결정 계수(Coefficient of Determination) 위의 분산 관계식에서 모형의 성능을 나타내는 결정계수(Coefficient of Determination) $R^2$를 정의할 수 있다. R^2 \\equiv 1- \\dfrac{\\text{RSS}}{\\text{TSS}} = \\dfrac{\\text{ESS}}{\\text{TSS}}분산 관계식과 모든 분산값이 0보다 크다는 점을 이용하면 $R^2$의 값은 다음과 같은 조건을 만족한다. 0 \\leq R^2 \\leq 1여기에서 $R^2$가 0 이라는 것은 오차의 분산 RSS가 최대이고 회귀분석 예측값의 분산 ESS가 0인 경우이므로 회귀분석 결과가 아무런 의미가 없다는 뜻이다. 반대로 $R^2$가 1이라는 것은 오차의 분산 RSS가 0이고 회귀분석 예측의 분산 ESS가 TSS와 같은 경우이므로 회귀분석 결과가 완벽하다는 뜻이다. 따라서 결정계수값은 회귀분석의 성능을 나타내는 수치라고 할 수 있다. 분산 분석표 분산 분석의 결과는 보통 다음과 같은 분산 분석표를 사용하여 표시한다. 아래의 표에서 $N$은 데이터의 갯수, $K$는 모수(독립변수의 갯수, 상수항 포함)의 갯수를 뜻한다. source degree of freedom sum of square mean square F test-statistics p-value Regression $K -1$ $\\text{ESS}$ $s_\\hat{y}^2 = \\dfrac{\\text{ESS}}{K-1}$ $F = \\dfrac{s_\\hat{y}^2}{s_e^2}$ p-value Residual $N - K$ $\\text{RSS}$ $s_e^2 = \\dfrac{\\text{RSS}}{N-K}$ Total $N - 1$ $\\text{TSS}$ $s_y^2 = \\dfrac{\\text{TSS}}{N-1}$ $R^2$ $\\dfrac{\\text{ESS}}{\\text{TSS}}$ 결정 계수와 상관 계수 $y$와 $\\hat{y}$의 샘플 상관계수 $r$의 제곱은 결정 계수 $R^2$와 같다. 상수항이 없는 모형의 경우 모형에서 상수항을 지정하지 않은 경우에는 결정계수의 정의에 사용되는 TSS의 정의가 다음과 같이 달라진다. \\text{TSS} = \\sum_{i=1}^{N}y_i^2 = y^Ty즉, 실제 샘플평균과 상관없이 $\\bar{y} = 0$ 이라는 가정하에 TSS를 계산한다. 이렇게 정의하지 않으면 TSS = RSS + ESS 관계식이 성립하지 않아서 결정계수의 값이 1보다 커지게 된다. 따라서 모형의 결정계수를 비교할 때 상수항이 없는 모형과 상수항이 있는 모형은 직접 비교하면 안된다. F 검정을 이용한 모형 비교 F 검정을 이용하면 다음과 같이 포함관계에 있는 두 모형의 성능을 비교할 수 있다. 전체 모형(Full Model): y = w_0 + w_1x_1 + w_2x_2 + w_3x_3 축소 모형(Reduced Model): y = w_0 + w_1x_1 다음과 같은 귀무가설을 검정하는 것은 위의 두 모형이 실질적으로 같은 모형이라는 가설을 검정하는 것과 같다. H_0 : w_2 = w_3 = 0이 검정도 F검정을 사용하여 할 수 있다. StatsModels에서는 anova_lm명령에 두 모형의 result 객체를 인수로 넣어주면 이러한 검정을 할 수 있다. 인수를 넣어줄 때는 축소 모형(reduced model), 전체 모형(full model)의 순서로 넣어준다.","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Regression","slug":"Math/Regression","permalink":"https://p829911.github.io/categories/Math/Regression/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Regression","slug":"Regression","permalink":"https://p829911.github.io/tags/Regression/"}]},{"title":"Jupyber notebook matplotlib 한글 설정","slug":"Jupyber-notebook-matplotlib-한글-설정","date":"2018-11-26T13:25:40.000Z","updated":"2018-11-26T14:52:32.000Z","comments":true,"path":"2018/11/26/Jupyber-notebook-matplotlib-한글-설정/","link":"","permalink":"https://p829911.github.io/2018/11/26/Jupyber-notebook-matplotlib-한글-설정/","excerpt":"","text":"우분투 폰트 경로 /usr/share/fonts/ 나눔 글꼴 또는 다른 폰트도 /usr/share/fonts/ 폴더에 복사에서 사용가능하다. 나눔글꼴 설치 12sudo apt-get install fonts-nanum*sudo fc-cache -fv apt-get 명령으로 나눔글꼴 설치 후, fc-cache 명령으로 폰트 캐시 삭제 다른 ttf 폰트 12sudo cp new_font.ttf / usr/share/fontssudo fc-cache -fv 우분투 폰트 경로로 ttf폰트 복사 후, fc-cache 명령으로 폰트 캐시 삭제 matplotlib 폴더에 글꼴 추가 123sudo cp /usr/share/fonts/truetype/D2Coding/D2* /home/p829911/.local/lib/python3.6/site-packages/matplotlib/mpl-data/rm -rf /home/ubuntu/.cache/matplotlib/* matplotlib 폴더에 글꼴을 복사 한 후 matplotlib의 폰트 캐시를 삭제 12# 캐쉬 디렉토리matplotlib.get_cachedir() 내 컴퓨터에 저장되어 있는 폰트 리스트 가져오기 123456789import matplotlib.font_manager as fmfont_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')# 전체개수print(len(font_list)) # 처음 10개만 출력font_list[:10] 사용가능한 시스템의 TTF 폰트 목록 123456import matplotlib.font_manager as fmfont_list = [(f.name, f.fname) for f in fm.fontManager.ttflist]print(len(font_list))font_list[:10] 내가 원하는 D2Coding 폰트의 저장 위치를 불러오기 123for font in font_list: if \"D2\" in font[0]: print(font) rcParams 를 설정 파일에 직접 적어주기 - 모든 노트북에 공통 적용 font.family: D2Coding 이곳에 폰트를 지정해 주면 노트북을 실행 할 때 바로 로드되도록 설정할 수 있다. 1print(matplotlib.matplotlib_fname()) 1vi /home/p829911/.local/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc matplotlibrc파일에서 font.family를 D2Coding으로 설정해준다. 1234567891011import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinedata = np.random.randint(-100, 100, 50).cumsum()dataplt.plot(range(50), data, 'r')plt.title('가격변동 추이')plt.ylabel('가격')plt.show()","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://p829911.github.io/tags/Jupyter/"}]},{"title":"Jupyter notebook 글꼴 설정","slug":"Jupyter-notebook-글꼴-설정","date":"2018-11-26T10:51:23.000Z","updated":"2018-11-26T14:52:46.000Z","comments":true,"path":"2018/11/26/Jupyter-notebook-글꼴-설정/","link":"","permalink":"https://p829911.github.io/2018/11/26/Jupyter-notebook-글꼴-설정/","excerpt":"","text":"D2Coding 설치 12345cd /home/username/.jupytermkdir customcd customtouch custom.cssvi custom.css 만약 custom.css에 쓰기 권한이 없으면 chmod명령으로 파일 권한을 바꿔준다. 1sudo chmod 777 custom.css vi 편집기로 custom.css 파일을 연 후 다음과 같이 설정 해 준다. 1.CodeMirror pre &#123;font-family: D2Coding; font-size: 12pt; line-height: 120%;&#125; jupyter notebook을 실행하면 글꼴이 D2Coding으로 바뀐 것을 볼 수 있다.","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://p829911.github.io/tags/Jupyter/"}]},{"title":"git blog 관리하기","slug":"git-blog-관리하기","date":"2018-11-26T10:50:56.000Z","updated":"2018-12-13T14:11:23.000Z","comments":true,"path":"2018/11/26/git-blog-관리하기/","link":"","permalink":"https://p829911.github.io/2018/11/26/git-blog-관리하기/","excerpt":"","text":"새 저장소(repository) 만들기Github에서 새 저장소(repository)를 만든다. 이 때 저장소의 이름을 자신의 username뒤에 .github.io가 붙은 이름으로 만든다. 이렇게 만들어 줘야 username.github.io의 도메인으로 접속할 수 있는 블로그가 된다. Hexo 설치하기git과 node.js는 설치돼 있어야 한다. 12sudo apt install npmnpm install -g hexo-cli # 오류 날 시 앞에 sudo를 붙여준다 1hexo -v 위의 명령어로 hexo가 제대로 설치 되었는지 확인 한다. Hexo 설치가 완료되었으면 다음과 같은 명령어를 입력해서 Hexo 디렉토리를 초기화한다. 1hexo init &lt;디렉토리명&gt; 설치가 모두 잘 되었다면 다음 명령어를 입력해서 내장 서버를 돌릴 수 있다. 123hexo server# 서버 실행 후 창 오픈hexo s -o 브라우저에서 http://0.0.0.0:4000/ 으로 접속해서 확인 할 수 있다. deployHexo가 설치된 디렉토리로 가서 _config.yml 파일을 열어 Site, URL, Deployment 항목을 수정해준다. 그리고 정적 파일을 생성한다. 1hexo generate 디플로이를 하기 위해서는 hexo-deployer-git 플러그인이 필요하다. 아래의 명령어를 사용해서 설치한다. 1npm install --save hexo-deployer-git 생성이 잘 되었다면 디플로이 명령어를 사용한다. 1hexo deploy Hueman 테마 적용하기블로그 루트 폴더에서 명령어로 테마를 받는다. 1git clone https://github.com/ppoffice/hexo-theme-hueman.git themes/hueman blog 폴더에 있는 _config.yml에서 Theme부분을 landscape에서 hueman으로 바꿔준다. themes/hueman 폴더에 있는 _config.yml.example을 _config.yml로 바꾼다. 12cd blog/themes/huemanmv _config.yml.example _config.yml 최신 버전을 다운받기 위해 pull해준다. 12cd themes/huemangit pull Hueman 테마의 Insight Search 검색엔진을 사용하기 위해 npm으로 hexo-generator-json-content을 설치한다. 1npm install -S hexo-generator-json-content hueman의 테마는 hueman폴더 안에 있는 _config.yml에서 설정할 수 있다. 포스트 작성하기1hexo new post \"post name\" 그러면 [blogFolder]/source/_posts에 새로운 마크다운 파일이 생성된다.자동으로 제목과 생성날짜가 들어간다. 글을 마크다운 파일로 작성 한 후 12hexo generatehexo deploy or 123hexo generate --deploy# 단축키hexo g -d 테마가 적용 안되는 경우12hexo cleanhexo generate --deploy","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Blog","slug":"Blog","permalink":"https://p829911.github.io/tags/Blog/"}]},{"title":"우분투 Root 비밀번호 설정","slug":"우분투-Root-비밀번호-설정","date":"2018-11-26T10:46:43.000Z","updated":"2018-11-26T11:07:04.000Z","comments":true,"path":"2018/11/26/우분투-Root-비밀번호-설정/","link":"","permalink":"https://p829911.github.io/2018/11/26/우분투-Root-비밀번호-설정/","excerpt":"","text":"우분투를 설치하면 기본으로 Root 비밀번호가 없는 상태이다. 아래와 같은 방법으로 root 비밀번호를 설정해본다. root 비밀번호 설정 1sudo passwd 위와 같이 비밀번호를 설정 했다면 1su 명령을 통해 root에 로그인 할 수 있다. passwd 1passwd 현재 로그인한 사용자 계정의 비밀번호를 변경할 수 있는 명령어이다. root에서 user 비밀번호 변경 1passwd p829911 root에서 p829911이라는 사용자의 비밀번호를 변경하고 싶을 때 사용하는 명렁어이다.","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"우분투 한글 입력기 설치","slug":"우분투-한글-입력기-설치","date":"2018-11-24T18:14:50.000Z","updated":"2018-11-24T18:51:55.000Z","comments":true,"path":"2018/11/25/우분투-한글-입력기-설치/","link":"","permalink":"https://p829911.github.io/2018/11/25/우분투-한글-입력기-설치/","excerpt":"","text":"한글 설치 fcitx-hangul 설치 1sudo apt-get install fcitx-hangul System Setting (설정) &gt; Language Support(언어 지원) 을 실행해서 설치되지 않은 언어팩 모두 설치한다. 키보드 입력기를 ibus에서 fcitx로 변경한다. 재부팅 시 오른쪽 위에 아래의 첫번째에서 보는 것과 같은 아이콘이 생성된 것을 볼 수 있다. 한영 변환 설정 &gt; 장치 &gt; 키보드 로 들어간 뒤 입력 중의 다음 입력소스로 전환, 이전 입력소스로 전환을 사용 않음으로 바꿔준다. 사용 않음으로 바꿔주기 위해선 클릭 후 backspace를 누르면 된다. 상단 메뉴바 오른쪽의 입력기 선택(위 그림에서 세번째) 후 현재 입력기 설정 클릭 Keyboard-English(US)가 있다면 + 를 눌러 Hangul을 추가한다. (uncheck “Only Show Current Language”). Korean이 아닌 Hangul을 선택한다. 전역 설정 &gt; 단축키 &gt; 입력기 전환에 ‘Shift + Space’를 추가한다. 전역 설정 &gt; 프로그램 윈도우 사이에 상태 공유를 ‘모두’로 바꿔준다","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"우분투 데이터분석 환경설정","slug":"우분투-데이터분석-환경설정","date":"2018-11-24T17:25:25.000Z","updated":"2018-11-24T18:12:21.000Z","comments":true,"path":"2018/11/25/우분투-데이터분석-환경설정/","link":"","permalink":"https://p829911.github.io/2018/11/25/우분투-데이터분석-환경설정/","excerpt":"","text":"우분투 버전: Ubuntu 18.04.1 LTS 이 글은 데이터 분석을 공부하면서 window 사용자가 우분투에 데이터 분석 환경 설정 하며 겪은 시행착오와 그 단계들을 모아둔 글입니다. 오류와 path충돌 때문에 눈물을 머금고 우분투를 4번 정도 다시 깔면서 다음에 다시 설치해야 할 상황이 왔을 때 참고하기 위한 글이고, 우분투로 처음 데이터 분석 환경 설정을 하려고 하는 분들을 위해 정리하는 글입니다. 이 포스트와 다음과 같은 내용이 포함되어 있습니다. 한글 입력기 설치 root 비밀번호 설정 패키지 관리 툴 apt 사용법 슬랙 설치 vim 설치 git 설치 python3 설치 및 기본 설정 python3 데이터 분석 관련 패키지 설치 markdown 편집기 typora 설치 Atom &amp; Atom package 설치 mysql &amp; mysql workbench 설치 AWS(Amazon Web Services) 가입 및 접속 AWS와 파일 주고 받기를 할 수 있는 FileZilla 설치 및 사용법","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]}]}
{"meta":{"title":"data science study blog","subtitle":"Data Science Study Blog","description":null,"author":"Seungwoo Hyun","url":"https://p829911.github.io"},"pages":[],"posts":[{"title":"ubuntu virtualbox 용량 늘리기","slug":"ubuntu-virtualbox-용량-늘리기","date":"2020-01-04T08:45:38.000Z","updated":"2020-01-04T08:47:49.973Z","comments":true,"path":"2020/01/04/ubuntu-virtualbox-용량-늘리기/","link":"","permalink":"https://p829911.github.io/2020/01/04/ubuntu-virtualbox-용량-늘리기/","excerpt":"","text":"ubuntu에서 virtualbox로 window를 쓰고 있다.window를 쓰는 와중에 하드디스크 용량이 부족하여 용량을 늘리고 싶어 virtualbox 설정을 이것저것 만져보았지만virtualbox 내의 설정 부분에서는 불가능하다는 것을 알았다. 1VBoxManage modifyhd window10.vdi --resize 60000 다음과 같은 커맨드라인 명령어로 사이즈를 바꿀 수 있다고 하는데, 적용하고자 하는 vdi 이미지가 고정 용량으로 되어있으면 위와 같은 결과가 나타난다. 우선 고정용량 vdi 파일을 불러와 동적 용량 vdi 파일로 복사, 저장한 후 동적 용량 vdi 파일을 VBoxMange 를 이용해 용량 변경을 한다. 용량을 늘릴 때만 가능, 용량을 줄일때는 아래와 같은 커맨드로 줄인다. 1VBoxManage modifyhd aa.vdi --compact vdi 파일일 때만 가능 vmdk 포맷이라면 아래와 같은 커맨드로 vdi로 바꿔준다. 1VBoxManage clonehd window10.vmdk window10.vdi --format vdi 마지막으로 virtualbox storage에서 용량을 늘린 vdi파일로 바꿔준다음, window에서 용량을 늘려준다 내컴퓨터 - 오른쪽마우스 클릭 - 관리 - 저장소_디스크관리 - 볼륨확장 - 용량 선택 적용","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"},{"name":"Virtualbox","slug":"Virtualbox","permalink":"https://p829911.github.io/tags/Virtualbox/"}]},{"title":"3.1.2 계수 추정값의 정확도 평가","slug":"3.1.2 계수 추정값의 정확도 평가","date":"2020-01-04T08:45:15.000Z","updated":"2020-01-04T08:46:58.447Z","comments":true,"path":"2020/01/04/3.1.2 계수 추정값의 정확도 평가/","link":"","permalink":"https://p829911.github.io/2020/01/04/3.1.2 계수 추정값의 정확도 평가/","excerpt":"","text":"$X$와 $Y$의 실제(true) 선형관계는 어떤 알려지지 않은 함수 $f$에 대해 $Y = f(x) + \\epsilon$의 형태를 가지며, $\\epsilon$은 평균이 영인 랜덤오차항이다. 만약 $f$가 선형함수로 근사된다면 이 관계를 다음과 같이 나타낼 수 있다. Y = \\beta_0 + \\beta_1X + \\epsilon여기서, $\\beta_0$는 절편 즉, $X = 0$일 때 $Y$의 기댓값이고 $\\beta_1$은 기울기 즉, $X$의 한 유닛 증가에 연관된 $Y$의 평균 증가이다.오차항은 이러한 단순한 모델로 나타낼 때 수반되는 여러 가지 한계를 위한 것이다. Advertising 자료에서 sales를 반응변수로 TV를 설명변수로 사용한 RSS의 등고선과 3차원 그래프. 붉은색 점은 최소제곱 추정치 $\\beta_0$ 와 $\\beta_1$에 해당한다. 예를 들어, $X$와 $Y$의 실제 관계는 아마도 선형적이지 않을 수 있고, $Y$ 값의 변화를 초래하는 다른 변수들이 있을 수 있으며, 측정 오차가 있을 수 있다. 오차항은 보통 $X$와 독립이라고 가정한다. $ Y=\\beta_0+\\beta_1X+\\epsilon$ 의 모델은 모회귀선(population regression line) 을 정의하며, $X$와 $Y$의 실제 상관관계에 가장 잘 맞는 선형근사이다. 최소제곱회귀계수의 추정치 ($\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$, $\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}$ )는 최소제곱직선 ($\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x$ )을 결정한다. 위 그림의 왼쪽 패널은 간단한 모의 데이터를 이용해 이러한 두 직선을 나타낸다. 100개의 $X$ 값을 임의로 생성하고 아래 모델로부터 100개의 대응하는 $Y$ 값을 생성하였다. Y = 2 + 3X + \\epsilon여기서, $\\epsilon$은 평균이 영인 정규분포로부터 생성되었다. 위 그림에서 왼쪽 패널의 붉은색 직선은 실제 상관관계 $f(x) = 2 + 3X$를 나타낸 것이고, 푸른색 직선은 관측된 데이터에 근거한 최소제곱 추정값이다. 실재하는 데이터의 경우, 실제 상관관계는 일반적으로 알려져 있지 않지만 최소제곱선은 계수추정값을 사용하여 항상 계산할 수 있다. 다시 말하면, 실제 응용에서는 관측 자료를 사용하여 최소제곱선을 계산할 수 있다. 하지만 모회귀선은 관측되지 않는다. 위 그림의 오른쪽 패널은 $Y = 2 + 3X + \\epsilon$ 의 모델을 사용하여 생성한 10개의 서로 다른 데이터셋에 대응하는 10개의 최소제곱선을 도시한 것이다. 언뜻 보기에 모회귀선과 최소제곱선 사이의 차이는 매우 작고 구별하기 어려울 수 있다. 자료가 하나밖에 없는데 두 개의 다른 직선이 설명변수와 반응변수의 상관관계를 기술하는 것은 무엇을 의미하는가?근본적으로 이 두 직선의 개념은 표본의 정보를 사용하여 큰 모집단의 특징을 추정하는 표준통계적 방법의 자연스러운 확장이다.예를 들어, 어떤 확률변수 $Y$의 모평균 $\\mu$를 알고자 한다고 할 때, 합리적인 추정값은 $\\hat{\\mu} = \\bar{y}$ 이고, 여기서 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i$는 표본 평균이다. 표본평균과 모평균은 다르지만 일반적으로 표본평균은 모평균의 좋은 추정값이 된다. 확률변수 $Y$의 모평균 $\\mu$의 추정에 대한 비유를 해보자. 표본평균 $\\hat{\\mu}$ 이 $\\mu$의 추정값으로 얼마나 정확한가?많은 수의 데이터셋에 대한 $\\hat{\\mu}$의 평균은 $\\mu$에 아주 근접하지만, 하나의 추정값 $\\hat{\\mu}$은 $\\mu$를 상당히 과소추정 또는 과대추정 할 수 있다. 하나의 추정값 $\\hat{\\mu}$는 $\\mu$와 얼마나 다를 것인가? 일반적으로 이 질문에 대한 답은 $SE(\\hat{\\mu})$로 표현하는 $\\hat{\\mu}$의 표준오차를 계산하는 것이다. 표준오차에 대한 잘 알려진 식은 아래와 같다. \\text{Var}(\\hat{\\mu}) = \\text{SE}(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}여기서 $\\sigma$는 $Y$의 값 $y_i$의 표준편차이다. 대체로 표준오차는 추정값 $\\hat{\\mu}$이 $\\mu$의 실제값과 평균적으로 어느 정도 다른지를 말한다. 위의 식은 또한 $n$이 증가함에 따라 이 편차가 얼마나 줄어드는지를 말해준다. 관측치의 수가 많을 수록 $\\hat{\\mu}$의 표준오차가 작아진다. 유사한 맥락으로 $\\hat{\\beta_0}$와 $\\hat{\\beta_1}$이 얼마나 $\\beta_0$와 $\\beta_1$에 근접할 수 있는지 궁금할 수 있다. $\\beta_0$와 $\\beta_1$의 표준오차를 계산하기 위해서는 다음 식을 사용한다. \\begin{eqnarray} \\text{SE}(\\hat{\\beta_0})^2 &=& \\sigma^2\\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\right] \\\\ \\text{SE}(\\hat{\\beta_1})^2 &=& \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\end{eqnarray}여기서 $\\sigma^2 = \\text{Var}(\\epsilon)$ 이다. 이 식들이 유효하려면 각 관측치에 대한 오차 $\\epsilon_i$가 공통의 분산 $\\sigma^2$과 무상관(uncorrelated)이라는 가정이 필요하다. 위 식에서 $\\text{SE}(\\hat{\\beta_1})$은 $x_i$가 넓게 퍼질수록 더 작아진다. 직관적으로 이 경우에는 기울기를 추정할 레버리지(leverage)가 더 많다. 또한, 만약 $\\bar{x}$가 0이면(이 경우 $\\hat{\\beta_0}$은 $\\bar{y}$와 동일할 것이다.) $\\text{SE}(\\hat{\\beta_0})$ 은 $\\text{SE}(\\hat{\\mu})$와 동일하게 될 것이라는 것을 알 수 있다. 일반적으로 $\\sigma^2$은 알려져 있지 않지만 데이터로부터 추정할 수 있다. $\\sigma$의 추정치는 잔차표준오차로 알려져 있으며 $\\text{RSE} = \\sqrt{\\text{RSS} / (n-2)}$로 구해진다. 엄밀히 말해, $\\sigma^2$이 추정될 때 추정값이라는 것을 나타내기 위해 $\\widehat{\\text{SE}}(\\hat{\\beta_1})$으로 표현해야 한다. 표준오차는 신뢰구간을 계산하는 데 사용될 수 있다. 신뢰구간은 값의 범위로 정의되며, 95% 신뢰구간은 이 값의 범위가 95%의 확률로 파라미터의 알려지지 않은 실제값을 포함하게 될 것이다. 이러한 범위는 데이터 표본으로부터 계산된 하한값과 상한값으로 정의된다. 선형회귀의 경우, $\\beta_1$에 대한 95% 신뢰구간은 대략 아래와 같은 형태를 가진다. \\hat{\\beta_1} \\pm 2\\cdot\\text{SE}(\\hat{\\beta_1})즉, 아래의 구간은 대략 95%의 확률로 $\\beta_1$의 실제값을 포함할 것이다. \\hat{\\beta_1} - 2 \\cdot \\text{SE}(\\hat{\\beta_1}),\\;\\; \\hat{\\beta_1} + 2 \\cdot \\text{SE}(\\hat{\\beta_1})마찬가지로, $\\beta_0$에 대한 신뢰구간은 대략 다음의 형태를 가진다. \\hat{\\beta_0} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_0})앞의 광고 데이터에서 $\\beta_0$에 대한 95% 신뢰구간은 $[6.130, 7.935]$이고 $\\beta_1$에 대한 95% 신뢰구간은 $[0.042, 0.053]$이다. 그러므로, 광고를 전혀 하지 않으면 평균 판매량은 6,130과 7,935대 사이의 어떤 값으로 떨어진다고 결론을 내릴 수 있다. 더불어, TV 광고 투자가 매 1천 달러 증가할 경우 판매량은 평균 42와 53대 사이의 어떤 값만큼 증가할 것이다. 표준오차는 또한 계수들에 대한 가설검정을 하는데 사용될 수 있다. 가장 흔히 사용되는 가설검정은 귀무가설(null hypothesis)과 대립가설(alternative hypothesis)을 검정한다. 귀무가설이 아래와 같이 표현된다고 하자. H_0 : X \\text{와} \\;Y \\text{사이에 상관관계가 없다}그리고 대립가설은 다음과 같다. H_a: X \\text{와} \\;Y \\text{사이에 어떤 상관관계가 있다}수학적으로 이것은 $H_0 : \\beta_1 = 0$ 인지 $H_a : \\beta_1 \\neq 0$ 인지를 검정하는 것과 같다. 만약 $\\beta_1 = 0$ 이면 모델 $Y=\\beta_0+\\beta_1X+\\epsilon$ 은 $Y=\\beta_0 + \\epsilon$ 이 되므로 $X$는 $Y$와 관련이 없다. 귀무가설을 검정하려면 $\\beta_1$이 영이 아니라고 확신할 수 있을만큼 $\\beta_1$에 대한 추정값 $\\hat{\\beta_1}$이 영과 충분히 다른지를 결정해야 한다. 영과 얼마나 다른 것이 충분한가? 물론 이것은 $\\hat{\\beta_1}$의 정확도에 따라 다르다. 즉, 이것은 $\\text{SE}(\\hat{\\beta_1})$에 따라 다르다. 만약 $\\text{SE}(\\hat{\\beta_1})$이 작으면 $\\hat{\\beta_1}$이 비교적 작아도 $\\beta_1 \\neq 0$ 이고 따라서 $X$와 $Y$는 서로 상관되어 있다는 강한 증거가 될 수 있다. 실제로는 아래와 같이 주어지는 t-통계량을 계산한다. t = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})}위 식은 $\\hat{\\beta_1}$ 이 영이 아닌 표준편차의 수를 측정한다. 만약 $X$와 $Y$ 사이에 아무 상관관계가 없으면 위의 식은 자유도가 $n-2$인 t-분포를 가질 것이다. t-분포는 종모양을 가지며 $n$이 대략 30보다 크면 정규분포와 아주 유사하다. 따라서 $\\beta_1 = 0$이라고 가정하면 어떤 값이 $|t|$와 같거나 큰 경우를 관측할 확률을 계산하는 것은 간단하다. 이 확률을 p-값이라고 한다. p-값이 작다는 것은 설명변수와 반응변수 사이에 어떠한 실질적인 상관성이 없는데도 우연에 의해 의미있는 상관성이 관측될 가능성이 거의 없음을 나타낸다. 그러므로 만약 p-값이 작으면 설명변수와 반응변수에 상관성이 있다고 유추할 수 있다. 만약 p-값이 충분히 작으면, 귀무가설을 기각하고 $X$와 $Y$ 사이에 상관관계가 있다고 한다. 귀무가설을 기각하기 위한 전형적인 p-값은 5% 또는 1%이며, n=30인 경우 위의 식의 t-통계량으로 약 2와 2.75에 각각 해당한다. 위의 표는 Advertising 자료에서 TV 광고예산에 따른 판매량의 최소제곱회귀모델에 대한 상세사항을 나타낸 것이다.표를 살펴보면, $\\hat{\\beta_0}$와 $\\hat{\\beta_1}$에 대한 계수들은 그들의 표준오차에 비해 상당히 큰 값이며, 그래서 t-통계량도 크다. 만약 $H_0$이 참이면 이러한 값을 관측할 확률은 거의 영이다. 그러므로 $\\beta_0 \\neq 0$ 이고 $\\beta_1 \\neq 0$ 이라고 결론을 내릴 수 있다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Linear Regression","slug":"Linear-Regression","permalink":"https://p829911.github.io/tags/Linear-Regression/"},{"name":"RSS","slug":"RSS","permalink":"https://p829911.github.io/tags/RSS/"},{"name":"RSE","slug":"RSE","permalink":"https://p829911.github.io/tags/RSE/"}]},{"title":"선형회귀","slug":"3.선형회귀","date":"2019-12-18T11:54:45.000Z","updated":"2019-12-18T12:40:24.283Z","comments":true,"path":"2019/12/18/3.선형회귀/","link":"","permalink":"https://p829911.github.io/2019/12/18/3.선형회귀/","excerpt":"","text":"3. 선형회귀선형회귀에서 답을 찾아봐야하는 몇 가지 중요한 질문들 광고예산과 판매 사이에 상관관계가 있는가? 광고예산과 판매 사이에 얼마나 강한 상관관계가 있는가? 어느 매체가 판매에 기여하는가? 판매에 대한 각 매체의 효과를 얼마나 정확하게 추정할 수 있는가? 미래의 판매에 대해 얼마나 정확하게 예측할 수 있는가? 상관관계는 선형인가? 광고 매체 사이에 시너지 효과가 있는가? 회귀 분석의 표준 가정 오차항은 모든 독립변수 값에 대하여 동일한 분산을 갖는다. 오차항의 평균 (기대값) 은 0이다. 수집된 데이터의 확률 분포는 정규분포를 이루고 있다. 독립변수 상호 간에는 상관관계가 없어야 한다. 시간에 따라 수집한 데이터들은 잡음의 영향을 받지 않아야 한다. 3.1 단순 선형회귀단순선형회귀는 매우 간단한 기법으로, 하나의 설명변수 $X$에 기초하여 양적 반응변수 $Y$를 예측한다.이 기법은 $X$와 $Y$ 사이에 선형적 상관관계가 있다고 가정한다. 수학적으로 선형적인 상관관계는 다음과 같이 나타낼 수 있다. Y \\approx \\beta_0 + \\beta_1X \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(3.1)위의 식은 $X$에 대한 $Y$의 회귀라고 한다. $\\beta_0$와 $\\beta_1$은 알려지지 않은 상수로, 선형모델의 절편(intercept)과 기울기를 나타내며 모델 계수 또는 파라미터로 알려져 있다. 훈련 데이터를 사용하여 모델 계수에 대한 추정치 $\\hat{\\beta_0}$와 $\\hat{\\beta_1}$ 을 구하면, $\\hat{y}$를 다음과 같이 예측할 수 있다. \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x3.1.1 계수 추정실제로 $\\beta_0$과 $\\beta_1$은 알려져 있지 않다. 그러므로 (3.1)의 식을 사용하여 예측하기 전에 데이터를 이용하여 계수를 추정해야 한다. 다음은 $X$와 $Y$ 측정값으로 구성된 $n$개의 관측치 쌍을 나타낸다고 하자. (x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n)여기서의 목적은 선형모델(3.1)이 이용가능한 데이터에 잘 적합되도록하는, 즉 $i = 1, \\cdots, n$ 에 대해 $y_i \\approx \\hat{\\beta_0} + \\hat{\\beta_1}x_i$ 가 되도록 하는 계수 추정값 $\\hat{\\beta_0}$와 $\\hat{\\beta_1}$을 얻는 것이다. 다시 말하면, 직선이 $n$ 개의 데이터 포인트에 가능한 한 가깝게 되도록 하는 절편과 기울기를 찾고자 한다. 가까움(closeness)을 측정하는 방법은 여러 가지가 있다. 하지만 가장 흔하게 사용되는 기법은 최소제곱 기준을 최소화 하는 것이다. $X$의 $i$번째 값에 기초한 $Y$의 예측값을 $\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i$ 라고 하자. 그러면 $e_i = y_i - \\hat{y_i}$는 $i$ 번째 잔차(residual)을 나타내며, 이것은 $i$ 번째 관측된 반응변수 값과 선형모델에 의해 예측된 $i$ 번째 반응변수 값 사이의 차이다. 잔차제곱합(residual sum of squares (RSS)) 은 다음과 같이 정의한다. \\text{RSS} = e_1^2 + e_2^2 + \\cdots + e_n^2이것은 또한 아래와 같이 쓸 수 있다. \\text{RSS} = (y_1-\\hat{\\beta_0}-\\hat{\\beta_1}x_1)^2 + (y_2 - \\hat{\\beta_0}-\\hat{\\beta_1}x_2)^2 + \\cdots + (y_n - \\hat{\\beta_0}-\\hat{\\beta_1}x_n)^2최소제곱법은 RSS를 최소화 하는 $\\beta_0$와 $\\beta_1$를 선택한다.미적분을 사용하여 수식을 정리하면 다음을 얻을 수 있다. \\begin{eqnarray} \\hat{\\beta_1} &=& \\dfrac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\ \\hat{\\beta_0} &=& \\bar{y} - \\hat{\\beta_1}\\bar{x} \\end{eqnarray} $\\hat{\\beta_0} = 7.03$이고 $\\hat{\\beta_1} = 0.0475$ 이다. 즉, 광고에 1천 달러를 더 사용하면 sales는 대략 47.5 유닛 늘어난다.Advertising 자료에서 TV 광고에 대한 sales의 회귀를 최소제곱으로 적합한 것이다. 적합은 오차제곱합을 최소로 함으로써 구해진다. 각 회색 선분은 오차를 나타내고 적합은 이 오차들의 제곱을 평균하여 절충한다. 이 값들은 명백히 RSS를 최소화 한다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Linear Regression","slug":"Linear-Regression","permalink":"https://p829911.github.io/tags/Linear-Regression/"},{"name":"RSS","slug":"RSS","permalink":"https://p829911.github.io/tags/RSS/"}]},{"title":"편향-분산 절충","slug":"2.2.2","date":"2019-12-11T05:52:30.000Z","updated":"2019-12-11T05:54:24.773Z","comments":true,"path":"2019/12/11/2.2.2/","link":"","permalink":"https://p829911.github.io/2019/12/11/2.2.2/","excerpt":"","text":"2.2.2 편향-분산 절충검정 MSE 곡선이 $U$ 모양을 보이는 것은 통계학습방법의 두 가지 상충되는 성질 때문이다.주어진 값($x_0$)에 대한 기대(expected) 검정 MSE는 항상 세 가지의 기본적 수량인 $\\hat{f}(x_0)$의 분산, $\\hat{f}(x_0)$의 제곱편향, 그리고 오차항 $\\epsilon$의 분산의 합으로 분해된다. E\\left(y_0 - \\hat{f}(x_0)\\right)^2 = \\text{Var}(\\hat{f}(x_0)) + [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\epsilon)여기서 $E\\left(y_0 - \\hat{f}(x_0)\\right)^2$ 은 기대 검정 MSE에 대한 정의로, 아주 큰 수의 훈련 자료들을 사용하여 $f$를 반복적으로 추정하고 각각을 $x_0$에서 검정했을 경우 얻어지는 검정 MSE의 평균을 말한다. 위의 식에 의하면 기대검정오차를 최소화하기 위해서는 낮은 분산과 낮은 편향을 동시에 달성하는 통계학습방법을 선택해야 한다. 분산은 본질적으로 음수가 아니고 제곱편향도 또한 음수가 아니다. 그러므로, 기대 검정 MSE는 축소불가능 오차인 $Var(\\epsilon)$보다 작을 수 없다. 분산: 다른 훈련 자료를 사용하여 추정하는 경우 $\\hat{f}$ 가 변동되는 정도를 말한다.훈련자료는 통계학습방법을 적합하는 데 사용되므로, 다른 훈련자료를 사용하면 $\\hat{f}$이 달라질 것이다.그러나 이상적으로는 $f$에 대한 추정이 훈련 자료에 따라 너무 많이 변동되지 않아야 한다.하지만, 분산이 높으면 훈련 데이터의 변화가 작아도 $\\hat{f}$는 크게 변할 수 있다.일반적으로 통계학습방법의 유연성이 높을수록 분산도 더 높다. 편향: 실제 문제를 훨씬 단순한 모델로 근사시킴으로 인해 발생하는 오차로, 극도로 복잡할수도 있다.일반적으로 유연성이 높은 방법일수록 편향이 적다. 원칙적으로 유연성이 높은 방법을 사용할수록 분산이 증가하고 편향은 감소할 것이다. 이러한 분산과 편향의 상대적 변동율이 검정 MSE가 증가 또는 감소하는지를 결정한다.통계방법의 유연성을 증가시킴에 따라 편향은 처음에는 분산의 증가보다 더 빠르게 감소하는 경향이 있다. 하지만, 어떤 지점에서 유연성 증가는 편향에 거의 영향이 없지만 분산은 크게 증가시키기 시작한다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Bias","slug":"Bias","permalink":"https://p829911.github.io/tags/Bias/"},{"name":"Variances","slug":"Variances","permalink":"https://p829911.github.io/tags/Variances/"},{"name":"Trade-Off","slug":"Trade-Off","permalink":"https://p829911.github.io/tags/Trade-Off/"}]},{"title":"모델의 정확도 평가","slug":"2.2","date":"2019-12-11T04:42:41.000Z","updated":"2019-12-11T04:43:19.907Z","comments":true,"path":"2019/12/11/2.2/","link":"","permalink":"https://p829911.github.io/2019/12/11/2.2/","excerpt":"","text":"2.2 모델의 정확도 평가통계 분야에서 가능한 모든 자료에 대해 어떤 한 방법이 다른 방법들보다 지배적으로 더 나은 경우는 없다.그러므로 주어진 자료에 대해 최고의 기법을 선택하는 것이 실제로 통계학습을 수행하는 데 있어서 가장 어려운 부분 중의 하나이다. 2.2.1 적합의 품질 측정주어진 자료에 대한 통계학습방법의 성능을 평가하기 위해서는 이 방법에 의한 예측이 관측된 데이터와 실제로 얼마나 잘 맞는지 측정하는 방법이 필요하다. 즉, 주어진 관측치에 대해 예측된 반응 값이 관측치에 대한 실제 반응 값에 얼마나 가까운지를 수량화하는 것이 필요하다. 이러한 회귀 설정에서 가장 일반적으로 사용되는 측도는 평균제곱오차(MSE: mean squared error) 이다. \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat{f}(x_i)\\right)^2여기서, $\\hat{f}(x_i)$는 $i$ 번째 관측치에 대한 예측값이다. 예측된 반응 값들이 실제 반응 값들과 아주 가까우면 MSE는 작을 것이다. 하지만, 일부 관측치들에 대한 예측값과 실제값이 상당히 다를 경우 MSE는 큰 값이 될 것이다.위의 MSE는 모델을 적합하는 데 사용된 훈련 데이터(training data)를 사용하여 계산되므로 좀 더 정확하게 말해 훈련 MSE라고 한다. 그러나, 일반적으로는 통계학습방법이 훈련 데이터에 대해 얼마나 잘 동작하는지는 관심이 없다. 실제로 관심이 있는 것은 통계학습방법을 사전에 본적이 없는 검정 데이터(test data)에 적용할 때 얻는 예측 정확도이다. 사용할 수 있는 검정 관측치가 없으면, 훈련 MSE를 최소로 하는 통계학습방법을 선택하는 것을 생각해 볼 수도 있다. 하지만 훈련 MSE가 가장 낮은 방법이 검정 MSE도 가장 낮게 할 것이라는 보장이 없다. 왼쪽 패널에서 검은색 곡선으로 주어진 실제 $f$를 가지고 $Y = f(X) + \\epsilon$ 다음의 함수로 부터 관측치들을 생성하였다. 오랜지색, 파란색, 녹색 곡선은 유연성 수준이 증가하는 방법들을 사용하여 얻은 세 가지 가능한 $f$에 대한 추정을 보여준다. 명백히 유연성의 수준이 증가할수록 곡선은 관측된 데이터를 더 가깝게 적합한다. 녹색 곡선이 가장 유연하며 데이터에 아주 잘 맞는다. 하지만, 이것은 너무 꾸불꾸불하기 때문에 실제 $f$를 잘 적합하지 못한다. 오른쪽 패널의 회색 곡선은 몇몇 함수들에 대해 평균 훈련 MSE를 유연성, 공식적으로는 자유도(degree of freedom) 의 함수로 나타낸 것이다. 자유도는 곡선의 유연성을 요약해주는 수치이다. 오랜지색, 파란색, 녹색 사각형들은 왼쪽 패널의 대응하는 곡선들과 연관된 MSE를 나타낸다. 검정 MSE는 처음에는 유연성이 증가함에 따라 줄어든다. 하지만, 어떤 지점 이후부터 검정 MSE는 다시 증가하기 시작한다. 오른쪽 패널의 수평 파선은 축소불가능 오차 $\\text{Var}(\\epsilon)$ 을 나타내며, 이것은 모든 가능한 방법들 중에서 달성할 수 있는 가장 낮은 검정 MSE에 해당한다. 통계학습의 유연성이 증가함에 따라 훈련 MSE는 단조 감소하지만 검정 MSE는 $U$ 모양을 보인다. 이것은 가지고 있는 자료와 사용되는 통계방법에 관계없이 성립하는 통계학습의 기본적인 성질이다. 모델의 유연성이 증가함에 따라 훈련 MSE는 감소할 것이지만, 검정 MSE는 그렇지 않을 수도 있다. 주어진 방법이 훈련 MSE는 작지만 검정 MSE는 큰 결과를 제공할 때 데이터를 과적합(overfitting)한다고 한다.이러한 과적합은 통계학습 절차가 훈련 데이터에서 패턴을 찾는 데 지나치게 집중하여 알려지지 않은 함수 $f$의 실제 성질에 의한 것이 아니라 단순히 우연에 의한 어떤 패턴을 찾을 수도 있기 때문에 발생한다. 검정 MSE가 최소로 되는 지점을 실제로 추정하는 데 사용될 수 있는 기법 중의 하나는 교차검증(cross-validation) 인데, 이것은 훈련 데이터를 사용하여 검정 MSE를 추정하는 방법이다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Prediction","slug":"Prediction","permalink":"https://p829911.github.io/tags/Prediction/"},{"name":"Inference","slug":"Inference","permalink":"https://p829911.github.io/tags/Inference/"}]},{"title":"예측 정확도와 모델 해석력 사이의 절충 (Trade-Off)","slug":"2.1.3","date":"2019-12-11T04:40:38.000Z","updated":"2019-12-11T04:42:12.869Z","comments":true,"path":"2019/12/11/2.1.3/","link":"","permalink":"https://p829911.github.io/2019/12/11/2.1.3/","excerpt":"","text":"2.1.3 예측 정확도와 모델 해석력 사이의 절충 (Trade-Off)좀 더 제한적인 모델을 선호할 수 있는 몇 가지 이유가 있다.만약 주 관심사가 추론이면, 제한적인 모델이 훨씬 더 해석하기 쉽다.예를 들어, 추론이 목적인 경우, 선형 모델은 $Y$와 $X_1, X_2, \\cdots, X_p$ 사이의 상관관계를 이해하는 것이 아주 쉽기 때문에 좋은 선택일 수 있다.이에 반해, 매우 유연한 기법들은 $f$ 추정이 복잡하게 되어 어떤 개별 설명변수가 반응변수와 어떻게 연관되는지 이해하기 어려울 수 있다.다음 그림은 이 책에서 다루는 일부 방법들에 대한 유연성과 해석력 사이의 관계를 보여 준다. 일반적으로 유연성이 증가함에 따라 해석력은 감소한다.이러한 현상은 아주 유연한 방법들의 잠재적인 과적합과 관련이 있다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Prediction","slug":"Prediction","permalink":"https://p829911.github.io/tags/Prediction/"},{"name":"Inference","slug":"Inference","permalink":"https://p829911.github.io/tags/Inference/"}]},{"title":"어떻게 $f$ 를 추정하는가?","slug":"2.1.2","date":"2019-11-29T16:28:13.000Z","updated":"2019-12-11T04:38:59.760Z","comments":true,"path":"2019/11/30/2.1.2/","link":"","permalink":"https://p829911.github.io/2019/11/30/2.1.2/","excerpt":"","text":"2.1.2 어떻게 $f$ 를 추정하는가?우리의 목적은 통계학습방법을 훈련 데이터에 적용하여 알려지지 않은 함수 $f$를 추정하는 것이다.다시 말하면, 임의의 관측치 $(X, Y)$ 에 대해 $Y \\approx \\hat{f}(X)$ 을 만족하는 함수 $f$를 찾고자 한다.넓게 얘기하면, 이 일을 위한 대부분의 통계학습방법들은 모수적(parametric) 또는 비모수적(non-parametric) 으로 특징지을 수 있다. 모수적 방법 (Parametric Methods)모수적 방법은 2단계로 된 모델 기반의 기법이다. 먼저, $f$의 함수 형태 또는 모양에 대해 가정한다. 예를 들어, 아주 단순하게 $f$는 $X$에 대해 선형적이라고 가정한다. f(X) = \\beta_0 + \\beta_1X_1+\\beta_2X_2 + \\cdots + \\beta_pX_p이것은 선형모델이다. 일단 $f$가 선형이라는 가정이 있으면, $f$를 추정하는 문제는 크게 단순화된다.완전히 임의의 $p$차원 함수 $f(X)$를 추정해야 하는 대신에, $p+1$개의 계수 $\\beta_0, \\beta_1, \\cdots, \\beta_p$ 만 추정하면 된다. 모델이 선택된 후 훈련 데이터를 사용하여 모델을 적합(fit)하거나 훈련시키는 절차가 필요하다.선형모델의 경우, 파라미터 $\\beta_0, \\beta_1, \\cdots, \\beta_p$ 를 추정해야 한다. 즉, 다음을 만족하는 파라미터들의 값을 찾고자 한다. Y \\approx \\beta_0 + \\beta_1X_1+\\beta_2X_2 + \\cdots + \\beta_pX_p선형모델 적합에 가장 일반적으로 사용되는 기법은 최소제곱(least squares)이다.하지만, 최소제곱은 선형모델을 적합하는 많은 가능한 방법들 중의 하나이다. 모수적 방법은 $f$를 추정하는 문제를 파라미터를 추정하는 문제로 간주한다. $f$에 대한 모수적 형태를 가정하는 것은 $f$를 추정하는 문제를 단순화한다. 왜냐하면, 선형모델의 파라미터를 추정하는 것이 전적으로 임의의 함수 $f$를 적합하는 것보다 일반적으로 훨씬 쉽기 때문이다. 모수적 방법의 잠재적인 단점은 선택하는 모델이 알려지지 않은 $f$의 형태와 보통은 맞지 않을 것이라는 것이다. 만약 선택된 모델이 $f$의 실제 모양과 너무 다르면 추정이 정확하지 않을 것이다. $f$에 대해 많은 다른 가능한 함수 형태를 적합할 수 있는 유연한 모델을 선택함으로써 이 문제를 해결하려고 시도할 수 있지만, 일반적으로 적합하는 모델이 유연할수록 추정해야 하는 파라미터 수도 많아진다. 이러한 좀 더 복잡한 모델들은 데이터에 대한 과적합(overfitting)을 초래할 수 있다. 과적합은 본질적으로 오차 또는 노이즈(noise)를 너무 면밀히 추적하는 것을 의미한다. Income 자료에 대한 최소제곱에 의한 선형모델의 적합. 관측치들은 붉은색으로 표시되고 노란색 평면은 데이터에 대한 최소제곱적합을 나타낸다. Income 자료에 대한 모수적 방법의 예를 보여준다. 아래와 같은 형태의 선형모델을 적합한다. \\text{income} \\approx \\beta_0 + \\beta_1 \\times \\text{education} + \\beta_2 \\times \\text{seniority} 실제 $f$ 는 선형적합으로는 포착되지 않는 곡선 부분이 있다. 하지만, 선형적합은 여전히 교육기간과 income 사이의 양의 상관관계뿐만 아니라 seniority와 income 사이의 약간 덜 긍정적인 상관관계를 합리적으로 포착하는것처럼 보인다. 이렇게 적은 수의 관측치로는 이것이 할 수 있는 최선일 수 있다. 비모수적 방법 (Non-parametric Methods)비모수적 방법은 $f$의 함수 형태에 대해 명시적인 가정을 하지 않는다. 대신에 너무 거칠거나 왔다갔다 하지 않으면서 데이터 포인트들에 가능하면 가까워지는 $f$의 추정을 얻으려고 한다. 이러한 접근법은 모수적 방법에 비해 주요한 장점이 있을 수 있다. 즉, $f$의 함수 형태에 대한 가정을 하지 않아도 되므로 더 넓은 범위의 $f$ 형태에 정확하게 적합될 가능성이 있다.어떠한 모수적 방법이라도 $f$를 추정하는 데 사용된 함수 형태가 실제 $f$와 많이 다를 수 있으며, 이 경우 결과 모델은 데이터에 잘 적합되지 않을 것이다. 이에 반해, 비모수적 방법은 $f$의 형태에 대한 어떠한 가정도 하지 않기 때문에 이러한 위험을 완전히 회피한다. 하지만, 비모수적 방법은 중요한 단점이 있다. 이 방법은 $f$를 추정하는 문제를 작은 수의 파라미터 추정 문제로 축소하지 않으므로, $f$에 대한 정확한 추정을 얻기 위해서는 아주 많은 수의 관측치가 필요하다. Income 자료를 적합하는 비모수적 방법의 한 예박판 스플라인(thin-plate spline)이 $f$를 추정하는데 사용된다. 이 기법은 $f$에 대해 어떠한 미리 지정된 모델을 고려하지 않는다. 박판 스플라인을 적합하기 위해서는 평활정도를 선택해야 한다. (유연성과 관계있음) 위의 그림은 거친적합을 초래하는 평활정도가 낮은 경우를 보여준다. 오버피팅의 한 예, 오차가 없다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Prediction","slug":"Prediction","permalink":"https://p829911.github.io/tags/Prediction/"},{"name":"Inference","slug":"Inference","permalink":"https://p829911.github.io/tags/Inference/"}]},{"title":"통계학습이란?","slug":"2.1","date":"2019-11-14T11:45:16.000Z","updated":"2019-12-04T11:57:17.913Z","comments":true,"path":"2019/11/14/2.1/","link":"","permalink":"https://p829911.github.io/2019/11/14/2.1/","excerpt":"","text":"ISLR (Introduction to Statistical Learning)Chapter 2. 통계학습2.1 통계학습이란?Advertising 자료는 200개의 다른 시장에서 제품의 sales(판매 수치)와 각 시장별로 그 제품에 대한 광고예산으로 구성되어 있다. 광고예산은 TV, radio, newspaper 에 대한 것이다. 각 그래프는 각 변수에 대한 sales의 단순최소제곱적합(simple least squares fit)을 보여준다. 다시 말하면, 각 그래프의 파란색 직선은 TV, radio, newspaper 각각을 사용하여 sales를 예측하는 데 사용될 수 있는 간단한 모델을 나타낸다. 우리의 고객이 직접적으로 제품의 판매를 증가시킬 수 있는 방법은 없다. 하지만 그는 세 매체에 대한 광고 지출을 제어할 수 있다. 그러므로, 만약 우리가 광고와 판매 사이의 상관관계를 결정할 수 있다면, 우리는 고객에게 광고예산을 조절하게 하여 간접적으로 판매를 증진시킬 수 있다. 다시 말하면, 우리의 목적은 세 매체에 대한 광고예산을 기반으로 판매를 예측할 수 있는 정확한 모델을 개발하는 것이다.여기서, 광고예산은 입력변수이고 sales는 출력변수이다. input variable - $X$ predictors (예측변수) independent variables (독립 변수) features (특징) variables (변수) output variable - $Y$ response (반응변수, 응답변수) dependent variables (종속변수) 좀 더 일반적으로, 양적(quantitative) 반응변수 $Y$ 개와 $p$ 개의 다른 설명변수, $X_1, X_2, \\cdots, X_p$ 가 관찰된다고 해보자. $Y$ 와 $X = (X_1, X_2, \\cdots, X_p)$ 사이에 어떤 상관관계가 있다고 가정하면 다음과 같은 일반적인 형태로 나타낼 수 있다. Y = f(X) + \\epsilon여기서, $f$ 는 $X_1, \\cdots, X_p$ 에 대한 알려지지 않은 어떤 고정 함수이고, $\\epsilon$ 은 랜덤 오차항(error term)이다.오차항은 $X$와 독립적이며 평균은 0이다. 이 식에서 $f$는 $X$가 $Y$에 대해 제공하는 체계적인 정보를 나타낸다. 이 그래프는 교육기간을 이용하여 income을 예측할 수도 있음을 시사한다. 하지만, 일반적으로 입력 변수를 출력 변수에 연결하는 함수 $f$는 알려져 있지 않다. 이러한 경우, 함수 $f$ 는 관찰된 점들을 기반으로 추정해야 한다. income은 모의 자료이므로, $f$ 는 알려져 있고 오른쪽 패널에 파란색으로 표시된다. 수직선은 오차항 $\\epsilon$ 을 나타낸다. 30개의 관찰치 일부는 파란색 곡선의 윗 부분에 있고 일부 다른 데이터는 곡선 아래에 있다. 전체적으로 오차의 평균은 대략 0이다. 위의 그래프에서 $f$는 관찰된 데이터에 기초하여 추정되어야 하는 2차원 곡면(surface)이다. 본질적으로, 통계학습은 $f$를 추정하는 일련의 기법들을 말하는 것이다. 2.1.1 $f$ 를 추정하는 이유는?$f$를 추정하고자 하는 두 가지 주요한 이유는 예측과 추론이다. 예측 (Prediction)많은 경우, 입력 $X$는 쉽게 얻을 수 있지만 출력 $Y$는 쉽게 얻을 수 없다. 여기서 오차항은 평균이 영이므로 다음 식을 사용하여 $Y$를 예측할 수 있다. \\hat{Y} = \\hat{f}(X)여기서, $\\hat{f}$ 는 $f$ 에 대한 추정을 나타내고 $\\hat{Y}$ 는 $Y$ 에 대한 예측 결과를 나타낸다. 이러한 설정에서 $\\hat{f}$ 는 보통 블랙박스(black box)로 취급된다. 이유는 $\\hat{f}$ 가 $Y$에 대한 정확한 예측을 제공한다면 그것의 정확한 형태에 대해서는 통상 신경쓰지 않기 때문이다.$Y$에 대한 예측인 $\\hat{Y}$ 의 정확성은 축소가능 오차(reducible error)와 축소불가능 오차(irreducible error) 라고 불리는 두 가지에 달려 있다. 일반적으로, $\\hat{f}$ 는 $f$ 를 완벽하게 추정하지 못하며, 이러한 부정확성으로 인해 오차가 발생될 것이다. 이러한 오차는 축소가능하다. 왜냐하면 가장 적절한 통계학습기법을 사용하여 $f$를 추정함으로써 $\\hat{f}$ 의 정확성을 개선할 수 있기 때문이다. 하지만, 심지어 $f$를 완벽하게 추정할 수 있어 추정된 반응변수 값이 $\\hat{Y} = f(X)$ 의 형태를 취하더라도 예측한 값은 여전히 어떤 오차를 가지고 있을 수 있다. 이러한 이유는 $Y$도 또한 $\\epsilon$ 의 함수이고, 정의에 의하면 $\\epsilon$ 은 $X$를 사용하여 예측할 수 없기 때문이다. 그러므로 $\\epsilon$ 과 관련된 변동성도 또한 예측의 정확성에 영향을 미친다. 이것은 축소불가능 오차로 알려져 있으며, 이유는 아무리 $f$를 잘 추정하더라도 $\\epsilon$ 에 의해 도입된 오차를 줄일 수 없기 때문이다. 축소가능 오차가 0보다 큰 이유: $\\epsilon$ 은 $Y$를 예측하는 데 유용한 측정되지 않은 변수를 포함할 수 있다. 이 변수들은 측정하지 않으므로 $f$는 예측에 이들을 사용할 수 없다. $\\epsilon$ 은 또한 측정할 수 없는 변동성을 포함할 수 있다. 예를 들어, 부작용의 위험성은 주어진 날 주어진 환자에 따라 다를 수 있고, 약물 자체의 제조상의 차이 또는 그 환자의 그 날 기분이나 상황에 따라 다를 수 있다. \\begin{align} E(Y-\\hat{Y})^2 & = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\ & = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{reducible}} + \\underbrace{Var(\\epsilon)}_{\\text{irreducible}} \\end{align}여기서, $E(Y-\\hat{Y})^2$ 은 $Y$의 예측 및 실제값 사이의 차이의 제곱에 대한 평균 또는 기대값(expected value)을 나타내며, $Var(\\epsilon)$은 오차항 $\\epsilon$ 과 관련된 분산(variable)을 나타낸다.축소불가능 오차는 항상 $Y$에 대한 예측 정확도의 상한선이 될 것이지만 그 경계는 현실적으로 거의 언제나 알려져있지 않다. 추론 (Inference)보통 $X_1, \\cdots, X_p$ 가 변함에 따라 $Y$가 어떻게 영향을 받는지 이해하는 데 관심이 있다. 이러한 상황에서, $f$를 추정하고자 하지만 반드시 $Y$에 대해 예측하는 것이 목적인 것은 아니다. 대신에 $X$ 와 $Y$ 사이의 관계를 이해 하길 원하거나, 좀 더 상세하게는 $X_1, \\cdots, X_p$ 의 함수로서 $Y$가 어떻게 변하는지 이해하고자 한다. 이제 $\\hat{f}$ 는 블랙박스로 취급될 수 없다. 왜냐하면, 그것의 정확한 형태를 알아야 할 필요가 있기 때문이다. 어떤 설명변수들이 반응변수와 관련되어 있는가? 반응변수와 각 설명변수 사이의 상관관계는 무엇인가? $Y$와 각 설명변수의 상관관계는 선형 방정식을 사용하여 충분히 요약될 수 있는가? 또는 이 상관관계는 더 복잡한가?역사적으로 $f$를 추정하는 대부분의 방법들은 선형 형태를 취한다. 어떤 경우에는 이러한 가정이 합리적이거나 심지어 바람직하다. 그러나, 실제 상관관계는 보통 더 복잡하며 선형모델은 입력과 출력변수들 사이의 상관관계를 정확하게 표현하지 못할 수 있다. 최종 목적이 예측, 추론 또는 이 둘을 결합한 것인지의 여부에 따라 $f$를 추정하는 데 다른 방법들을 사용하는 것이 적절할 수 있다. 예를 들어, 선형모델들은 비교적 간단하고 해석 가능한 추론을 할 수 있지만 몇몇 다른 기법들만큼 정확한 예측을 할 수 없을 수 있다. 반대로, 몇가지 고도의 비선형적인 기법들은 잠재적으로 $Y$에 대해 아주 정확한 예측을 제공할 수 있지만 추론을 더욱 어렵게 만드는 이해하기 어려운 모델을 초래한다.","categories":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/categories/ISLR/"}],"tags":[{"name":"ISLR","slug":"ISLR","permalink":"https://p829911.github.io/tags/ISLR/"},{"name":"Statistical Learning","slug":"Statistical-Learning","permalink":"https://p829911.github.io/tags/Statistical-Learning/"},{"name":"Prediction","slug":"Prediction","permalink":"https://p829911.github.io/tags/Prediction/"},{"name":"Inference","slug":"Inference","permalink":"https://p829911.github.io/tags/Inference/"}]},{"title":"python gc collect","slug":"python-gc-collect","date":"2019-11-11T05:13:05.000Z","updated":"2019-11-11T05:13:40.238Z","comments":true,"path":"2019/11/11/python-gc-collect/","link":"","permalink":"https://p829911.github.io/2019/11/11/python-gc-collect/","excerpt":"","text":"python gc collect보통 파이썬은 레퍼런스 카운팅 방식으로 가비지 컬렉션을 수행해 메모리를 관리하고, 레퍼런스 카운팅을 사용했을 때 발생할 수 있는 순환 참조 상황을 별도의 가비지 컬렉터로 해결한다고 알고 있다. python에는 Garbage Collection이라는 것이 있기 때문에 C/C++ 처럼 메모리를 직접 할당/해제하는 수고를 하지 않아도 되며 메모리를 직접 다룸으로 인해 발생되는 수많은 버그나 위험으로 부터 안전한 편이다. 왜냐하면 Garbage Collection이라는 것이 생성된 객체들을 순회하며 해당 객체가 현재 쓰이는 곳이 없을 경우 자동으로 해제를 해주기 때문이다. 그렇다고 사용자가 아무것도 하지 않아서는 안된다. Garbage Collection이 쓸모 없어진 객체들을 잘 해제할 수 있도록 레퍼런스 카운트에 신경을 써주어야 한다. 특히, 순환참조의 경우는 프로그램이 종료될때까지 메모리에 남아 있게 되므로 특히 주의 하여야한다. GC는 어떨 때 사용할까?파이썬에선 기본적으로 garbage collection과 reference counting을 통해 할당된 메모리를 관리한다.기본적으로 참조 횟수가 0이 된 객체를 메모리에서 해제하는 레퍼런스 카운팅 방식을 사용하지만, 참조 횟수가 0은 아니지만 도달할 수 없는 상태인 reference cycles(순환 참조) 가 발생했을 때는 가비지 컬렉션으로 그 상황을 해결한다. 엄밀히 말하면 레퍼런스 카운팅 방식을 통해 객체를 메모리에서 해제하는 행위가 가비지 컬렉션의 한 형태지만 여기서는 순환 참조가 발생했을 때 cyclic garbage collector를 통한 가비지 컬렉션과 레퍼런스 카운팅을 통한 가비지 컬렉션을 구분했다. 레퍼런스 카운팅모든 객체는 참조 당할 때 레퍼런스 카운터를 증가시키고 참조가 없어질 때 카운터를 감소시킨다.이 카운터가 0이 되면 객체가 메모리에서 해제 된다. 어떤 객체의 레퍼런스 카운트를 보고 싶다면 sys.getrefcount() 로 확인할 수 있다. 순환 참조의 예123l = []l.append(l)del l l의 참조 횟수는 1이지만 이 객체는 더 이상 접근할 수 없으며 레퍼런스 카운팅 방식으로는 메모리에서 해제될 수 없다. 12345678class A: def __del__(self): print(\"Deleted\") a = A()del(a)# 결과: Deleted 123456789class A: def __del__(self): print(\"Deleted\") a = A()b = adel(a)# 결과: None 이전과는 다르게 delete라는 메시지를 출력하지 않는다. 즉, a라는 객체가 메모리에서 지워지지 않았다는 얘기이다. 왜냐하면 b라는 변수에서 a를 참조중이기 때문이다. 오브젝트가 현재 참조중인 목록 알아내기gc.get_referents 오브젝트를 참조중인 목록 알아내기gc.get_referrers(a) 출처 https://weicomes.tistory.com/277 https://wikidocs.net/13969 https://winterj.me/python-gc/","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"GC Collect","slug":"GC-Collect","permalink":"https://p829911.github.io/tags/GC-Collect/"}]},{"title":"python GIL","slug":"python-GIL","date":"2019-11-11T05:12:22.000Z","updated":"2019-11-11T05:12:51.524Z","comments":true,"path":"2019/11/11/python-GIL/","link":"","permalink":"https://p829911.github.io/2019/11/11/python-GIL/","excerpt":"","text":"Python GILGlobal Interpreter Lock In CPython, the global interpreter lock, or GIL, is a mutex that protects access to python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. Process &amp; Thread 운영체제가 생성하는 작업 단위를 process라고 한다. 이 process 안에서 공유되는 메모리를 바탕으로 여러 작업을 또 생성할 수 있는데, 이 때의 작업 단위를 thread라고 한다 따라서 각 thread 마다 할당된 개인적인 메모리가 있으면서, thread가 속한 process가 가지는 메모리에도 접근할 수 있다. Thread-safe 하지 않다는 것은 무슨 의미인지 부터 알아보겠다. 위에서 thread들은 process가 공유하는 메모리에 접근할 수 있다고 언급했는데, 이로 인해 참사가 발생할 수 있다. 12345678910111213141516171819202122import threadingx = 0 # A shared valuedef foo(): global x for i in range(1000000): x += 1 def bar(): global x for i in range(1000000): x -= 1 t1 = threading.Thread(target=foo)t2 = threading.Thread(target=bar)t1.start()t2.start()t1.join()t2.join() # Wait for completionprint(x) print(x) 의 결과가 0으로 나오는게 정상적으로 작동한 것이다. 하지만 실제 계산을 해보면 x의 값은 전혀 이상한 숫자가 된다. 전역 변수 x에 두 개의 thread가 동시에 접근해서 각자의 작업을 하면서 어느 한 쪽의 작업 결과가 반영이 되지 않기 때문이다. 이렇게 여러 thread가 공유된 데이터를 변경함으로써 발생하는 문제를 race condition 이라고도 부른다. 따라서 thread-safe 하다는 것은 thread들이 race condition을 발생시키지 않으면서 각자의 일을 수행한다는 뜻임을 알 수 있다. mutexThread-safe한 코드를 만들기 위해서 사용하는 것 중 하나가 mutex (mutual exclusion)이다. race condition을 막기 위해서, 공유되는 메모리의 데이터를 여러 thread가 동시에 사용할 수 없도록 잠그는 일을 mutex가 맡는다. 휴대폰이 없던 시절에는 공중 전화를 주로 이용했었다. 거리의 모든 남자들은 각자의 아내에게 전화를 너무나 걸고 싶어한다. 어떤 한 남자가 처음으로 공중 전화 부스에 들어가서 그의 사랑하는 아내에게 전화를 걸었다면, 그는 꼭 전화 부스의 문을 꼭 잡고 있어야 한다. 왜냐하면 사랑에 눈이 먼 다른 남자들이 전화를 걸기 위해 시도때도 없이 달려들고 있기 때문이다. 줄 서는 질서 문화 따위는 없다. 심지어 그 문을 놓친다면, 전화 부스에 들이닥친 남자들이 수화기를 뺏어 당신의 아내에게 애정 표현을 할 지도 모른다. 아내와의 즐거운 통화를 무사히 마쳤다면, 이제 문을 잡고 있던 손을 놓고 부스 밖으로 나가면 된다. 그러면 공중 전화를 쓰기 위해 달려드는 다른 남자들 중 제일 빠른 한 명이 부스에 들어가서 똑같이 문을 꼭 잡고 그의 아내와 통화할 수 있다. thread: 각 남자들 mutex: 공중 전화 부스의 문 lock: 그 문을 잡고 있는 남자의 손 resource: 공중 전화 CPython이 reference counting을 하는 과정에서 문제가 일어날 수 있음을 알 수 있다.Reference counting 중에 race condition이 일어난다면, 그 결과는 결국 메모리 유실(memory leak)일 것이다. (반대로 살아있어야 할 Object를 죽여버릴 수도 있다.)이를 해결하기 위해서는 mutex를 이용하면 된다고 했다.CPython의 결정은 mutex를 통해 모든 reference 개수를 일일이 보호하지 말고, Python interpreter 자체를 잠그기로 한 것이다. 이거 하나만 mutex로 보호하면 그동안 우려했던 문제를 해결할 수 있다. 하지만, 얼마나 많은 thread를 사용하던지에 상관없이 오직 한 thread만이 Python code를 실행할 수 있다는 의미이기도 하다. 왜 GIL을 선택했나?Python이 태동하던 시기에는 thread라는 개념이 없었을 당시였고, 쉽고 간결한 언어를 표방했던 Python에 많은 사용자들이 모여들고 있었다. 수 많은 C extension들이 이미 만들어졌는데, 시간이 지나서 thread 개념으로 인한 문제를 해결하기 위해서 가장 현실적인 방안은 GIL이었다. 거대한 커뮤니티에서 만들어낸 C extension들을 새로운 메모리 관리 방법에 맞춰서 모두 바꾸는 것은 불가능하다. 대신 Python이 GIL을 도입하면 C extension들을 바꾸지 않아도 됐던 것이다. 결론 병렬 처리에 관해서는 굳이 thread가 아니더라도 multiprocessing이나 asyncio 등의 많은 선택지가 있다. 출처 https://dgkim5360.tistory.com/entry/understanding-the-global-interpreter-lock-of-cpython https://magi82.github.io/process-thread/ https://118k.tistory.com/606 H3 2011 파이썬으로 클라우드 하고 싶어요_분산기술Lab 하용호 From KTH, 케이티하이텔 [2D4]Python에서의 동시성_병렬성 from NAVER D2","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"GIL","slug":"GIL","permalink":"https://p829911.github.io/tags/GIL/"}]},{"title":"프로세스와 스레드의 차이","slug":"프로세스와-스레드의-차이","date":"2019-11-11T05:10:40.000Z","updated":"2019-11-11T05:12:11.537Z","comments":true,"path":"2019/11/11/프로세스와-스레드의-차이/","link":"","permalink":"https://p829911.github.io/2019/11/11/프로세스와-스레드의-차이/","excerpt":"","text":"프로세스와 스레드의 차이(Process vs Thread)프로세스(Process) 의미 컴퓨터에서 연속적으로 실행되고 있는 컴퓨터 프로그램 메모리에 올라와 실행되고 있는 프로그램의 인스턴스(독립적인 개체) 운영체제로부터 시스템 자원을 할당받는 작업의 단위 종종 스케줄링의 대상이 되는 작업(task)이라는 용어와 거의 같은 의미로 쓰인다. 할당받는 시스템 자원의 예 CPU 시간 운영되기 위해 필요한 주소 공간 Code, Data, Stack, Heap의 구조로 되어 있는 독립된 메모리 영역 특징 프로세스는 각각 독립된 메모리 영역(Code, Data, Stack, Heap의 구조)을 할당 받는다. 기본적으로 프로세스 당 최소 1개의 스레드(메인 스레드)를 가지고 있다. 각 프로세스는 별도의 주소 공간에서 실행되며, 한 프로세스는 다른 프로세스의 변수나 자료구조에 접근 할 수 없다. 한 프로세스가 다른 프로세스의 자원에 접근하려면 프로세스 간의 통신(IPC, inter-process communication) 을 사용해야 한다. 파이프, 파일, 소켓 등을 이용한 통신 방법 이용 프로세스 특징프로세스는 프로그램 실행 시 Code, Data, Stack, Heap의 구조로 되어 있는 독립된 메모리 영역을 할당 받는다.이러한 특징은 멀티 프로세싱 방식에서 단점을 가지게 된다.멀티 프로세싱의 방식은 CPU에서 여러 프로세스를 로테이션으로 돌면서 처리를 하게 된다.동작 중인 프로세스가 대기를 하면서 해당 프로세스의 상태(context)를 보관하고, 대기하고 있던 다음 순번의 프로세스가 동작하면서 이전에 보관했던 프로세스의 상태(context)를 복구하게 된다.이러한 일련의 과정을 Context Switching 이라고 하는데 프로세스는 각각 독립된 메모리 영역이다보니 캐쉬 메모리 초기화 등 꽤나 무거운 작업이 진행되고 오버헤드가 발생하게 된다. 쓰레드(Thread) 사전적 의미 프로세스 내에서 실행되는 여러 흐름의 단위 프로세스의 특정한 수행 경로 프로세스가 할당 받은 자원을 이용하는 실행의 단위 특징 기본적으로 프로세스당 최소 1개의 스레드를 가지고 있고 그것을 메인 스레드라고 한다. 프로세스는 1개 이상의 스레드를 가질 수 있으며 멀티스레드라고 한다. 위에서 설명한 Context Switching은 사실 프로세스가 가지고 있는 스레드를 처리하는 과정이다. 스레드는 프로세스 내에서 각각 Stack만 따로 할당받고 Code, Data, Heap 영역은 공유한다. 스레드는 한 프로세스 내에서 동작되는 여러 실행의 흐름으로, 프로세스 내의 주소 공간이나 자원들(힙 공간 등)을 같은 프로세스 내에 스레드끼리 공유하면서 실행된다. 같은 프로세스 안에 있는 여러 스레드들은 같은 힙 공간을 공유한다. 반면에 프로세스는 다른 프로세스의 메모리에 직접 접근할 수 없다. 각각의 스레드는 별도의 레지스터와 스택을 갖고 있지만, 힙 메모리는 서로 읽고 쓸 수 있다. 한 스레드가 프로세스 자원을 변경하면, 다른 이웃 스레드(sibling thread)도 그 변경 결과를 즉시 볼 수 있다.) 장점 메모리 공유로 인한 시스템 자원 소모가 줄어든다. 응답시간이 단축 된다. Context Switching에 대한 오버헤드가 줄어든다. 단점 서로 데이터를 사용하다가 충돌이 일어날 가능성이 있다. 디버깅이 다소 까다로워 진다. 출처 https://magi82.github.io/process-thread/ https://gmlwjd9405.github.io/2018/09/14/process-vs-thread.html","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Process","slug":"Process","permalink":"https://p829911.github.io/tags/Process/"},{"name":"Thread","slug":"Thread","permalink":"https://p829911.github.io/tags/Thread/"}]},{"title":"동시성과 병렬성","slug":"동시성과-병렬성","date":"2019-11-11T05:09:04.000Z","updated":"2019-11-11T05:12:06.004Z","comments":true,"path":"2019/11/11/동시성과-병렬성/","link":"","permalink":"https://p829911.github.io/2019/11/11/동시성과-병렬성/","excerpt":"","text":"동시성과 병렬성 (threading &amp; multiprocessing, parmap) 사전지식 task (작업): 일의 단위라고 보면 된다. 1부터 100까지 더하는 것도 하나의 컴퓨팅 작업이며, 프로그램을 다운로드 하는 것도 하나의 작업이 될 수 있고, 프린트를 하는 것이나, 음악을 재생하는 것, 문서를 저장하는 것 등등이 모두 개별적인 서로 다른 작업이다. sub-task (부분작업): 하나의 작업은 다시 작은 단위의 소작업들로 나뉠 수 있다. 이 때 소작업은 원래 작업과 하는 일 자체는 똑같다. 단지 전체 해야할 일에 일부를 할 뿐이다. 예를 들어, 1부터 100까지 더하는 task를 두 개의 sub-task 로 나눈다면, 1부터 50까지 더하고 51부터 100까지 더하는 것이다. 최종적으로 두 sub-task의 결과를 더하면 원래 task의 결과물이 나온다. simultaneous (동시의): concurrent와 단어 자체의 의미는 같지만, 컴퓨터 분야에서는 서로의 의미가 조금 다르다. 이 때 simultaneous는 ‘그 순간에 정말로 동시에’ 라는 의미이다. at the exact time 이라고 종종 영어로 표현 되기도 한다. 이 단어가 중요한 것은 parallel 을 표현할 때 simultaneous 가 꼭 사용되기 때문이다. 우체국이 있습니다.저는 소포를 보내려는 손님이죠.손님들 100명이 우체국에 일렬로 줄을 서있고, 한명씩 처리하는것이 -&gt; 싱글스레드 / 동기 처리입니다.손님들 100명이 우체국의 100명의 직원에게 각각 처리하는 것이 -&gt; 멀티쓰레드 / 동기 처리입니다. 여기까진 명쾌하죠.그럼 손님들 100명이 우체국에서 번호표를 받아서 각각 자기 할일 하다가, 우체국에서 스마트폰으로 니 차례야 하고 알려주면 그 때 우체국에 순간이동(할 수 있다고 합시다)해서 소포를 보낼 수 있을 만큼 보내는것은?? 네 이것은 싱글쓰레드 / 동기 처리입니다. 동기? 왜 동기 일까요? 사실 이것을 비동기로 봐도 되긴 합니다.왜냐면 손님은 일단 기다리지 않고 (블럭되지 않고) 자기 할일을 할 수 있기 때문에, 비동기로 봐도 됩니다.다만 하마님아~~~ 님 차례 됬다. 라고 알려주면 하마는 자기 소포를 보내게 되는데! 바로 이 순간은 동기의 순간입니다. 즉 내가 너무 많은 소포를 보내면 , 우체국은 다른 사람들에게 니 차례야 라고 말해줄 수 없는거에요. 병목이 생긴다는 의미입니다. (이게 Windows 에서 SELECT 이자, Java NIO 이며, Node.js 가 왜 비지니스로직이 길면 문제가 생기는지에 대한 대답입니다. 또한 node 나 자바,파이선의 selector 가 비동기 방식이라고 말하고 있는데 생각하기 나름입니다. 하이레벨에서의 구분이냐? 로우레벨에서의 구분이냐에 따라서 달리 볼수 있기 때문에~) 하지만손님들 100명이 우체국 뒷마당에다가 소포를 던져두고, 이벤트 알림표를 받고 집에가서 자기 할일을 합니다.그러다가 우체국에서 알림이 오겠죠. “니 소포 다 보냈어” , 다른 손님에게도 알림이 갑니다. “니 소포도 다 보냈어” 자~~~ 이렇게 되면 모두 병목에서 해방됩니다. 대신 우체국(OS) 가 더 많은 일을 하게 되겠죠.이게 바로 진정한(?) 비동기 입니다. Windows 의 IOCP 입니다. 간단하게 차이를 정리하면- 일을 할 수 있는지 알려주는 방식 (react)- 일의 완료를 알려 주는 방식(proact) 입니다. 초기 각 언어의 라이브러리들은 반동기식(?)의 i/o를 지원해줬었고, 이제 몇몇 라이브러리들은 Proactor 패턴식의 비동기 I/O 입출력도 지원해 주기 시작했습니다. 파이썬도 EpollSelector / KqueueSelector / SelectSelector 등 지원함. 출처: 하마 블로그 https://hamait.tistory.com/833 동시성 (Concurrency) 한 번에 여러 스레드를 다루는 것 하나의 코어에서 여러 스레드가 번갈아가며 실행되는 성질 동시에 실행되는 것처럼 보이지만 실제로는 시분할하여 번갈아 실행 논리적 관점 병렬성(Parallelism) 한번에 여러 스레드를 실행하는 것 다중 코어에서 각 코어들이 동시에 실행되는 성질 물리적 관점 결론파이썬에서 병렬처리를 구현하는 방식은 두 가지로 멀티 쓰레드를 사용하거나 멀티 프로세스를 사용하는 것이다. 쓰레드는 가볍지만 GIL로 인해 계산 처리를 하는 작업은 한번에 하나의 쓰레드에서만 작동하여 cpu 작업이 적고 I/O 작업이 많은 병렬 처리 프로그램에서 효과를 볼 수 있다. 프로세스는 각자가 고유한 메모리 영역을 가지기 때문에 더 많은 메모리를 필요로 하지만, 각각 프로세스에서 병렬로 cpu 작업을 할 수 있고 이를 이용해 여러 머신에서 동작하는 분산 처리 프로그래밍도 구현할 수 있다. 출처 https://www.youtube.com/watch?v=Iv3e9Dxt9WY https://hamait.tistory.com/833 https://monkey3199.github.io/develop/python/2018/12/04/python-pararrel.html https://oaksong.github.io/2017/12/23/concurrency-and-parallelism/","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Threading","slug":"Threading","permalink":"https://p829911.github.io/tags/Threading/"},{"name":"Multiprocessing","slug":"Multiprocessing","permalink":"https://p829911.github.io/tags/Multiprocessing/"},{"name":"Parmap","slug":"Parmap","permalink":"https://p829911.github.io/tags/Parmap/"}]},{"title":"통계 검정","slug":"통계-검정","date":"2019-11-11T04:54:21.000Z","updated":"2019-11-11T05:08:10.267Z","comments":true,"path":"2019/11/11/통계-검정/","link":"","permalink":"https://p829911.github.io/2019/11/11/통계-검정/","excerpt":"","text":"추론 통계란 모집단에서 샘플링한 표본을 가지고 모집단의 특성을 추론하고 그 결과가 신뢰성이 있는지 검정하는 것이다. 요즘에는 빅데이터라는 개념과 함께 모집단과 표본집단을 구분하기 보다는 내가 가지고 있는 데이터 전체를 표본으로 보고 내가 수집하지 못한 현실 세계 전체의 데이터나 미래에 대한 데이터를 모집단이라고 본다. 추론 통계시 집중하는 부분 표본집단은 모집단을 대표할 수 있는가?모집단의 일부인 표본을 보고 모집단을 추정하기 때문에 표본의 특성이 모집단을 잘 반영하고 있어야 한다. 표본의 확률분포는 어떠한가? 어떤 분포이냐에 따라 추정을 위한 기법이 달라지기 때문에 중요하다.다만 표본의 수가 많아질 수록 정규분포에 근사하게 된다. 추정된 결과에 신뢰성이 있는가?추정된 결과를 활용할 수 있는지를 결정하는 요소이기 때문에 중요하다. 가설 검정의 절차 주요 용어 정리 가설귀무가설 (=영가설=H0)일반적으로 맞다고 가정하는 가설을 말한다. 대립가설 (=H1)새롭게 맞다고 증명하려는 가설을 말한다. 예를 들어 회귀분석의 경우 귀무가설은 “설명변수(x)는 반응변수(y)에 영향을 주지 않는다.” 이고 대립가설은 “설명변수(x)는 반응변수(y)에 영향을 준다” 이다. 귀무가설은 차이가 없다, 영향력이 없다, 연관성이 없다, 효과가 없다.대립가설은 차이가 있다, 영향력이 있다, 연관성이 있다, 효과가 있다. 검정방법 (양측검정, 단측검정, 좌측검정, 우측검정)양측검정귀무가설을 기각하는 영역이 양쪽에 있는 검정을 말한다.대립가설이 000가 아니다 (크거나 작다) 라면 양측검정을 사용한다. 단측검정 귀무가설을 기각하는 영역이 한쪽 끝에 있는 검정을 말한다.대립가설이 000 보다 작다 또는 크다 인 경우 단측 검정을 사용한다. 좌측검정단측검정 중 하나로, 귀무가설을 기각하는 영역이 왼쪽에 있는 검정을 말한다.대립가설이 000 보다 작다 인 경우 좌측검정을 사용한다. 우측검정단측검정 중 하나로, 귀무가설을 기각하는 영역이 오른쪽에 있는 검정을 말한다.대립가설이 000보다 크다 인 경우 우측검정을 사용한다. 신뢰/유의신뢰수준가설을 검정할 때 얼마나 빡빡하게 검정할 것인지를 결정하는 수준을 말한다.연구활동은 99%, 일반적으로는 95%, 단순설문조사는 90% 정도의 신뢰수준을 사용한다. 유의수준 ($\\alpha$)가설을 검정할 때 이 정도까지 벗어나면 귀무가설이 오류라고 인정하겠다 하는 수준을 말한다.유의수준 = 1 - 신뢰수준유의수준 = $\\sum$ 기각역 기각역확률분포에서 귀무가설을 기각하는 영역을 말한다.기각역에 검정통계량이 위치하면 귀무가설을 기각한다.양측검정인 경우 기각역은 유의수준 / 2 이고, 단측검정인 경우 기각역은 유의수준과 같다. 신뢰구간신뢰구간에 포함되는 x값 구간을 말한다. 임계치신뢰구간에서 기각역으로 넘어가는 기준이 되는 x값을 말한다. 검정통계량가설을 검정하기 위한 기준으로 사용하는 값(t 값 등)을 말한다.검정통계량이 확률분포 상에 어디에 위치하는지에 따라 귀무가설을 기각하거나 기각하지 않는다. \\text{검정통계량} = \\frac{(\\text{표본평균} - \\text{모평균})}{\\text{표본 표준편차}}유의확률 (p-value)자유도를 고려했을 때 검정통계량에 대한 확률을 말한다. (귀무가설의 신뢰구간을 벗어나는 확률)기각역보다 유의확률이 작아야 귀무가설을 기각할 수 있다. 자유도x값이 가질 수 있는 값의 범위를 말한다.자유도가 주어지지 않는 경우, $\\text{자유도}=\\text{표본수}(n) - 1$ 가설 검정 예시 출처 https://kkokkilkon.tistory.com/36","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Statistics","slug":"Math/Statistics","permalink":"https://p829911.github.io/categories/Math/Statistics/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Statistics","slug":"Statistics","permalink":"https://p829911.github.io/tags/Statistics/"}]},{"title":"Zip 압축 풀 때 한글 파일명 오류 해결법","slug":"Zip-압축-풀-때-한글-파일명-오류-해결법","date":"2019-09-18T11:43:12.000Z","updated":"2019-09-18T11:46:41.126Z","comments":true,"path":"2019/09/18/Zip-압축-풀-때-한글-파일명-오류-해결법/","link":"","permalink":"https://p829911.github.io/2019/09/18/Zip-압축-풀-때-한글-파일명-오류-해결법/","excerpt":"","text":"Windows 에서 압축(zip)한 파일을 Linux에서 압축풀때, 한글로 되어 있는 파일이 깨져서 나올 때가 있다. 이는 Windows의 한글 문자셋(CP949)과 Linux의 한글 문자셋(UTF-8)이 다르기에 발생하는 문제이다. 압축해제 명령 1unzip filename.zip 인코딩 지정 1unzip -O cp949 filename.zip 기본 환경설정에 추가 1234vi ~/.profileexport UNZIP=\"-O cp949\"export ZIPINFO=\"-O cp949\"","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"Python Flask Tutorial: Full-Featured Web App Part 1 - Getting Started","slug":"Python-Flask-Tutorial-Full-Featured-Web-App-Part-1-Getting-Started","date":"2019-02-10T08:05:20.000Z","updated":"2019-02-10T08:20:04.971Z","comments":true,"path":"2019/02/10/Python-Flask-Tutorial-Full-Featured-Web-App-Part-1-Getting-Started/","link":"","permalink":"https://p829911.github.io/2019/02/10/Python-Flask-Tutorial-Full-Featured-Web-App-Part-1-Getting-Started/","excerpt":"","text":"Python Flask Tutorial: Full-Featured Web App Part 1 - Getting Started pyenv로 flask 폴더 생성 후 12pip install flaskvi hello.py 123456from flask import Flaskapp = Flask(__name__)@app.route('/')def hello(): return \"Hello World\" 터미널 이동 후 12export FLASK_APP=hello.pyflask run 위에 보이는 http://127.0.0.1:5000/ or http://localhost:5000/ 에서 결과물을 확인 할 수 있다. 123456from flask import Flaskapp = Flask(__name__)@app.route('/')def hello(): return \"&lt;h1&gt;Hello World&lt;/h1&gt;\" html 코드를 한 줄 넣어서 홈페이지를 새로고침 하면 바뀌지 않는다. 터미널에서 CTRL+C로 flask 실행 중지 후 flask run 으로 다시 실행해야 한다. 위의 작업을 하지 않으려면 Debug mode 를 사용해야 하는데 1export FLASK_DEBUG=1 터미널 창에서 위의 코드를 실행 후 flask run 을 통해 flask를 실행해 준다. 실행 후 확인 코드 123456from flask import Flaskapp = Flask(__name__)@app.route('/')def hello(): return \"&lt;h1&gt;Home Page&lt;/h1&gt;\" 홈페이지 내에서 새로고침만 해도 결과물이 바뀐다. 지금까지는 터미널 모드에서 환경변수를 지정해주므로써 flask를 실행하는 방법을 알아 봤고 python을 이용해 직접 실행할 수 있는 방법을 알아보자. 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def hello(): return \"&lt;h1&gt;Home Page&lt;/h1&gt;\"if __name__ == '__main__': app.run(debug=True) 지금 까지 작성한 코드 밑에 if ~ 코드를 추가해 주면 python을 통해 직접 실행 되도록 하는 방법인데 이 경우 터미널에서 1python hello.py 작성만으로 flask를 실행 할 수 있다. __name__ 이라는 인자는 Python 을 이용해 직접 실행하면 __main__ 이 되고 이 module을 다른 장소에서 import 할 경우 __name__ 이라는 인자는 module 의 이름이 된다 (hello.py) 따라서 위의 코드 밑에 추가한 if ~ 부분은 Python 을 통해 직접 실행할 때만 실행되는 코드이다. about 페이지 추가localhost:5000/about의 주소로 들어가면 404 response가 나온다 404 response는 페이지가 존재하지 않는다는 소리이다. 이제 about 페이지를 추가해보자. 1234567891011121314from flask import Flaskapp = Flask(__name__)@app.route('/')@app.route('/home')def hello(): return \"&lt;h1&gt;Home Page&lt;/h1&gt;\"@app.route('/about')def about(): return \"&lt;h1&gt;About Page&lt;/h1&gt;\"if __name__ == '__main__': app.run(debug=True) 두가지 코드를 추가했는데 하나는 home page의 route 하나를 추가하였다. @app.route(&#39;/home&#39;) 또 하나는 @app.route(&#39;/about&#39;) about 페이지를 추가하였다. 이렇게 코드 한줄로 페이지를 쉽게 만들 수 있다.","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"},{"name":"Flask","slug":"Python/Flask","permalink":"https://p829911.github.io/categories/Python/Flask/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Flask","slug":"Flask","permalink":"https://p829911.github.io/tags/Flask/"}]},{"title":"타겟 넘버","slug":"타겟-넘버","date":"2019-02-06T14:48:02.000Z","updated":"2019-02-06T14:53:26.946Z","comments":true,"path":"2019/02/06/타겟-넘버/","link":"","permalink":"https://p829911.github.io/2019/02/06/타겟-넘버/","excerpt":"","text":"문제 설명n개의 음이 아닌 정수가 있습니다. 이 수를 적절히 더하거나 빼서 타겟 넘버를 만들려고 합니다. 예를 들어 [1, 1, 1, 1, 1]로 숫자 3을 만들려면 다음 다섯 방법을 쓸 수 있습니다. 12345-1+1+1+1+1 = 3+1-1+1+1+1 = 3+1+1-1+1+1 = 3+1+1+1-1+1 = 3+1+1+1+1-1 = 3 사용할 수 있는 숫자가 담긴 배열 numbers, 타겟 넘버 target이 매개변수로 주어질 때 숫자를 적절히 더하고 빼서 타겟 넘버를 만드는 방법의 수를 return 하도록 solution 함수를 작성해주세요. 제한사항 주어지는 숫자의 개수는 2개 이상 20개 이하입니다. 각 숫자는 1 이상 50 이하인 자연수입니다. 타겟 넘버는 1 이상 1000 이하인 자연수입니다. 입출력 예 numbers target return [1, 1, 1, 1, 1] 3 5 입출력 예 설명문제에 나온 예와 같습니다. 나의 풀이1234567891011121314numbers = [1,1,1,1,1]target = 3from itertools import combinationsdef solution(numbers, target): total = 0 for i in range(1,len(numbers)+1): for j in combinations(range(len(numbers)),i): num = numbers.copy() for z in j: num[z]=-num[z] if sum(num) == target: total += 1 return total 다른 사람의 풀이123456789def solution(numbers, target): # number가 비어있고, target 이 0이면 1을 리턴해라 if (not numbers) and (not target): return 1 # number가 비어있고 target 이 0이 아니면 0을 리턴해라 elif not numbers: return 0 else: return solution(numbers[1:], target-numbers[0]) + solution(numbers[1:], target+numbers[0]) 위와 똑같은 코드를 lambda 함수로 구현 1234solution = lambda numbers, target: 1 \\if not numbers and target == 0 \\else 0 if not numbers \\else solution(numbers[1:], target - numbers[0]) + solution(numbers[1:], target + numbers[0]) Python Combinations &amp; Permutation12345678910111213141516171819from itertools import permutationsfrom itertools import combinationsperm = permutations([1,2,3])for i in perm: print(i)# (1, 2, 3)# (1, 3, 2)# (2, 1, 3)# (2, 3, 1)# (3, 1, 2)# (3, 2, 1)comb = combinations([1,2,3],2)for i in comb: print(i)# (1, 2)# (1, 3)# (2, 3) Combinations Functions Structures123456789101112131415161718def combinations(iterable, r): # combinations('ABCD', 2) --&gt; AB AC AD BC BD CD pool = tuple(iterable) n = len(pool) if r &gt; n: return indices = list(range(r)) yield tuple(pool[i] for i in indices) while True: for i in reversed(range(r)): if indices[i] != i + n - r: break else: return indices[i] += 1 for j in range(i+1, r): indices[j] = indices[j-1] + 1 yield tuple(pool[i] for i in indices) 위의 코드를 이해하자. 생소한 for ~ else 구문도 이해하고 넘어가기.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"올바른 괄호","slug":"올바른-괄호","date":"2019-02-02T12:37:59.000Z","updated":"2019-02-02T12:39:59.651Z","comments":true,"path":"2019/02/02/올바른-괄호/","link":"","permalink":"https://p829911.github.io/2019/02/02/올바른-괄호/","excerpt":"","text":"문제 설명올바른 괄호란 두 개의 괄호 ‘(‘ 와 ‘)’ 만으로 구성되어 있고, 괄호가 올바르게 짝지어진 문자열입니다. 괄호가 올바르게 짝지어졌다는 것은 ‘(‘ 문자로 열렸으면 반드시 짝지어서 ‘)’ 문자로 닫혀야 합니다.예를들어 “()()” 또는 “(())()” 는 올바른 괄호입니다. “)()(“ 또는 “(()(“ 는 올바르지 않은 괄호입니다. ‘(‘ 또는 ‘)’ 로만 이루어진 문자열 s가 주어졌을 때, 문자열 s가 올바른 괄호이면 true를 return 하고, 올바르지 않은 괄호이면 false를 return하는 solution 함수를 완성해 주세요. 제한사항 문자열 s의 길이 : 100,000 이하의 자연수 문자열 s는 ‘(‘ 또는 ‘)’ 로만 이루어져 있습니다. 입출력 예 s answer “()()” true “(())()” true “)()(“ false “(()(“ false 입출력 예 설명입출력 예 #1,2,3,4 문제의 예시와 같습니다. 나의 첫번째 풀이123456def solution(s): for _ in range(50000): s = s.replace(\"()\",\"\") if not s: return True return False 효율성 테스트 실패나의 두번째 풀이12345678910def solution(s): if len(s) % 2: return False while True: s_rep = s.replace(\"()\",\"\") if s_rep == s: return False elif not s_rep: return True s = s_rep 효율성 테스트 실패나의 세번째 풀이12345678910111213def solution(s): num = 0 for char in s: if num &lt; 0: return False if char == \"(\": num += 1 elif char == \")\": num -= 1 if not num: return True else: return False 통과","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"124 나라의 숫자","slug":"124 나라의 숫자","date":"2019-01-31T14:02:06.000Z","updated":"2019-01-31T14:04:46.372Z","comments":true,"path":"2019/01/31/124 나라의 숫자/","link":"","permalink":"https://p829911.github.io/2019/01/31/124 나라의 숫자/","excerpt":"","text":"문제 설명124 나라가 있습니다. 124 나라에서는 10진법이 아닌 다음과 같은 자신들만의 규칙으로 수를 표현합니다. 124 나라에는 자연수만 존재합니다. 124 나라에는 모든 수를 표현할 때 1, 2, 4만 사용합니다. 예를 들어서 124 나라에서 사용하는 숫자는 다음과 같이 변환됩니다. 10진법 124 나라 10진법 124 나라 1 1 6 14 2 2 7 21 3 4 8 22 4 11 9 24 5 12 10 41 자연수 n이 매개변수로 주어질 때, n을 124 나라에서 사용하는 숫자로 바꾼 값을 return 하도록 solution 함수를 완성해 주세요. 제한사항 n은 500,000,000이하의 자연수 입니다. 입출력 예 n result 1 1 2 2 3 4 4 11 나의 풀이1234567891011121314151617def solution(n): ls = [] three_num = 1 while True: if n % (3**three_num) == 0: ls.append(str(4)) n -= (3**three_num) else: ls.append(str((n % (3**three_num)) // (3**(three_num-1)))) n -= n%(3**three_num) if n == 0: break three_num += 1 return \"\".join(ls[::-1]) 다른 사람의 풀이1234567891011def change124(n): num = ['1','2','4'] answer = \"\" while n &gt; 0: n -= 1 answer = num[n % 3] + answer n //= 3 return answer 다른 유형의 진법 문제 많이 풀어보기","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"동전줍기 & 채점하기","slug":"동전줍기-채점하기","date":"2019-01-25T11:59:20.000Z","updated":"2019-01-25T12:02:05.042Z","comments":true,"path":"2019/01/25/동전줍기-채점하기/","link":"","permalink":"https://p829911.github.io/2019/01/25/동전줍기-채점하기/","excerpt":"","text":"동전 줍기길에 떨어져 있는 많은 동전들의 위치와 갯수를 의미하는 리스트 A가 있습니다. 당신은 길위에 동전을 수집하려고 합니다. 출발하는 위치 k와 이동가능한 거리를 m이 주어질때,가장 많은 동전을 획득하려고하면 몇개를 획득할 수 있는지 알려주는 함수를 만드세요. 예를 들어 리스트 A와 k, m이 아래와 같을때 123A = [2, 3, 7, 5, 1, 3, 9]k=4m=6 가장 많은 동전을 주울수있는 방법은 아래와 같고 12345674번째에서 출발 - 1개 획득 왼쪽으로 이동 - 5개 획득 (1번이동)왼쪽으로 이동 - 7개 획득 (2번이동)오른쪽으로 이동 - 0개 획득 (3번이동)오른쪽으로 이동 - 0개 획득 (4번이동)오른쪽으로 이동 - 3개 획득 (5번이동)오른쪽으로 이동 - 9개 획득 (6번이동) 총1, 5, 7, 3, 9 = 25개의 버섯을 주울 수 있다. 25를 리턴하는 함수 solution을 생성하세요. 조건 def solution(A, k, m): 1 &lt;= len(A) &lt;= 10**5 0 &lt;= k, m &lt;= 10**5 나의 풀이123456789101112131415161718192021222324252627282930313233343536373839def solution(A, k, m): acc_ls = [] sum = A[k] for i in range(k-1,-1,-1): sum += A[i] acc_ls.append(sum) acc_ls = sorted(acc_ls, reverse=True) acc_ls.append(A[k]) sum = 0 for j in range(k+1,len(A)): sum += A[j] acc_ls.append(sum) result = 0 min = k - m if min &lt; 0: min = 0 max = k + m if max &gt; len(A)-1: max = len(A)-1 result = A[min] if result &lt; A[max]: result = A[max] for i in range(1, round(m/2)): up = i down = m - 2*i if up + down &gt; m: continue try: if acc_ls[k+up] + acc_ls[k-down] &gt; result: result = acc_ls[k+up] + acc_ls[k-down] if acc_ls[k-up] + acc_ls[k+down] &gt; result: result = acc_ls[k-up] + acc_ls[k+down] except: continue return result 강사님 풀이12345678910111213141516171819202122232425262728from math import ceildef accumulate(A): result = [0] for a in A: result.append(result[-1]+a) return resultdef range_sum(A, x, y): return A[y+1] - A[x]def solution(A, k, m): acc_A = accumulate(A) result = A[k] # 왼쪽으로 출발 후에 오른쪽으로 방향 이동 for left in range(0, ceil(m/2)): # 왼쪽으로 0번 이동한 것 포함, 즉 오른쪽으로만 이동한 경우 포함 right = m - (left*2) left_idx = max(k-left, 0) # IndexError 방지 right_idx = min(len(A)-1, k+right) # IndexError 방지 result = max(result, range_sum(acc_A, left_idx, right_idx)) # 오른쪽으로 출발 후에 왼쪽으로 방향 이동 # 동일하게 작성 return result 채점하기매주 금요일 알고리즘 테스트 결과를 채점하기 귀찮아졌습니다. 그래서 직접하지 않고, 수강생분들에게 공부가 된다는 사탕발림으로 서로의 시험결과를 채점하게 하려고합니다. 단, 양심적으로 진행하기 위해서 그 누구도 자신의 시험을 채점하지 않는다고 할때. 채점할 수 있는 경우의 수를 구하는 함수를 구하세요. 는 n=1일때 0개n=2일때 1개그리고 n&gt;=3일때 (n-1) * ( (n-1)명이서 서로 채점하는 경우의 수 + (n-2)명이서 서로 채점하는 경우의 수)) 라는 특징을 따른다고 합니다. def solution(n):함수를 생성하세요. 조건 1 &lt;= n &lt;= 1000 1234def solution(n): if n &lt;= 2: return n-1 return (n-1) * (solution(n-1) + solution(n-2)) memorization function 중요12345678def deco(func): m = &#123;&#125; def inner(n): if not m.get(n): result = func(n) m[n] = result return m[n] return inner 12345@decodef solution(n): if n &lt;= 2: return n-1 return (n-1) * (solution(n-1) + solution(n-2)) 코드 실행시간 정확히 측정하는법1234567import timestart = time.perf_counter()end = time.perf_counter()end - start","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"정규표현식","slug":"정규표현식","date":"2019-01-25T11:53:18.000Z","updated":"2019-01-25T11:55:14.937Z","comments":true,"path":"2019/01/25/정규표현식/","link":"","permalink":"https://p829911.github.io/2019/01/25/정규표현식/","excerpt":"","text":"1. 정규표현식 (Regular Expression)정규 표현식은 특정한 규칙을 가진 문자열의 패턴을 표현하는 데 사용하는 표현식(Expression)으로 텍스트에서 특정 문자열을 검색하거나 치환할 때 흔히 사용된다. 예를 들어, 웹페이지에서 전화번호나 이메일 주소를 발췌한다거나 로그파일에서 특정 에러메시지가 들어간 라인들을 찾을 때 정규 표현식을 사용하면 쉽게 구현할 수 있다. 정규 표현식은 간단히 정규식, Regex 등으로 불리우곤 한다. 2. 정규 표현식 사용정규식에서 가장 단순한 것은 특정 문자열을 직접 리터럴로 사용하여 해당 문자열을 검색하는 것이다. 예를 들어, 로그 파일에 “에러 1033” 이라는 문자열을 검색하여 이 문자열이 있으면 이를 출력하고 없으면 None을 리턴하는 경우이다. 이러한 간단한 검색을 파이썬에서 실행하는 방법은 다음과 같다. 먼저 파이썬에서 정규표현식을 사용하기 위해서는 Regex를 위한 모듈인 re 모듈을 사용한다 re 모듈의 compile 함수는 정규식 패턴을 입력으로 받아들여 정규식 객체를 리턴하는데, 즉 re.complie(검색할 문자열) 와 같이 함수를 호출하면 정규식 객체 (re.RegexObject 클래스 객체)를 리턴하게 된다.re.RegexObject 클래스는 여러 메서드들을 가지고 있는데, 이 중 여기서는 특정 문자열을 검색하여 처음 맞는 문자열을 리턴하는 search() 메서드를 사용해 본다. 이 search() 는 처음 매칭되는 문자열만 리턴하는데, 매칭되는 모든 경우를 리턴하려면 findall() 을 사용한다. search()는 검색 대상이 있으면 결과를 갖는 MatchObject 객체를 리턴하고, 맞는 문자열이 없으면 None 을 리턴한다.MatchObject 객체로부터 실제 결과 문자열을 얻기 위해서는 group()메서드를 사용한다. 12345678import retext = \"에러 1122: 레퍼런스 오류\\n 에러 1033: 아규먼트 오류\"regex = re.compile(\"에러 1033\")mo = regex.search(text)if mo != None: print(mo.group())# 에러 1033 3. 전화번호 발췌하기정규 표현식은 단순한 리터럴 문자열을 검색하는 것보다 훨씬 많은 기능을 제공하는데, 즉 특정 패턴의 문자열을 검색하는데 매우 유용하다. 그 한가지 예로 웹페이지나 텍스트에서 특정 패턴의 전화번호를 발췌하는 기능에 대해 알아보자. 전화번호의 패턴은 032-232-3245와 같이 3자리-3자리-4자리로 구성되어 있다고 가정하자. 정규식에서 숫자를 의미하는 기호로 \\d 를 사용한다. 여기서 d는 digit 을 의미하고 0 ~ 9까지의 숫자 중 아무 숫자나 될 수 있다. 따라서, 위 전화번호 패턴을 정규식으로 표현하면 \\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d 와 같이 될 수 있다. 아래는 이러한 패턴을 사용하여 전화번호를 발췌하는 예이다. 12345678910import retext = \"문의사항이 있으면 032-232-3245 으로 연락주시기 바랍니다.\"regex = re.compile(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d')matchobj = regex.search(text)phonenumber = matchobj.group()print(phonenumber)# 032-232-3245 위 예제에서 re.compile(전화번호패턴) 함수는 전화번호 패턴에 맞는 정규식 객체를 리턴하게 되고, search()를 사용하여 첫번째 전화번호 패턴에 매칭되는 번호를 리턴한다. 그리고 이로부터 실제 전화번호를 얻기 위해서는 group() 메서드를 사용하였다. 4. 다양한 정규식 패턴 표현위의 전화번호 예제에서는 숫자를 표현하는 \\d 만을 살펴보았는데, 정규표현식에는 매우 다양한 문법과 기능들이 제공되고 있다. 아래는 이러한 다양한 정규식 표현 중 자주 사용되는 패턴들을 정리한 것이다. 패턴 설명 예제 ^ 이 패턴으로 시작해야 함 ^abc : abc로 시작해야 함 (abcd, abc12 등) $ 이 패턴으로 종료되어야 함 xyz$ : xyz로 종료되어야 함 (123xyz, strxyz 등) [문자들] 문자들 중에 하나이어야 함. 가능한 문자들의 집합을 정의함. [Pp]ython : “Python” 혹은 “python” 문자들 [문자들]의 반대로 피해야할 문자들의 집합을 정의함. aeiou : 소문자 모음이 아닌 문자들 \\ 두 패턴 중 하나이어야 함 (OR 기능) a \\ b : a 또는 b 이어야 함 ? 앞 패턴이 없거나 하나이어야 함 (Optional 패턴을 정의할 때 사용) \\d? : 숫자가 하나 있거나 없어야 함 + 앞 패턴이 하나 이상이어야 함 \\d+ : 숫자가 하나 이상이어야 함 * 앞 패턴이 0개 이상이어야 함 \\d* : 숫자가 없거나 하나 이상이어야 함 패턴{n} 앞 패턴이 n번 반복해서 나타나는 경우 \\d{3} : 숫자가 3개 있어야 함 패턴{n, m} 앞 패턴이 최소 n번, 최대 m 번 반복해서 나타나는 경우 (n 또는 m 은 생략 가능) \\d{3,5} : 숫자가 3개, 4개 혹은 5개 있어야 함 \\d 숫자 0 ~ 9 \\d\\d\\d : 0 ~ 9 범위의 숫자가 3개를 의미 (123, 000 등) \\w 문자를 의미 \\w\\w\\w : 문자가 3개를 의미 (xyz, ABC 등) \\s 화이트 스페이스를 의미하는데, [\\t\\n\\r\\f] 와 동일 \\s\\s : 화이트 스페이스 문자 2개 의미 (\\r\\n, \\t\\t 등) . 뉴라인(\\n) 을 제외한 모든 문자를 의미 .{3} : 문자 3개 (F15, 0x0 등) 정규식 패턴의 한 예로 “에러 {에러번호}”와 같은 형식을 띄는 부분을 발췌해 내는 예제를 살펴보자. 여기서 에러 패턴은 “에러” 라는 리터럴 문자열과 공백 하나, 그 뒤에 1개 이상의 숫자이다. 이를 표현하면 아래와 같다. 123456import retext = \"에러 1122 : 레퍼런스 오류\\n 에러 1033: 아규먼트 오류\"regex = re.compile(\"에러\\s\\d+\")mc = regex.findall(text)print(mc)# 출력: ['에러 1122', '에러 1033'] 위 예제는 첫번째 패턴 매칭값을 리턴하는 seach() 메서드 대신 패턴에 매칭되는 모든 결과를 리턴하는 findall()을 사용하였다. findall()는 결과 문자열의 리스트(list)를 리턴한다. 5. 정규식 그룹(Group)정규식 표현식에서 () 괄호는 그룹을 의미한다. 예를 들어, 전화번호의 패턴을\\d{3}-\\d{3]-\\d{4} 와 같이 표현하였을 때, 지역번호 3자를 그룹1으로 하고 나머지 7자리를 그룹2로 분리하고 싶을 때, (\\d{3})-(\\d{3}-\\d{4}) 와 같이 둥근 괄호로 묶어 두 그룹으로 분리할 수 있다. 이렇게 분리된 그룹들은 MatchObject의 group() 메서드에서 그룹 번호를 파라미터로 넣어 값을 가져올 수 있는데, 첫번째 그룹 지역번호는 group(1)으로, 두번째 그룹은 group(2) 와 같이 사용한다. 그리고 전체 전화번호를 가져올 때는 group() 혹은 group(0) 을 사용한다. 12345678910import retext = \"문의사항이 있으면 032-232-3245 으로 연락주시기 바랍니다.\"regex = re.compile(r'(\\d&#123;3&#125;)-(\\d&#123;3&#125;-\\d&#123;4&#125;)')matchobj = regex.search(text)areaCode = matchobj.group(1)num = matchobj.group(2)fullNum = matchobj.group()print(areaCode, num) # 032 232-3245 그룹을 위와 같이 숫자로 인덱싱하는 대신 그룹이름을 지정할 수도 있는데 이를 정규식에서 Named Capturing Group 이라 한다. 파이썬에서 Named Capturing Group을 사용하는 방법은 (?P&lt;그룹명&gt;정규식) 와 같이 정규식 표현 앞에 ?P&lt;그룹명&gt; 을 쓰면 된다. 그리고 이후 MatchObject에서 group(‘그룹명’) 을 호출하면 캡쳐된 그룹 값을 얻을 수 있다. 123456789import retext = \"문의사항이 있으면 032-232-3245 으로 연락주시기 바랍니다.\"regex = re.compile(r'(?P&lt;area&gt;\\d&#123;3&#125;)-(?P&lt;num&gt;\\d&#123;3&#125;-\\d&#123;4&#125;)')matchobj = regex.search(text)areaCode = matchobj.group(\"area\")num = matchobj.group(\"num\")print(areaCode, num) # 032 232-3245 출처","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Regular Expression","slug":"Regular-Expression","permalink":"https://p829911.github.io/tags/Regular-Expression/"}]},{"title":"파이썬 package 배포","slug":"파이썬-package-배포","date":"2019-01-21T07:08:56.000Z","updated":"2019-01-21T07:10:12.349Z","comments":true,"path":"2019/01/21/파이썬-package-배포/","link":"","permalink":"https://p829911.github.io/2019/01/21/파이썬-package-배포/","excerpt":"","text":"setup.py 설정setup.py 파일을 프로젝트의 root 디렉토리에 생성한다. 대부분의 빌드 설정을 setup.py 를 통해서 한다. setup.py 파일을 통해 할 수 있는 설정들은 여러가지가 있는데 그 중 가장 자주 쓰이는 설정들은 아래와 같다. name: 프로젝트 이름 version: 배포 버전 description: 프로젝트 설명 author: 프로젝트 저자 author_email: 저자의 이메일 url: 프로젝트 사이트 주소. 오픈소스의 경우 대부분 깃헙 주소를 사용한다. download_url: 해당 라이브러리의 실행 바이너리 다운 받는 주소. 마찬가지로 오픈소스의 경우 대부분 깃헙의 archive 주소를 설정한다. install_requires: 해당 라이브러리를 사용하기 위해서 인스톨 되야하는 dependency package, 해당 라이브러리를 pip 를 통해 인스톨 할 때 이곳에 나열된 라이브러리들을 같이 인스톨한다. packages: 빌드에 포함될 package들. 한 프로젝트에 여러 package가 있을수도 있고 또한 빌드에서 제외할 package들 (예를 들어 test 코드나 doc)이 있을 수 있으므로 그러한 설정을 한다. 대부분 setuptools.find_packages 함수를 사용하여 자동으로 포함될 package들을 찾게 하고 대신에 제외되어야 할 package 들을 exclude 인자를 통해 설정해준다. keywords: 해당 프로젝트 관련 키워드 python_requires: 서포트 하는 파이썬 버전 설정 Python 3.6 에서만 실행된다면: =3.6 Python 3 버전 이상에서는 다 실행된다면: &gt;=3 package_data: 기본적으로 setup 은 파이썬 파일만 빌드에 포함시킨다. 파이썬 파일이 아닌 외부 파일을 포함시키기 위해선 포함시키고자 하는 파일들을 이 옵션에 명시해 줘야 포함된다. 그렇지 않으면 포함이 되지 않는다. 이 옵션 설정을 해주지 않거나 잘못 설정해주어서 문제가 생기는 경우가 있으므로 주의해야 한다. zip_safe: 위의 package_data 설정을 하였으면 zip_safe 설정도 해주어야 하며 False 로 설정해 주어야 한다. classifiers: PYPI에 등록될 메타 데이터 설정이다. 예를 들어 서포트 하는 python 버전 정보를 명시할 수 있다. 하지만 이건 PYPI에 등록될 메타 데이터일 뿐이고 실제 빌드에는 영향을 주지 않는다. 123456789101112131415161718192021222324252627282930from setuptools import setup, find_packagessetup( name = 'pyquibase', version = '1.0', description = 'Python wrapper for liquibase', author = 'Eun Woo Song', author_email = 'songew@gmail.com', url = 'https://github.com/rampart81/pyquibase', download_url = 'https://githur.com/rampart81/pyquibase/archive/1.0.tar.gz', install_requires = [ ], packages = find_packages(exclude = ['docs', 'tests*']), keywords = ['liquibase', 'db migration'], python_requires = '&gt;=3', package_data = &#123; 'pyquibase' : [ 'db-connectors/sqlite-jdbc-3.18.0.jar', 'db-connectors/mysql-connector-java-5.1.42-bin.jar', 'liquibase/liquibase.jar' ]&#125;, zip_safe=False, classifiers = [ 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.2', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6' ]) setuptools 를 사용해 setup 설정을 해줬기 때문에 아직 setuptools 가 인스톨 되어 있지 않다면 인스톨 한다. 1pip install setuptools Setup.cfg만일 README 파일이 마크다운 형태로 되어 있다면 setup.cfg 파일을 아래와 같이 설정 해준다. 12[metadata]description-file = README.md 빌드하기빌드 하는 방법은 몇가지가 있지만 공식적으로는 wheel 을 사용하여 빌드하는 것이 권장된다. wheel 은 build package인데 일반적인 source distribution보다 더 빨리 인스톨 되기에 공식적으로 권장 된다. 아래 커맨드를 실행하여 wheel 을 인스톨하고 빌드를 하도록 한다. 12pip install wheelpython setup.py bdist_wheel 위의 커맨드를 실행하면 dist 폴더가 생기고 그 안에 빌드 파일이 생성된다. 배포하기배포는 twine으ㄹ 사용하여 배포하도록 한다. python setup.py upload 커맨드를 사용하지 않고 twine 을 사용하는 이유는 upload 커맨드는 일반 HTTP를 사용해서 배포하기 때문에 아이디나 패스워드가 노출될 수 있는데 twine 은 TLS 를 사용하기 때문이다. 그리고 PYPI에 이미 계정이 있어야 하는데, 만일 PYPI 계정이 없다면 twine 이 그 또한 가이드 해준다. 아래 커맨드를 사용하여 PYPI에 배포한다. 1twine upload dist/&lt;build_file_name&gt;.whl 이제 pip install &lt;package name&gt; 을 실행하여 패키지를 어디서든 누구나 인스톨 할 수 있다. 주의할 점은 인스톨은 곧바로 되는데 검색은 PYPI가 인덱스 하는데 시간이 걸린다. 즉 pip search &lt;package name&gt; 명령어를 치면 곧바로 검색이 안될 수 있다. 배포한 package 관리는 PYPI 사이트에 로그인해서 해당 package의 페이지로 가면 된다. Tips Distutils VS Setuptools setup.py 설정을 하도록 해주는 라이브러리는 크게 distutils와 setuptools 2가지가 있다. distutils는 파이썬의 기본 라이브러리에 포함이 되어있고 setuptools는 별도로 인스톨을 해야한다. 그래서 distutils를 사용하는게 default라고 생각할수 있지만 실제로는 setuptools를 사용하는게 default 이다. distutils의 단점들을 보완한게 setuptools라서 대부분 setuptools를 사용하고 있고 파이썬 공식 사이트에서도 setuptools를 권장하고 있다. Package_data: 만일 파이썬 파일이 아닌 다른 binary 파일을 포함해줘야 한다면 (예를 들어 jar 파일) setup.py파일에서 package_data 설정을 꼭 해주어야 한다. 이 설정을 잘못하거나 하지 않으면 포함되어야 하는 파일들이 포함이 되지 않은체 빌드가 되서 작동이 잘 되지 않는 경우가 종종있다. 하지만 빌드는 성공하므로 문제가 생길때까지 몰르게 되는 경우가 많다. zip_safe 꼭 False로 같이 설정 해주어야 한다. PYPI에 package를 업로드 한후 검색이 되기까지는 시간이 걸리므로 주의하자. 인스톨은 곧바로 된다. 출처","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Package","slug":"Package","permalink":"https://p829911.github.io/tags/Package/"}]},{"title":"마지막 인덱스 찾기","slug":"마지막-인덱스-찾기","date":"2019-01-21T06:56:11.000Z","updated":"2019-01-21T07:07:50.404Z","comments":true,"path":"2019/01/21/마지막-인덱스-찾기/","link":"","permalink":"https://p829911.github.io/2019/01/21/마지막-인덱스-찾기/","excerpt":"","text":"1부터 M까지 숫자가 들어있는 길이가 N인 리스트에서, 각 숫자가 마지막으로 등장하는 index를 순차적으로 담은 리스트를 리턴하세요. 예제 m data return 3 [1, 2, 3, 1, 2, 3, 1] [6, 4, 5] 설명 1이 등장한 index는 0과 3과 6, 이중에 가장 마지막에 등장한 index 6 2가 등장한 index는 1과 4, 이중에 마지막인 4 3이 등장한 index는 2와 5, 이중에 마지막인 5 조건 M은 2보다 크고 1만보다 작은 숫자 N은 1보다 크고 10만보다 작은 숫자 123M = 3N = 7data = [1,2,3,1,2,3,1] 11234567891011121314def solution(M, N, data): dic = &#123;&#125; dic_ls = [0] * M for i in range(N-1, -1, -1): last = data[i] if last in dic: continue else: dic[last] = i dic_ls[last-1] = i if len(dic) == M: break return dic_ls 21234567891011121314151617def solution(M, N, data): dic_ls = [0] * M zero_count = M for i in range(N-1, -1, -1): last = data[i] if dic_ls[last-1] == 0: dic_ls[last-1] = i zero_count -= 1 else: continue if zero_count == 0: break return dic_ls","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"NumberPalindrome","slug":"NumberPalindrome","date":"2019-01-21T06:55:37.000Z","updated":"2019-01-21T07:06:35.595Z","comments":true,"path":"2019/01/21/NumberPalindrome/","link":"","permalink":"https://p829911.github.io/2019/01/21/NumberPalindrome/","excerpt":"","text":"0 에서 10**n-1 사이의 정수 10**n 개 중에서,그냥 보았을때, 그리고 역순으로 뒤집어서 보았을때 같은 숫자를 카운트 하는 함수를 작성하세요. 조건 def solution(n): n은 1부터 1000까지 정수중 하나 예 n = 1 인 경우 0부터 10**까지 즉 10까지 숫자중 회문이 성립하는 순자의 갯수를 카운트 1, 2, 3, 4, 5, 6, 7, 8, 9 이 성립 10을 리턴 뒤집어도 같은 정수란 3, 44, 12321 과 같은 정수들을 뜻합니다. 내가 푼 코드123456789101112131415161718def solution(n): total = 10 if n == 1: return total for i in range(11, 10**n): ls = list(str(i)) l = len(ls) if l % 2 == 1: a = int((l-1)/2) else: a = int(l/2) if ls[:a] == ls[-a:]: total += 1 return total 단순한 코드1234567def solution(n): count = 0 for number in range(0, (10**n)+1): str_number = str(number) if str_number == str_number[::-1]: count += 1 return count 효율적인 코드점화식으로 만든 후 재귀 함수 사용 123451자리수일때: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 -&gt; 10개2자리수일때: 11, 22, 33, 44 ,55 ,66 ,77 ,88, 99 -&gt; 9개3자리수일때: 1x1, 2x2, 3x3, ....,9x9 -&gt; 9 * 10개4자리수일때: 1xx1, 2xx2,......, 9xx9 -&gt; 9 * 10개5자리수일때: 1xyx1, ....... , 9xyx9 -&gt; 9 * 10 * 10개 1234def solution(n): if n == 1: return 10 return solution(n-1) + 9 * (10 ** ((n-1)//2)) 점화식 A1 = a \\\\ A2 = a + d \\\\ An = A(n-1) + da가 3 d가 2일 때 재귀함수 1234def a(n): if n == 1: return 3 return a(n-1) + 2 피보나치 수열1234def fib(n): if n &lt; 2: return n return fib(n-1) + fib(n-2) memorization12345678910def make_memo(func): me = &#123;&#125; def memo(n): if me.get(n): return me[n] else: result = func(n) me[n] = result return result return memo 12345@make_memodef fib(n): if n &lt; 2: return n return fib(n-1) + fib(n-2) functors 패키지의 leu_cache12345@lru_cache(maxsize=128)def fib(n): if n &lt; 2: return n return fib(n-1) + fib(n-2) decoration 추가 공부 하기풀기 전에 5분 ~ 10분 정도 충분히 생각한 후 가장 효율적인 방법이라고 생각하는 코드를 짠다. 코드 짜기 부터 하지 말자.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"누적합","slug":"누적합","date":"2019-01-21T06:55:22.000Z","updated":"2019-01-21T07:00:09.544Z","comments":true,"path":"2019/01/21/누적합/","link":"","permalink":"https://p829911.github.io/2019/01/21/누적합/","excerpt":"","text":"길이가 N인 리스트에서 누적합을 구해서 리턴하는 함수를 구하세요.1부터 N까지 숫자로 이뤄진 길이가 N인 리스트와, 구하려는 누적합의 시작하는 지점과 끝나는 지점을 담은 M개의 쿼리 데이터를 받습니다. 1234N = 5M = 5data = [10, 20, 30, 40, 50]queries = [[1, 3], [2, 4], [3, 5], [1, 5], [4, 4]] 일때 결과는 data에서 1번째부터 3번째까지의 누적합 : 60 2번째부터 4번째까지의 누적합 : 90 3번째부터 5번째까지의 누적합 : 120 1번째부터 5번째까지의 누적합 : 150 4번째부터 4번째까지의 누적합 : 40 즉 [60, 90, 120, 150, 40]을 리턴합니다. 조건 def solution(data, queries): N은 data의 길이, data요소의 최대값 1&lt;=N&lt;=십만 M은 queries의 길이 1&lt;=M&lt;=만 나의 코드1234N = 5M = 5data = [10, 20, 30, 40, 50]queries = [[1, 3], [2, 4], [3, 5], [1, 5], [4, 4]] 123456def solution(data, queries): ls = [] for i,j in queries: ls.append(sum(data[i-1:j])) return ls 주어진 조건 중 최대 길이의 sample 생성 123456from random import randintN = 100000M = 10000data = [randint(1, N) for _ in range(N)]q = [ sorted([randint(1, N), randint(1, N)]) for _ in range(M) ] 실행시간에서 S가 나오면 의심해 봐야 한다. 효율적인 코드prefix_sum: 사전에 미리 누적합을 구해놓는다. 내가 다시 푼 코드 12345678910111213def solution(data, queries): ls = [0] total = 0 for num in data: total += num ls.append(total) result = [] for start, end in queries: result.append(ls[end] - ls[start-1]) return result 아래와 같이 누적합을 itertools 패키지를 통해 구할 수 있다. 1234from itertools import accumulateacc_data = [0] + list(accumulate(data))acc_data","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"예산","slug":"예산","date":"2019-01-17T10:08:02.000Z","updated":"2019-01-17T10:11:08.752Z","comments":true,"path":"2019/01/17/예산/","link":"","permalink":"https://p829911.github.io/2019/01/17/예산/","excerpt":"","text":"문제 설명S사에서는 각 부서에 필요한 물품을 지원해 주기 위해 부서별로 물품을 구매하는데 필요한 금액을 조사했습니다. 그러나, 전체 예산이 정해져 있기 때문에 모든 부서의 물품을 구매해 줄 수는 없습니다. 그래서 최대한 많은 부서의 물품을 구매해 줄 수 있도록 하려고 합니다. 물품을 구매해 줄 때는 각 부서가 신청한 금액만큼을 모두 지원해 줘야 합니다. 예를 들어 1,000원을 신청한 부서에는 정확히 1,000원을 지원해야 하며, 1,000원보다 적은 금액을 지원해 줄 수는 없습니다. 부서별로 신청한 금액이 들어있는 배열 d와 예산 budget이 매개변수로 주어질 때, 최대 몇 개의 부서에 물품을 지원해 줄 수 있는지 return 하도록 solution 함수를 완성해주세요. 제한사항 d는 부서별로 신청한 금액이 들어있는 배열이며, 길이(전체 부서의 개수)는 1 이상 100 이하입니다. d의 각 원소는 부서별로 신청한 금액을 나타내며, 부서별 신청 금액은 1 이상 100,000 이하의 자연수입니다. budget은 예산을 나타내며, 1 이상 10,000,000 이하의 자연수입니다. 물품을 구매해 줄 수 있는 부서 개수의 최댓값을 return 하세요. 입출력 예 d budget result [1,3,2,5,4] 9 3 [2,2,3,3] 10 4 입출력 예 설명입출력 예 #1 각 부서에서 [1원, 3원, 2원, 5원, 4원]만큼의 금액을 신청했습니다. 만약에, 1원, 2원, 4원을 신청한 부서의 물품을 구매해주면 예산 9원에서 7원이 소비되어 2원이 남습니다. 항상 정확히 신청한 금액만큼 지원해 줘야 하므로 남은 2원으로 나머지 부서를 지원해 주지 않습니다. 위 방법 외에 3개 부서를 지원해 줄 방법들은 다음과 같습니다. 1원, 2원, 3원을 신청한 부서의 물품을 구매해주려면 6원이 필요합니다. 1원, 2원, 5원을 신청한 부서의 물품을 구매해주려면 8원이 필요합니다. 1원, 3원, 4원을 신청한 부서의 물품을 구매해주려면 8원이 필요합니다. 1원, 3원, 5원을 신청한 부서의 물품을 구매해주려면 9원이 필요합니다. 3개 부서보다 더 많은 부서의 물품을 구매해 줄 수는 없으므로 최대 3개 부서의 물품을 구매해 줄 수 있습니다. 입출력 예 #2 모든 부서의 물품을 구매해주면 10원이 됩니다. 따라서 최대 4개 부서의 물품을 구매해 줄 수 있습니다. 나의 풀이1234567891011def solution(d, budget): total = 0 cnt = 0 d = sorted(d) for i, bud in enumerate(d): total += bud if total &gt; budget: return cnt else: cnt += 1 return len(d) 개선 코드12345678def solution(d, budget): total = 0 d = sorted(d) for i, bud in enumerate(d): total += bud if total &gt; budget: return i return len(d) cnt 라는 불필요한 변수를 제거하였다.알고리즘 문제를 풀 때는 꼭 노트를 활용하자.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"그래프 확률 모형","slug":"그래프-확률-모형","date":"2019-01-16T12:54:47.000Z","updated":"2019-01-16T13:07:06.776Z","comments":true,"path":"2019/01/16/그래프-확률-모형/","link":"","permalink":"https://p829911.github.io/2019/01/16/그래프-확률-모형/","excerpt":"","text":"여러 확률변수의 결합분포를 구해야 하는 경우를 생각하자. 예를 들어 A, B, C, 3개의 확률변수가 있고 각 확률변수가 0, 1, 2 세가지의 값만 가질 수 있는 카테고리 확률변수인 경우 이 세 확률변수의 결합분포는 다음과 같이 표로 나타낼 수 있다. 이 표는 $3^3 - 1 = 26$ 개의 모수를 가지므로 (합이 1이 되어야 하므로 하나는 다른 값들에 의존한다.) 이 표를 저장하려면 26개의 저장 공간이 필요하다. A B C P(A, B, C) 0 0 0 P(A=0,B=0,C=0) 0 0 1 P(A=0,B=0,C=1) 0 0 2 P(A=0,B=0,C=2) 0 1 0 P(A=0,B=1,C=0) 0 1 1 P(A=0,B=1,C=1) 0 1 2 P(A=0,B=1,C=2) 0 2 0 P(A=0,B=2,C=0) 0 2 1 P(A=0,B=2,C=1) 0 2 2 P(A=0,B=2,C=2) 1 0 0 P(A=1,B=0,C=0) 1 0 1 P(A=1,B=0,C=1) 1 0 2 P(A=1,B=0,C=2) 1 1 0 P(A=1,B=1,C=0) 1 1 1 P(A=1,B=1,C=1) 1 1 2 P(A=1,B=1,C=2) 1 2 0 P(A=1,B=2,C=0) 1 2 1 P(A=1,B=2,C=1) 1 2 2 P(A=1,B=2,C=2) 2 0 0 P(A=2,B=0,C=0) 2 0 1 P(A=2,B=0,C=1) 2 0 2 P(A=2,B=0,C=2) 2 1 0 P(A=2,B=1,C=0) 2 1 1 P(A=2,B=1,C=1) 2 1 2 P(A=2,B=1,C=2) 2 2 0 P(A=2,B=2,C=0) 2 2 1 P(A=2,B=2,C=1) 2 2 2 P(A=2,B=2,C=2) 베이지안 네트워크 모형그런데 현실에서는 모든 확률변수가 서로 영향을 미치는 복잡한 경우 보다 특정한 몇개의 확률분포들이 서로 영향을 미친다. 예를 들어 A, B, C가 각각 어떤 학생의 A: 건강 상태 B: 공부 시간 C: 시험 성적 을 나타낸 것이라고 하자. 이 확률변수는 각각 $\\{0, 1, 2\\}$ 라는 값을 가질 수 있는데 하(0), 중(1), 상(2)의 상태를 나타낸다. 즉 $A = 0$이면 건강상태가 안좋은 것이고 $B = 1$ 이면 공부 시간이 보통이며 $C = 2$이면 시험 성적이 좋은 것이다. 공부 시간 $B$는 시험 성적 $C$에 영향을 미친다. 하지만 건강 상태 $A$는 공부 시간 $B$와 인과 관계가 있지만 시험 성적 $C$와는 직접적인 인과 관계가 없다. 이렇게 다수의 확률변수 중 특정한 소수의 확률변수들이 가지는 관계를 그래프로 표현한 것을 그래프 확률모형(graphical probability model)이라고 하고 그래프 확률모형 중에서도 이렇게 인과관계가 확실하여 방향성 그래프로 표시할 수 있는 것을 베이지안 네트워크 모형이라고 한다. 위의 확률변수를 베이지안 네트워크 모형으로 그리면 다음과 같다. 1234567891011import networkx as nxfrom IPython.core.display import Imagefrom networkx.drawing.nx_pydot import to_pydotg1 = nx.DiGraph()g1.add_path([\"A\", \"B\", \"C\"])d1 = to_pydot(g1)d1.set_dpi(300)d1.set_rankdir(\"LR\")d1.set_margin(0.2)Image(d1.create_png(), width=600) 이러한 그래프를 방향성 그래프(directed graph)라고 한다. 방향성 그래프에서 확률변수는 하나의 노드(node) 또는 정점(vertex)로 나타내고 인과관계는 화살표 간선(edge, link)으로 나타낸다. 우리가 다루는 방향성 그래프는 화살표가 여러 확률변수를 거쳐 자기 자신에게 돌아오는 루프(loop)가 없는 DAG(Directed Acyclic Graph) 모형이다. 방향성 그래프 모델에서는 원인과 결과가 되는 두 확률변수의 관계를 조건부 확률분포로 표현한다. 위의 모형에서처럼 A가 B의 원인이 된다면 이 두 확률변수의 관계를 $P(B \\mid A)$ 로 나타내고 B가 C의 원인이 되므로 두 확률변수의 관계는 $P(C \\mid B)$ 로 나타낸다. 그리고 전체 확률변수들간의 관계는 이러한 조건부 확률분포를 결합하여 나타낼 수 있다. 위의 그래프에서 전체 결합분포는 다음과 같다. P(A, B, C) = P(A)P(B \\mid A)P(C \\mid B)단, 여기에서 유의해야 할 점은 A와 B는 직접적인 인과 관계가 없지만 상관관계는 있을 수 있다는 점이다. 예를 들어 A-B, B-C간의 관계가 모두 양의 상관관계면 A가 커졌을 때 B도 커지고 따라서 C도 커지므로 A와 B가 양의 상관관계를 가지게 된다. A, B, C가 의미하는 바로 해석하면 건강 상태와 시험 성적은 직접적인 인과관계는 없지만 건강상태가 좋을 때 공부 시간도 많아질 가능성이 높고 공부 시간이 많을 때 시험 성적이 좋아진다면 건강 상태와 시험 성적은 양의 상관관계를 가질 수 있다. 결합확률분포를 이루는 팩터를 각각 표로 나타내면 다음과 같다. $A$ $𝑃(𝐴)$ $A=0$ $𝑃(𝐴=0)$ $A=1$ $𝑃(𝐴=1)$ $A=2$ $𝑃(𝐴=2)$ $B$ $𝑃(𝐵\\mid𝐴=0)$ $𝑃(𝐵\\mid 𝐴=1)$ $𝑃(𝐵\\mid 𝐴=2)$ $B=0$ $𝑃(𝐵=0 \\mid 𝐴=0)$ $𝑃(𝐵=0\\mid 𝐴=1)$ $𝑃(𝐵=0\\mid 𝐴=2)$ $B=1$ $𝑃(𝐵=1\\mid 𝐴=0)$ $𝑃(𝐵=1\\mid 𝐴=1)$ $𝑃(𝐵=1\\mid 𝐴=2)$ $B=2$ $𝑃(𝐵=2\\mid 𝐴=0)$ $𝑃(𝐵=2\\mid 𝐴=1)$ $𝑃(𝐵=2\\mid 𝐴=2)$ $C$ $𝑃(𝐶\\mid 𝐵=0)$ $𝑃(𝐶\\mid 𝐵=1)$ $𝑃(𝐶\\mid 𝐵=2)$ $C=0$ $𝑃(𝐶=0\\mid 𝐵=0)$ $𝑃(𝐶=0\\mid 𝐵=1)$ $𝑃(𝐶=0\\mid 𝐵=2)$ $C=1$ $𝑃(𝐶=1\\mid 𝐵=0)$ $𝑃(𝐶=1\\mid 𝐵=1)$ $𝑃(𝐶=1\\mid 𝐵=2)$ $C=2$ $𝑃(𝐶=2\\mid 𝐵=0)$ $𝑃(𝐶=2\\mid 𝐵=1)$ $𝑃(𝐶=2\\mid 𝐵=2)$ 이 경우에 우리가 알아야 하는 모수의 수는 $P(A): 3- 1 = 2$ $P(B \\mid A): (3-1) \\times 2 = 6$ $P(C \\mid B): (3-1) \\times 2 = 6$ 따라서 총 14개이다. 변수간의 인과 관계라는 추가 정보로 인해 모수의 숫자가 26개에서 14개로 크게 감소하였다. 이렇게 우리가 알고 있는 확률변수간의 정보를 그래프를 이용하여 추가하면 문제를 더 간단하게 만들 수 있다. pgmpy 패키지를 사용하여 앞의 예제를 파이썬으로 구현해 보자. 조건부 확률 $P(A), P(B \\mid A), P(C \\mid B)$ 는 TabularCPD 클래스로 다음과 같이 구현할 수 있다. 우선 건강 상태는 나쁠(A=0) 확률이 20%, 보통(A=1)일 확률이 60%, 좋을 확률이 20%라고 하자. 1234from pgmpy.factors.discrete import TabularCPDP_A = TabularCPD('A', 3, [[0.2, 0.6, 0.2]])print(P_A) 건강 상태가 나쁘면(A=0), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 50%, 30%, 20%이다. 건강 상태가 보통이면(A=1), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 20%, 60%, 20%이다. 건강 상태가 좋으면(A=2), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 20%, 30%, 50%이다. 1234P_B_I_A = TabularCPD('B', 3, np.array([[0.6, 0.2, 0.2], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]), evidence=['A'], evidence_card=[3])print(P_B_I_A) 비슷하게 공부시간이 적으면(B=0), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 80%, 10%, 10%이다. 공부시간이 보통이면(B=1), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 10%, 80%, 10%이다. 공부시간이 많으면(B=2), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 10%, 10%, 80%이다. 1234P_C_I_B = TabularCPD('C', 3, np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]), evidence=['B'], evidence_card=[3])print(P_C_I_B) 이 조건부 확률들을 결합하여 하나의 베이지안 네트워크로 만들려면 BayesianModel 클래스를 사용한다. 생성자에는 노드를 연결한 그래프 정보를 넣고 add_cpds 메서드로 조건부확률을 추가할 수 있다. 1234567from pgmpy.models import BayesianModelmodel = BayesianModel([('A', 'B'), ('B', 'C')])model.add_cpds(P_A, P_B_I_A, P_C_I_B)model.check_model()# True graphviz와 pydot 패키지를 사용하여 시각화 할 수도 있다. 12345678from IPython.core.display import Imagefrom networkx.drawing.nx_pydot import to_pydotd = to_pydot(model)d.set_dpi(300)d.set_margin(0.2)d.set_rankdir(\"LR\")Image(d.create_png(), width=600) 이렇게 만들어진 모형으로부터 여러가지 추론(inference)을 할 수 있다. 예를 들어 전체 결합확률분포 함수를 찾고 그 함수로부터 A, B, C의 marginal 확률분포를 계산하면 A, B, C의 값으로 어떤 값이 가장 확률이 높은지 알 수 있다. 분석 결과를 보면 시험 성적이 좋을 확률은 35.9%이다. 추론에 사용된 VariableElimination 클래스의 사용법에 대해서는 곧 학습한다. 1234from pgmpy.inference import VariableEliminationinference = VariableElimination(model)result = inference.query(variables=[\"C\"])print(result[\"C\"]) 베이지안 네트워크의 결합확률분포베이지안 네트워크를 만들려면 조사 대상이 되는 확률변수를 노드(node)로 생성하고 인과관계가 있는 노드를 방향성 간선(directed edge)로 연결하는 것이다. 이렇게 베이지안 네트워크가 만들어지면 이 확률변수들의 결합확률분포는 다음처럼 주어진다. P(X_1, \\cdots, X_N) = \\prod_{i=1}^N P(X_i \\mid Pa(X_i))이 식에서 $Pa(X_i)$ 는 $X_i$ 의 부모 노드이다. 1234567891011121314g1 = nx.DiGraph()g1.add_edge(\"X1\", \"X3\")g1.add_edge(\"X1\", \"X4\")g1.add_edge(\"X3\", \"X4\")g1.add_edge(\"X2\", \"X4\")g1.add_edge(\"X2\", \"X7\")g1.add_edge(\"X4\", \"X5\")g1.add_edge(\"X4\", \"X6\")d1 = to_pydot(g1)d1.set_dpi(300)d1.set_margin(0.2)d1.set_rankdir(\"LR\")Image(d1.create_png(), width=800) 예를 들어 $X_1, \\cdots, X_6$ 의 관계가 앞 그래프와 같다면 결합확률분포는 다음과 같다. P(X_1, X_2, X_3, X_4, X_5, X_6, X_7) = \\\\ P(X_1)P(X_2)P(X_3 \\mid X_1)P(X_4 \\mid X_2,X_3)p(X_5 \\mid X_4)P(X_6 \\mid X_4)P(X_7 \\mid X_2)조건부 독립베이지안 네트워크를 만들 때 중요한 것은 확률변수간의 조건부 독립 관계가 그래프에 나타나고 있어야 한다는 점이다. 조건부 독립(conditional independence)은 일반적인 독립과 달리 조건이 되는 확률변수가 존재해야 한다. 일반적으로는 확률변수 A, B가 독립인 정의는 다음과 같다. P(A, B) = P(A)P(B)조건부 독립은 조건이 되는 C라는 확률변수에 대한 조건부 결합확률분포에 대해 다음이 만족되어야 한다. P(A, B \\mid C) = P(A \\mid C) P(B \\mid C)즉, C에 대한 조건부 결합확률분포가 조건부 확률분포의 곱으로 나타난다. 기호로는 다음과 같이 표기한다. A \\text{⫫} B \\;\\vert\\; C같은 방식으로 무조건부 독립은 다음과 같이 표기하기도 한다. A \\text{⫫} B \\; \\vert\\; \\emptysetA, B가 C에 대해 조건부 독립이면 다음도 만족한다. P(A \\mid B, C) = P(A \\mid C)\\\\ P(B \\mid A, C) = P(B \\mid C)주의할 점은 조건부 독립과 (무조건부) 독립은 관계가 없다는 점이다. 즉, 두 확률변수가 독립이라고 항상 조건부 독립이 되는 것도 아니고 조건부 독립이라고 꼭 독립이 되는 것도 아니다. P(A, B) = P(A)P(B) \\;\\; \\bcancel{\\implies} \\;\\; P(A,B \\mid C) = P(A \\mid C)P(B \\mid C) \\\\ P(A,B \\mid C) = P(A \\mid C)P(B \\mid C) \\;\\; \\bcancel{\\implies} \\;\\; P(A, B) = P(A)P(B)방향성 분리방향성 분리(d-separation, directed separation) 정리는 방향성 그래프 모형에서 어떤 두 노드(확률변수)가 조건부 독립인지 아닌지 알아보는 방법이다. 다음과 같은 세가지 간선 결합을 알아야 한다. 꼬리-꼬리 결합 머리-꼬리 결합 머리-머리 결합 꼬리-꼬리 결합우선 다음과 같이 확률변수 A, B가 공통의 부모 C를 가지는 경우를 보자. C에서는 간선(화살표)의 꼬리가 2개 붙어 있기 때문에 C는 꼬리-꼬리(tail-to-tail) 결합이라고 한다. 1234567g2 = nx.DiGraph()g2.add_path([\"C\", \"A\"])g2.add_path([\"C\", \"B\"])d2 = to_pydot(g2)d2.set_dpi(300)d2.set_margin(0.2)Image(d2.create_png(), width=400) 이 때 A와 B는 독립은 아니지만 조건부 독립이 성립한다. P(A,B \\mid C) = \\dfrac{P(A, B, C)}{P(C)} = \\dfrac{P(A \\mid C)P(B \\mid C)P(C)}{P(C)} = P(A \\mid C)P(B \\mid C)이런 상태를 C가 A, B 사이를 막고있다(block)고 한다. 머리-꼬리 결합다음으로는 인과관계인 확률변수 A, B 사이에 C가 끼어 있는 경우를 살펴보자. 이때 노드 C에서 간선의 머리와 꼬리가 만나기 때문에 머리-꼬리(head-to-tail) 결합이라고 한다. 1234567g3 = nx.DiGraph()g3.add_path([\"A\", \"C\", \"B\"])d3 = to_pydot(g3)d3.set_dpi(300)d3.set_rankdir(\"LR\")d3.set_margin(0.2)Image(d3.create_png(), width=600) 이 경우에도 A와 B는 독립이 아니지만 조건부 독립이 성립한다. P(A,B|C) = \\dfrac{P(A, B, C)}{P(C)} = \\dfrac{P(A)P(C|A)P(B|C)}{P(C)} = \\dfrac{P(A,C)P(B|C)}{P(C)} = P(A|C)P(B|C)위의 경우와 마찬가지로 C가 A, B 사이를 막고(block) 있다고 한다. 머리-머리 결합마지막으로 두 확률변수 A, B를 부모로 가지는 C가 있는 경우를 살펴보자. 이러한 구조는 V-구조(V-structure) 또는 머리-머리(head-to-head) 결합이라고 한다. 1234567g4 = nx.DiGraph()g4.add_path([\"A\", \"C\"])g4.add_path([\"B\", \"C\"])d4 = to_pydot(g4)d4.set_dpi(300)d4.set_margin(0.2)Image(d4.create_png(), width=400) 이 경우에는 앞의 두 경우와 달리 A와 B가 독립이다. P(A,B,C) = P(A)P(B)P(C|A,B) \\\\ P(A,B) = \\sum_c P(A)P(B)P(C|A,B) = P(A)P(B)하지만 조건부 독립은 성립하지 않는다. 예를 들어 A가 늦잠을 자는 것을 나타내는 확률변수, B가 길이 막히는 것을 나타내는 확률변수, C가 지각하는 것을 나타내는 확률변수라고 할 때, 늦잠을 자는 것과 길이 막히는 것은 서로 독립이다. 하지만 일단 지각(C)이 발생한 상황에서는 A, B는 서로 독립이 아니며 이 경우에는 반-상관관계를 가진다. 즉, 늦잠을 자지 않았다면 길이 막혔을 가능성이 높아지고 길이 막히지 않았다면 늦잠을 잤을 가능성이 높아진다. 이러한 것을 explaining-out이라고 한다. 이러한 상황은 C가 A, B의 직접적 자식이 아니라 후손(descendent)인 경우에도 성립한다. 12345678g5 = nx.DiGraph()g5.add_path([\"A\", \"D\"])g5.add_path([\"B\", \"D\"])g5.add_path([\"D\", \"C\"])d5 = to_pydot(g5)d5.set_dpi(300)d5.set_margin(0.2)Image(d5.create_png(), width=400) 이상의 상황을 정리한 것이 바로 방향성 분리(d-separation) 정리이다. 방향성 분리(d-separation) 정리에 따르면 A와 B가 C에 대해서 조건부 독립인 경우는 다음 조건이 만족되어야 한다 C가 A, B 사이의 경로에 있는 꼬리-꼬리 결합이거나 머리-꼬리 결합이다. C가 A, B 사이의 경로상에 있는 머리-머리 결합 노드 혹은 그 자손이 아니어야 한다. 마코프 네트워크때로는 변수간의 인과관계가 순환(cycle)관계를 이루기 때문에 방향성이 있는 베이지안 네트워크로 구현할 수 없는 경우도 있다. 이 때는 무방향성 그래프(undirected graph)인 마코프 네트워크(Markov network)를 사용한다. 마코프 무작위장(Markov random field)라고도 한다. 예를 들어 3 X 3 이미지의 픽셀 9개의 값이 확률변수라고 하면 이 4개의 확률변수 중 어떤 2개도 서로 관련성을 가진다. 이 때는 다음과 같은 마코프 네트워크를 사용할 수 있다. 12345678910111213141516171819202122import networkx as nxfrom IPython.core.display import Imagefrom networkx.drawing.nx_pydot import to_pydotg1 = nx.Graph()g1.add_edge(\"X11\", \"X12\")g1.add_edge(\"X11\", \"X21\")g1.add_edge(\"X12\", \"X22\")g1.add_edge(\"X12\", \"X13\")g1.add_edge(\"X21\", \"X31\")g1.add_edge(\"X21\", \"X22\")g1.add_edge(\"X22\", \"X23\")g1.add_edge(\"X22\", \"X32\")g1.add_edge(\"X31\", \"X32\")g1.add_edge(\"X32\", \"X33\")g1.add_edge(\"X13\", \"X23\")g1.add_edge(\"X23\", \"X33\")d1 = to_pydot(g1)d1.set_dpi(300)d1.set_margin(0.5)d1.set_rankdir(\"LR\")Image(d1.create_png(), width=600) 클리크와 팩터마코프 네트워크는 클리크(clique)로 구성되는데 클리크를 구성하는 확률변수의 분포는 포텐셜 함수(potential function)또는 팩터(factor)로 표현할 수 있다. 팩터는 결합확률분포에 양의 상수를 곱한 함수이다. 결합확률분포에 비례하지만 모든 확률을 더해서 1이 되어야 한다는 조건이 없다. 마코프 네트워크의 확률분포마코프 네트워크의 결합확률분포는 마코프 네트워크를 구성하는 모든 클리크의 팩터의 곱으로 나타난다. P(X) = \\dfrac{1}{Z(X)} \\prod_{\\{C\\}} \\psi_C(X_C)이 식에서 $C$는 클리크, $X_C$ 는 그 클리크 안의 확률변수, $\\psi_C$ 는 그 클리크의 팩터, $\\{C\\}$ 는 모든 클리크의 집합, $Z$ 는 파티션 함수(partition)함수를 나타낸다. 예를 들어 3 X 3 이미지의 경우 9개의 확률변수의 결합확률분포는 다음처럼 표현할 수 있다. P(X_{11}, \\ldots, X_{33}) = \\dfrac{1}{Z} \\prod \\psi(X_{11}, X_{12}) \\psi(X_{11}, X_{21}) \\psi(X_{12}, X_{13}) \\cdots \\psi(X_{23}, X_{33}) \\psi(X_{32}, X_{33})에너지 함수팩터 함수는 다음과 같은 형태로 표시할 수 있다. \\psi(X) = \\exp(−E(X))이 식에서 $E(X)$ 를 에너지 함수(energy function)라고 한다. 확률이 높을수록 에너지 함수의 값은 작아진다. 예를 들어 0과 1이라는 값을 가지는 베르누이 확률변수 $X_1, X_2$가 다음과 같은 에너지 함수로 표현되는 경우, E(X_1, X_2) = -3(2X_1 - 1)(2X_2 - 1)팩터의 값을 구하면 \\psi(X_1 = 1, X_2 = 1) = e^3 \\\\ \\psi(X_1 = 0, X_2 = 0) = e^3 \\\\ \\psi(X_1 = 1, X_2 = 0) = e^{-3} \\\\ \\psi(X_1 = 0, X_2 = 1) = e^{-3} \\\\즉 $X_1, X_2$ 둘 다 같은 값을 가질 확률은 서로 다른 값을 가질 확률에 비해 높아진다. 즉 서로 양의 상관관계를 가지게 된다. pgmpy에서는 MarkovModel 클래스로 마코프 네트워크 모형을 구현한다. 12345678910111213141516171819202122from pgmpy.models import MarkovModeln = 3ginfo = []for i in range(n): for j in range(n): if j &lt; n - 1: v1 = \"X&#123;&#125;&#123;&#125;\".format(i + 1, (j + 1)) v2 = \"X&#123;&#125;&#123;&#125;\".format(i + 1, (j + 2)) ginfo.append((v1, v2)) if i &lt; n - 1: v1 = \"X&#123;&#125;&#123;&#125;\".format(i + 1, (j + 1)) v2 = \"X&#123;&#125;&#123;&#125;\".format(i + 2, (j + 1)) ginfo.append((v1, v2)) model = MarkovModel(ginfo)d = to_pydot(model)d.set_dpi(300)d.set_margin(0.2)d.set_rankdir(\"LR\")Image(d.create_png(), width=600) 팩터는 DiscreteFactor 클래스로 구현할 수 있다. 예를 들어 $X_{11}, X_{12}$ 의 팩터 함수가 𝑋12=0 𝑋12=1 𝑋11=0 10 1 𝑋11=1 1 10 이면 다음처럼 구현한다. 12345from pgmpy.factors.discrete import DiscreteFactorfactor = DiscreteFactor(['X11', 'X12'], cardinality=[2, 2], values=[[10, 1], [1, 10]])model.add_factors(factor) 이미지 완성마코프 네트워크의 예로 다음과 같은 두 종류의 5 X 5 이미지 데이터를 생각하자. 12345678910111213141516171819202122n_char = 2images = np.zeros((n_char, 5, 5))idx = []idx.append(np.array([ (0, 1), (0, 2), (0, 3), (1, 0), (1, 4), (2, 0), (2, 2), (2, 4), (3, 0), (3, 4), (4, 1), (4, 2), (4, 3), ]))idx.append(np.array([ (0, 0), (0, 4), (1, 1), (1, 3), (2, 2), (3, 1), (3, 3), (4, 0), (4, 4),]))for k, idx in enumerate(idx): for i, j in idx: images[k, i, j] = 1plt.figure(figsize=(6, 2))for i in range(n_char): plt.subplot(1, n_char, i + 1) plt.imshow(images[i], cmap=plt.cm.bone_r) plt.grid(False) plt.xticks(()) plt.yticks(()) plt.title(i) 다음과 같이 팩터 함수를 학습한다. 1234567891011121314151617181920212223242526272829303132333435363738394041from pgmpy.models import MarkovModelfrom pgmpy.factors.discrete import DiscreteFactordef get_factor(v1, v2, idx1, idx2): p00 = p01 = p10 = p11 = 0 for k in range(num_images): if images[k, idx1[0], idx1[1]] == 0 and images[k, idx2[0], idx2[1]] == 0: p00 += 1 if images[k, idx1[0], idx1[1]] == 0 and images[k, idx2[0], idx2[1]] == 1: p01 += 1 if images[k, idx1[0], idx1[1]] == 1 and images[k, idx2[0], idx2[1]] == 0: p10 += 1 if images[k, idx1[0], idx1[1]] == 1 and images[k, idx2[0], idx2[1]] == 1: p11 += 1 factor = DiscreteFactor([v1, v2], cardinality=[2, 2], values=[[p00, p01], [p10, p11]]) return factormodel = MarkovModel()num_images = images.shape[0]n1 = images.shape[1]n2 = images.shape[2]for i in range(n1): for j in range(n2): if j &lt; n2 - 1: v1 = \"X&#123;&#125;&#123;&#125;\".format(i + 1, (j + 1)) v2 = \"X&#123;&#125;&#123;&#125;\".format(i + 1, (j + 2)) model.add_edge(v1, v2) factor = get_factor(v1, v2, (i, j), (i, j + 1)) model.add_factors(factor) if i &lt; n1 - 1: v1 = \"X&#123;&#125;&#123;&#125;\".format(i + 1, (j + 1)) v2 = \"X&#123;&#125;&#123;&#125;\".format(i + 2, (j + 1)) model.add_edge(v1, v2) factor = get_factor(v1, v2, (i, j), (i + 1, j)) model.add_factors(factor)model.check_model()# True 학습된 팩터의 일부를 살펴보자. 일례로 $X_{11}$ 과 $X_{12}$ 의 결합확률 팩터는 다음과 같다. 1234f = model.get_factors(\"X11\")[0]f# &lt;discretefactor representing phi(x11:2, x12:2) at 0x7f3b741f2c18&gt; 1print(f) 다음은 전체 네트워크 모양이다. 12345d = to_pydot(model)d.set_dpi(300)d.set_margin(0.2)d.set_rankdir(\"LR\")Image(d.create_png(), width=600) 다음 코드는 이미지의 일부를 알고 있을 때 나머지 이미지를 추정한 결과이다. 코드에서 evidence는 관측된 이미지의 일부이다. 여기에서 사용하는 BeliefPropagation 클래스는 다음 절에서 공부한다. 12345678910111213141516171819202122232425262728293031323334353637from pgmpy.inference import BeliefPropagationinference = BeliefPropagation(model)vars = []for i in range(n1): for j in range(n2): vars.append(\"X&#123;&#125;&#123;&#125;\".format(i + 1, j + 1))evidence1 = &#123; \"X11\": 0, \"X21\": 1, \"X31\": 1, \"X41\": 1, \"X51\": 0, \"X15\": 0, \"X25\": 1, \"X35\": 1, \"X45\": 1, \"X55\": 0,&#125;evidence2 = &#123; \"X11\": 1, \"X21\": 0, \"X31\": 0, \"X41\": 0, \"X51\": 1, \"X15\": 1, \"X25\": 0, \"X35\": 0, \"X45\": 0, \"X55\": 1,&#125;evidence = evidence1for x in evidence: vars.remove(x) result = inference.map_query(variables=vars, evidence=evidence)result.update(evidence)image = np.zeros((n1, n2))for i in range(n1): for j in range(n2): image[i, j] = result[\"X&#123;&#125;&#123;&#125;\".format(i + 1, j + 1)]print(image)[[0. 1. 1. 1. 0.] [1. 0. 0. 0. 1.] [1. 0. 1. 0. 1.] [1. 0. 0. 0. 1.] [0. 1. 1. 1. 0.]]","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Network","slug":"Math/Network","permalink":"https://p829911.github.io/categories/Math/Network/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","permalink":"https://p829911.github.io/tags/Unsupervised-Learning/"},{"name":"Network","slug":"Network","permalink":"https://p829911.github.io/tags/Network/"}]},{"title":"주식 가격","slug":"주식-가격","date":"2019-01-16T11:49:35.000Z","updated":"2019-01-16T11:51:39.620Z","comments":true,"path":"2019/01/16/주식-가격/","link":"","permalink":"https://p829911.github.io/2019/01/16/주식-가격/","excerpt":"","text":"문제 설명초 단위로 기록된 주식가격이 담긴 배열 prices가 매개변수로 주어질 때, 가격이 유지된 기간은 몇 초인지를 return 하도록 solution 함수를 완성하세요. 제한사항 prices의 각 가격은 1 이상 10,000 이하인 자연수입니다. prices의 길이는 2 이상 100,000 이하입니다. 입출력 예 prices return [498,501,470,489] [2,1,1,0] 입출력 예 설명 1초 시점의 ₩498은 2초간 가격을 유지하고, 3초 시점에 ₩470으로 떨어졌습니다. 2초 시점의 ₩501은 1초간 가격을 유지하고, 3초 시점에 ₩470으로 떨어졌습니다. 3초 시점의 ₩470은 최종 시점까지 총 1초간 가격이 떨어지지 않았습니다. 4초 시점의 ₩489은 최종 시점까지 총 0초간 가격이 떨어지지 않았습니다. 나의 풀이12345678910def solution(prices): ls = [] for i, price in enumerate(prices): cnt = 0 for p in prices[i+1:]: cnt += 1 if price &gt; p: break ls.append(cnt) return ls 효율성 줄이기 실패…정답12345678910def solution(prices): answer = [0] * len(prices) for i in range(len(prices)-1): for j in range(i, len(prices)-1): if prices[i] &gt; prices[j]: break else: answer[i] +=1 return answer 출처 나의 풀이에는 for의 iterator에 list의 값을 넣었지만정답은 인덱스 만을 이용하여 price list를 건드리지 않고 값을 뽑아내어 풀었다.또한 list에 추가하는 것보다는 리스트의 값을 바꾸는 방식으로 효율성을 높인 것 같다.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"문자열 내 마음대로 정렬하기","slug":"문자열-내-마음대로-정렬하기","date":"2019-01-16T11:19:39.000Z","updated":"2019-01-16T11:20:35.896Z","comments":true,"path":"2019/01/16/문자열-내-마음대로-정렬하기/","link":"","permalink":"https://p829911.github.io/2019/01/16/문자열-내-마음대로-정렬하기/","excerpt":"","text":"문제 설명문자열로 구성된 리스트 strings와, 정수 n이 주어졌을 때, 각 문자열의 인덱스 n번째 글자를 기준으로 오름차순 정렬하려 합니다. 예를 들어 strings가 [“sun”, “bed”, “car”]이고 n이 1이면 각 단어의 인덱스 1의 문자 “u”, “e”, “a”로 strings를 정렬합니다. 제한 조건 strings는 길이 1 이상, 50이하인 배열입니다. strings의 원소는 소문자 알파벳으로 이루어져 있습니다. strings의 원소는 길이 1 이상, 100이하인 문자열입니다. 모든 strings의 원소의 길이는 n보다 큽니다. 인덱스 1의 문자가 같은 문자열이 여럿 일 경우, 사전순으로 앞선 문자열이 앞쪽에 위치합니다. 입출력 예 strings n return [“sun”,”bed”,”car”] 1 [“car”,”bed”,”sun”] [“abce”,”abcd”,”cdx”] 2 [“abcd”,”abce”,”cdx”] 입출력 예 설명입출력 예 1“sun”, “bed”, “car”의 1번째 인덱스 값은 각각 “u”, “e”, “a” 입니다. 이를 기준으로 strings를 정렬하면 [“car”, “bed”, “sun”] 입니다. 입출력 예 2“abce”와 “abcd”, “cdx”의 2번째 인덱스 값은 “c”, “c”, “x”입니다. 따라서 정렬 후에는 “cdx”가 가장 뒤에 위치합니다. “abce”와 “abcd”는 사전순으로 정렬하면 “abcd”가 우선하므로, 답은 [“abcd”, “abce”, “cdx”] 입니다. 나의 풀이 1123def solution(strings, n): answer = sorted(sorted(strings), key=lambda x: x[n]) return answer","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"체육복","slug":"체육복","date":"2019-01-15T14:15:29.000Z","updated":"2019-01-15T14:17:21.455Z","comments":true,"path":"2019/01/15/체육복/","link":"","permalink":"https://p829911.github.io/2019/01/15/체육복/","excerpt":"","text":"문제 설명오늘은 체육수업이 있는 날입니다. 그런데 점심시간에 도둑이 들어 몇몇 학생의 체육복이 도난을 당했습니다. 다행히 일부 학생들이 여벌의 체육복을 가져왔습니다. 학생들의 번호는 체격 순으로 매겨져 있기 때문에 바로 앞번호의 학생이나 바로 뒷번호의 학생에게만 체육복을 빌려주려고 합니다. 예를 들어, 4번 학생은 3번 학생이나 5번 학생에게만 체육복을 빌려줄 수 있습니다. 당연히 체육복을 2벌 가져온 학생의 체육복이 도난을 당했다면, 여벌의 체육복을 빌려줄 수 없습니다. 체육복이 없으면 체육수업을 들을 수 없기 때문에 체육복을 적절히 빌려 최대한 많은 학생이 체육수업을 듣고 싶습니다. 전체 학생의 수 n, 체육복을 도난당한 학생들의 번호가 담긴 배열 lost, 여벌의 체육복을 가져온 학생들의 번호가 담긴 배열 reserve가 매개변수로 주어질 때, 체육수업을 들을 수 있는 학생의 최댓값을 return 하도록 solution 함수를 작성해주세요. 제한사항 전체 학생의 수는 2명 이상 30명 이하입니다. 체육복을 도난당한 학생의 수는 2명 이상 n명 이하이고 중복되는 번호는 없습니다. 여벌의 체육복을 가져온 학생의 수는 1명 이상 n명 이하이고 중복되는 번호는 없습니다. 입출력 예 n lost reserve return 5 [2,4] [1,3,5] 5 5 [2,4] [3] 4 입출력 예 설명예제 #11번 학생이 2번 학생에게 체육복을 빌려주고, 3번 학생이나 5번 학생이 4번 학생에게 체육복을 빌려주면 학생 5명이 체육수업을 들을 수 있습니다. 예제 #23번 학생이 2번 학생이나 4번 학생에게 체육복을 빌려주면 학생 4명이 체육수업을 들을 수 있습니다. 1123456789101112def solution(n, lost, reserve): dup = set(lost) &amp; set(reserve) lost = list(set(lost) - dup) reserve = list(set(reserve) - dup) a = [i-1 for i in lost] b = [i+1 for i in lost] c = set(reserve) &amp; (set(a) | set(b)) num = len(lost) - len(c) if num &lt; 0: return n else: return n - num 21234567891011def solution(n, lost, reserve): dup = set(lost) &amp; set(reserve) lost = list(set(lost) - dup) reserve = list(set(reserve) - dup) for i in reserve: if i-1 in lost: lost.remove(i-1) continue elif i+1 in lost: lost.remove(i+1) return n - len(lost) comment문제를 정확히 읽고 제출하기 전에 충분히 검토해보고 제출하기.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"K번째수","slug":"K번째수","date":"2019-01-14T17:10:10.000Z","updated":"2019-01-16T12:11:08.546Z","comments":true,"path":"2019/01/15/K번째수/","link":"","permalink":"https://p829911.github.io/2019/01/15/K번째수/","excerpt":"","text":"문제 설명배열 array의 i번째 숫자부터 j번째 숫자까지 자르고 정렬했을 때, k번째에 있는 수를 구하려 합니다. 예를 들어 array가 [1, 5, 2, 6, 3, 7, 4], i = 2, j = 5, k = 3이라면 array의 2번째부터 5번째까지 자르면 [5, 2, 6, 3]입니다. 1에서 나온 배열을 정렬하면 [2, 3, 5, 6]입니다. 2에서 나온 배열의 3번째 숫자는 5입니다. 배열 array, [i, j, k]를 원소로 가진 2차원 배열 commands가 매개변수로 주어질 때, commands의 모든 원소에 대해 앞서 설명한 연산을 적용했을 때 나온 결과를 배열에 담아 return 하도록 solution 함수를 작성해주세요. 제한사항 array의 길이는 1 이상 100 이하입니다. array의 각 원소는 1 이상 100 이하입니다. commands의 길이는 1 이상 50 이하입니다. commands의 각 원소는 길이가 3입니다. 입출력 예 array commands return [1,5,2,6,3,7,4] [[2,5,3],[4,4,1],[1,7,3]] [5,6,3] 입출력 예 설명[1, 5, 2, 6, 3, 7, 4]를 2번째부터 5번째까지 자른 후 정렬합니다. [2, 3, 5, 6]의 세 번째 숫자는 5입니다 [1, 5, 2, 6, 3, 7, 4]를 4번째부터 4번째까지 자른 후 정렬합니다. [6]의 첫 번째 숫자는 6입니다. [1, 5, 2, 6, 3, 7, 4]를 1번째부터 7번째까지 자릅니다. [1, 2, 3, 4, 5, 6, 7]의 세 번째 숫자는 3입니다. 나의 풀이123456array = [1,5,2,6,3,7,4]commands = [ [2,5,3], [4,4,1], [1,7,3],] 12345def solution(array, commands): answer = [] for command in commands: answer.append(sorted(array[command[0]-1:command[1]])[command[2]-1]) return answer 다른 사람의 풀이12def solution(array, commands): return list(map(lambda x:sorted(array[x[0]-1:x[1]])[x[2]-1], commands)) lambda(람다) 함수lambda 함수는 익명함수로 메모리를 절약하는 이점이 있다.일반적인 함수는 객체를 만들고, 재사용을 위해 함수 이름(메모리)를 할당 한다. 123456# lambda 인수1, 인수2, ...: 인수를 이용한 표현식sum = lambda a, b: a + bresult = sum(3,4)print(result)# 7 익명함수이기 때문에 한번 쓰이고 다음줄로 넘어가면 힙(heap) 메모리 영역에서 증발하게 된다.자세하게 설명하자면 파이썬에서 모든 것이 객체로 관리되고 각 객체는 레퍼런스 카운터를 갖게 되는데 해당 카운터가 0 - 어떤 것도 참조를 하지 않으면 메모리를 환원한다 - 이러한 역할을 하는 것이 가비지 컬럭터이다. map 함수내장함수이며 입력받은 자료형의 각 요소가 함수에 의해 수행된 결과를 묶어서 map iterator 객체로 반환한다. map 함수는 게으른 연산을 진행해서 메모리를 크게 절약할 수 있다. 게으른 연산을 진행하기 때문에 값을 한번 사용했던 값은 다시 호출할 수가 없다. 1234567# map(function, iterable)# 예시li = [1,2,3]result = list(map(lambda i: i ** 2, li))print(result)# [1,4,9] 게으른 연산이란(lazy evaluation)?필요할 때만 가져다 사용한다. 보통 iterator 객체들이 게으른 함수이다. iterator 객체 next() 메소드로 데이터를 순차적으로 호출 가능 마지막 데이터 이후 next()를 호출하면 StopIteration 에러 발생 for 문을 사용할 때, 파이썬 내부에서는 임시로 list를 iterator로 변환해서 사용","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"pyenv, virtualenv, autoenv","slug":"pyenv-virtualenv-autoenv","date":"2019-01-14T17:01:17.000Z","updated":"2019-02-09T14:58:14.613Z","comments":true,"path":"2019/01/15/pyenv-virtualenv-autoenv/","link":"","permalink":"https://p829911.github.io/2019/01/15/pyenv-virtualenv-autoenv/","excerpt":"","text":"pyenv 파이썬 버전을 관리하는 툴 하나의 컴퓨터에 다양한 파이썬 버전을 설치하고 관리 pyenv-virtualenv virtualenv은 파이썬 환경을 격리하는 툴 pyenv-virtualenv은 virtualenv의 pyenv 확장 플러그인 파이썬 버전과 라이브러리의 완전한 격리 환경을 제공 autoenv autoenv는 디렉터리 이동 시 실행되는 스크립트 pyenv-virtualenv 사용 시 불편한 수작업을 자동화 특정 프로젝트 폴더로 들어가면 .env 파일을 실행하여 가상환경 활성화 맥, OS X 사전 준비사항12sudo xcode-select --installbrew install homebrew/dupes/zlib pyenv 설치1234brew install pyenv# pyenv 업그레이드brew upgrade pyenv 환경변수 적용 및 업데이트다음 명령을 터미널에서 수행하여 환경변수를 등록한다. zsh를 사용할 때는 &quot;~/.bash_profile&quot;을 &quot;~/.zshrc&quot;로 변경하여 실행한다. 1234echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bash_profileecho 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bash_profileecho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bash_profilesource ~/.bash_profile pyenv를 이용한 python 설치12345678910pyenv install 3.6.5pyenv versions* system (set by /Users/p829911/.pyenv/version) 3.6.5 3.6.5/envs/tensorflow 3.7.2 3.7.2/envs/speech speech tensorflow 여기서 설치가 되지 않고 오류가 나는 경우 다음 명령으로 실행시켜 준다. 123CFLAGS=\"-I$(brew --prefix openssl)/include -I$(xcrun --show-sdk-path)/usr/include\" \\LDFLAGS=\"-L$(brew --prefix openssl)/lib\" \\pyenv install -v 3.6.5 pyenv-virtualenv 사용하기1234567891011121314151617# pyenv-virtualenv 설치brew install pyenv-virtualenv# 환경변수 설정echo 'eval \"$(pyenv virtualenv-init -)\"' &gt;&gt; ~/.bashrc# virtualenv 생성pyenv virtualenv &lt;python versions&gt; &lt;virtualenv-name&gt;# virtualenv 활성화pyenv activate &lt;virtualenv-name&gt;# virtualenv 비활성화pyenv deactivate# virtualenv 삭제pyenv uninstall &lt;version&gt; / &lt;virtualenv-name&gt; autoenv123456# 설치brew install autoenv# 환경설정echo \"source $(brew --prefix autoenv)/activate.sh\" &gt;&gt; ~/.bashrcsource ~/.bashrc 가상 환경을 활성화 할 폴더에 .env 파일을 만들어 준다. 123vi &lt;virtualenv-folder&gt;/.envpyenv activate &lt;virtualenv-name&gt; 폴더를 나오면 다시 deactivate를 적용시켜 준다. 12345vi ~/.envpyenv deactivate# orpyenv shell system pip를 이용해 라이브러리 설치가상환경은 pip를 이용해 설치하는 패키지 또한 분리하여 관리해주기 때문에 원래 설치해 두었던 라이브러리들을 다시 깔아 줄 필요가 있다. 1pip3 freeze &gt; requirements.txt 위 명령어는 현재 pip3로 python3 환경에서 깔려있는 라이브러리이름과 버전을 requirement.txt 파일에 저장해 주는 명령어이다. 1234567891011121314jupyter==1.0.0jupyter-client==5.1.0jupyter-console==5.2.0jupyter-core==4.3.0MarkupSafe==1.0mistune==0.7.4nbconvert==5.3.1nbformat==4.4.0notebook==5.0.0pandocfilters==1.4.2pexpect==4.2.1pickleshare==0.7.4prompt-toolkit==1.0.15... requirements.txt 파일에 적힌 모든 라이브러리를 설치하는 pip 명령은 다음과 같다. 가상환경폴더로 이동 한 후 1pip install -r requirements.txt","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"}]},{"title":"모의고사","slug":"모의고사","date":"2019-01-14T07:20:05.000Z","updated":"2019-01-14T07:22:27.797Z","comments":true,"path":"2019/01/14/모의고사/","link":"","permalink":"https://p829911.github.io/2019/01/14/모의고사/","excerpt":"","text":"문제 설명수포자는 수학을 포기한 사람의 준말입니다. 수포자 삼인방은 모의고사에 수학 문제를 전부 찍으려 합니다. 수포자는 1번 문제부터 마지막 문제까지 다음과 같이 찍습니다. 1번 수포자가 찍는 방식: 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, …2번 수포자가 찍는 방식: 2, 1, 2, 3, 2, 4, 2, 5, 2, 1, 2, 3, 2, 4, 2, 5, …3번 수포자가 찍는 방식: 3, 3, 1, 1, 2, 2, 4, 4, 5, 5, 3, 3, 1, 1, 2, 2, 4, 4, 5, 5, … 1번 문제부터 마지막 문제까지의 정답이 순서대로 들은 배열 answers가 주어졌을 때, 가장 많은 문제를 맞힌 사람이 누구인지 배열에 담아 return 하도록 solution 함수를 작성해주세요. 제한 조건 시험은 최대 10,000 문제로 구성되어 있습니다. 문제의 정답은 1, 2, 3, 4, 5 중 하나입니다. 가장 높은 점수를 받은 사람이 여럿일 경우, return 하는 값을 오름차순 정렬해주세요 입출력 예 answers return [1,2,3,4,5] [1] [1,3,2,4,2] [1,2,3] 입출력 예 설명입출력 예 # 1 수포자 1은 모든 문제를 맞혔습니다. 수포자 2는 모든 문제를 틀렸습니다. 수포자 3은 모든 문제를 틀렸습니다. 따라서 가장 문제를 많이 맞힌 사람은 수포자 1입니다. 입출력 예 # 2 수포자 1은 [1,4]번 문제를 맞혔습니다. 수포자 2는 다섯 번째 문제를 맞혔습니다. 나의 풀이123456789101112131415161718192021222324252627282930def solution(answer): dic = &#123;1: 0, 2: 0, 3: 0&#125; math1 = [1,2,3,4,5] * 8 math2 = [2,1,2,3,2,4,2,5] * 5 math3 = [3,3,1,1,2,2,4,4,5,5] * 4 mul = 2 while len(math1) &lt; len(answer): math1 = math1 * mul math2 = math2 * mul math3 = math3 * mul mul += 1 for i in range(len(answer)): if math1[i] == answer[i]: dic[1] += 1 if math2[i] == answer[i]: dic[2] += 1 if math3[i] == answer[i]: dic[3] += 1 max_num = max(dic.values()) ls = [] for i in range(1, 4): if dic.get(i) &gt; (max_num-1): ls.append(i) return sorted(ls) 다른 사람의 풀이1234567891011121314151617181920def solution(answers): pattern1 = [1,2,3,4,5] pattern2 = [2,1,2,3,2,4,2,5] pattern3 = [3,3,1,1,2,2,4,4,5,5] score = [0,0,0] result = [] for idx, answer in enumerate(answers): if answer == pattern1[idx%len(pattern1)]: score[0] += 1 if answer == pattern2[idx%len(pattern2)]: score[1] += 1 if answer == pattern3[idx%len(pattern3)]: score[2] += 1 for idx, s in enumerate(score): if s == max(score): result.append(idx+1) return result 한계점 %를 이용한 몫 계산을 생각하지 못했다. 1 % 5 = 1, 5 % 5 = 0, 6 % 5 = 1 마지막 결과가 ascending 이면 되기 때문에 range와 for을 이용해 앞 index 부터 접근하는 경우 sorted 함수를 안써도 됐다.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"완주하지 못한 선수","slug":"완주하지-못한-선수","date":"2019-01-12T06:02:05.000Z","updated":"2019-01-14T07:19:53.297Z","comments":true,"path":"2019/01/12/완주하지-못한-선수/","link":"","permalink":"https://p829911.github.io/2019/01/12/완주하지-못한-선수/","excerpt":"","text":"문제 설명수많은 마라톤 선수들이 마라톤에 참여하였습니다. 단 한 명의 선수를 제외하고는 모든 선수가 마라톤을 완주하였습니다. 마라톤에 참여한 선수들의 이름이 담긴 배열 participant와 완주한 선수들의 이름이 담긴 배열 completion이 주어질 때, 완주하지 못한 선수의 이름을 return 하도록 solution 함수를 작성해주세요. 제한사항 마라톤 경기에 참여한 선수의 수는 1명 이상 100,000명 이하입니다. completion의 길이는 participant의 길이보다 1 작습니다. 참가자의 이름은 1개 이상 20개 이하의 알파벳 소문자로 이루어져 있습니다. 참가자 중에는 동명이인이 있을 수 있습니다. 입출력 예 participant completion return [“leo”, “kiki”, “eden”] [“eden”, “kiki”] “leo” [“marina”, “josipa”, “nikola”, “vinko”, “filipa”] [“josipa”, “filipa”, “marina”, “nikola”] “vinko” [“mislav”, “stanko”, “mislav”, “ana”] [“stanko”, “ana”, “mislav”] “mislav” 입출력 예 설명예제 #1 leo는 참여자 명단에는 있지만, 완주자 명단에는 없기 때문에 완주하지 못했습니다. 예제 #2 vinko는 참여자 명단에는 있지만, 완주자 명단에는 없기 때문에 완주하지 못했습니다. 예제 #3 mislav는 참여자 명단에는 두 명이 있지만, 완주자 명단에는 한 명밖에 없기 때문에 한명은 완주하지 못했습니다. 프로그래머스 첫번째 시도1234def solution(participant, completion): for c in completion: participant.remove(c) return participant[0] 정확성: 50 효율성: 0두번째 시도123456789101112def solution(participant, completion): dic = &#123;&#125; for p in participant: try: dic[p] += 1 except: dic[p] = 1 for c in completion: if c in dic: dic[c] -= 1 if dic[c] == 0: del dic[c] return list(dic.keys())[0] 통과다른 사람들의 풀이112345from collections import Counterdef solution(participant , completion): answer = Counter(participant) - Counter(completion) return list(answer.keys())[0] 21234567def solution(participant, completion): participant.sort() completion.sort() for p,c in zip(participant, completion): if p != c: return p return participant[-1] Collections module function description namedtuple() factory function for creating tuple subclasses with named fields deque list-like container with fast appends and pops on either end Counter dict subclass for counting hashable objects OrderedDict dict subclass that remembers the order entries were added defaultdict dict subclass that calls a factory function to supply missing values Counter1234567# 목록의 단어 집계cnt = Counter()for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']: cnt[word] += 1cnt# Counter(&#123;'red': 2, 'blue': 3, 'green': 1&#125;) 123456# Counter objects는 사전과 사용 인터페이스는 같지만 missing item에 대해서 # key error를 발생시키지 않고, 0을 반환한다.c = Counter(['eggs', 'ham'])c['bacon']# 0 12345c = Counter(a=4, b=2, c=0, d=-2)c.update(['a','b','c','d'])c# Counter(&#123;'a': 5, 'b': 3, 'c': 1, 'd': -1&#125;) 123Counter('abracadabra').most_common(3)# [('a', 5), ('b', 2), ('r', 2)]","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://p829911.github.io/tags/Algorithm/"}]},{"title":"crontab","slug":"crontab","date":"2019-01-11T02:53:56.000Z","updated":"2019-01-12T06:21:52.617Z","comments":true,"path":"2019/01/11/crontab/","link":"","permalink":"https://p829911.github.io/2019/01/11/crontab/","excerpt":"","text":"리눅스 크론탭 사용법 “특정 시간에 특정 작업을 해야한다.” 1. 크론탭 기본 (crontab basic)12345678910111213# 크론탭 실행crontab -e# 현재 크론탭에 어떤 내용이 들어있는지 보기crontab -l# 크론탭을 지우고 싶을 때crontab -r# 저장은 vi 처럼 콜론(:) 입력 후 wq로 갱신***** ls -al# 별이 다섯개 있는 경우에 \"매분마다 실행\" 2. 주기 결정12* * * * *분(0-59) 시간(0-23) 일(1-31) 월(1-12) 요일(0-7) 각 별 위치에 따라 주기를 다르게 설정 할 수 있다. 순서대로 분-시간-일-월-요일 순이다. 그리고 괄호 안의 숫자 범위 내로 별 대신 입력할 수 있다. 요일에서 0과 7은 일요일이다. 1부터 월요일이고, 6이 토요일이다. 3. 주기별 예제1234567891011121314151617# 매분 test.sh 실행* * * * * /home/script/test.sh# 매주 금요일 오전 5시 45분에 test.sh 를 실행45 5 * * 5 /home/script/test.sh# 매일 매시간 0분, 20분, 40분에 test.sh 를 실행0,20,40 * * * * /home/script/test.sh# 매일 1시 0분부터 30분까지 매분 test.sh 를 실행0-30 1 * * * /home/script/test.sh# 매 10분마다 test.sh 를 실행*/10 * * * * /home/script/test.sh# 5일에서 6일까지 2시, 3시, 4시에 매 10분마다 test.sh를 실행*/10 2,3,4 5-6 * * /home/script/test.sh 주기 입력 방법엔 *, - , / 을 이용하는 방법이 있다. 위에서 봤듯이 각각의 특수기호가 하는 기능이 다르고 조합을 어떻게 하느냐에 따라 주기를 설정 할 수 있다. 크론 사용 팁1234567# 한 줄에 하나의 명령만 쓴다. # 잘못된 예* * * 5 5/home/script/test.sh# 잘된 예* * * 5 5 /home/script.test.sh 크론 로깅 (cron logging)해당 처리 내역에 대해 로그를 남기고 싶을 때 1* * * * * /home/script/test.sh &gt; /home/script/test.sh.log 2&gt;&amp;1 위에서 2&gt;&amp;1이란make를 하면 일반 출력은 stdout, 에러 메시지는 stderr로 출력 된다.stderr을 stdout으로 돌려서 에러 메세지도 저장되게 하라는 의미이다.n&gt;&amp;m: 표준 출력과 표준 에러를 서로 바꾸기 0: 표준 입력 1: 표준 출력 2: 표준 에러 너무 자주 실행 되고 지속적으로 로깅이 되야 해서 로그를 계속 남겨둬야 한다면 1* * * * * /home/script/test.sh &gt;&gt; /home/script/test.sh.log 2&gt;&amp;1 위의 코드를 실행하면 계속 로그가 누적이 되는 것을 확인 할 수 있다. 그러나 로그가 과도하게 쌓이면 리눅스 퍼포먼스에 영향을 주므로 가끔씩 비워주거나 파일을 새로 만들어주는 작업이 필요하다. 크론탭 백업 (crontab backup)혹시라도 crontab -r을 쓰거나 실수로 crontab 디렉토리를 날려버려서 기존 크론 내역들이 날아갔을 때를 대비하여 주기적으로 크론탭을 백업해 주어야 한다. 1crontab -l &gt; /home/bak/crantab_bak.txt 크론탭 내용을 txt파일로 만들어 저장해 두는 것이다. 12# 자동화50 23 * * * crontab -l &gt; /home/bak/crontab_bak.txt 매일 오후 11시 50분에 크론탭을 백업해 두는 명령어이다.","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"},{"name":"Crontab","slug":"Crontab","permalink":"https://p829911.github.io/tags/Crontab/"}]},{"title":"그래프 이론 기초","slug":"그래프-이론-기초","date":"2019-01-11T02:51:47.000Z","updated":"2019-01-11T02:53:22.398Z","comments":true,"path":"2019/01/11/그래프-이론-기초/","link":"","permalink":"https://p829911.github.io/2019/01/11/그래프-이론-기초/","excerpt":"","text":"그래프(graph)는 다음 그림처럼 노드(node, vertex)와 그 사이를 잇는 간선(edge)으로 이루어진 구조를 말한다. 123456789import networkx as nxfrom IPython.core.display import Imagefrom networkx.drawing.nx_pydot import to_pydotg = nx.complete_graph(4)d = to_pydot(g)d.set_dpi(600)d.set_rankdir(\"LR\")Image(d.create_png(), width=600) 수학적으로 그래프 $G$ 는 노드(vertex) 집합 $V$ 와 간선(edge) 집합 $E$ 로 구성된다. G = (V, E)간선은 두 개의 노드로 이루어진 순서가 있는 쌍(ordered pair)이다. E \\subseteq V \\times V위에서 그린 그래프는 4개의 노드 집합 V = \\{0, 1, 2, 3 \\}과 6개의 간선 집합 E = \\{ (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3) \\}을 가진다. 방향성 그래프와 비방향성 그래프만약 간선 (a, b)와 (b, a)이 있을 때 이 두 간선을 다른 것으로 본다면 간선의 방향이 있는 방향성 그래프(directed graph)이고 두 간선을 같은 것으로 본다면 간선의 방향이 없는 비방향성 그래프(undirected graph)이다. 그래프를 시각화를 할 때 방향성은 화살표로 표시한다. NetworkX 패키지NetworkX는 그래프를 다루기 위한 파이썬 패키지이다. 그래프를 만드는 클래스 Graph, DiGraph 를 제공한다. Graph 클래스는 비방향성 그래프, DiGraph 클래스는 방향성 그래프를 나타낸다. NetworkX 패키지는 원래 버전 2 이상이 나와 있지만 나중에 사용할 pgmpy 패키지와의 호환성을 위해 버전 1.11을 사용하도록 한다. 1234import networkxnetworkx.__version__'1.11' 12import networkx as nxg1 = nx.DiGraph() 노드를 추가할 때는 add_node 메서드를 사용한다. 노드의 이름으로는 숫자나 문자열을 사용할 수 있다. 그래프에 포함된 노드는 nodes 메서드(버전 2에서는 속성)으로 확인할 수 있다. 123456g1.add_node(\"a\")g1.add_node(1)g1.add_node(2)g1.nodes()['a', 1, 2] 간선을 추가할 때는 add_edge 메서드를 사용한다. 간선을 이을 두 노드를 인수로 입력한다. 그래프에 포함된 노드는 edges 속성으로 확인할 수 있다. 12345g1.add_edge(1, \"a\")g1.add_edge(1, 2)g1.edges()[(1, 'a'), (1, 2)] 만약 graphviz 프로그램과 pydot 패키지가 설치되어 있다면 이를 이용하여 시각화 할 수도 있다. 12345678from IPython.core.display import Imagefrom networkx.drawing.nx_pydot import to_pydotd1 = to_pydot(g1)d1.set_dpi(300)d1.set_rankdir(\"LR\")d1.set_margin(1)Image(d1.create_png(), width=300) 노드 집합 $V$와 간선 집합 $V$ 를 가지는 그래프 $G$ 에 포함된 노드의 갯수를 그래프의 크기(cardinality)라고 하며 $|G|$ 또는 $|V|$ 로 나타내고 간선의 갯수는 $|E|$ 로 나타낸다. NetworkX 패키지에서는 각각 len 명령, number_of_nodes, number_of_edges 메서드로 계산할 수 있다. 123len(g1), g1.number_of_nodes(), g1.number_of_edges()(3, 3, 2) 만약 두 노드$a$,$b$ 를 포함하는 간선 $(a, b)$가 $E$ 안에 존재하면 두 노드는 인접하다(adjacent)고 하며 인접한 두 노드는 서로 이웃(neighbor)이라고 한다. (a, b) \\in ENetworkX 패키지 Graph 클래스의 neighbors 메서드는 인수로 받은 노드에 인접한 노드를 생성하므로 인접성을 확인하는데 사용할 수 있다. 12345for n in g1.neighbors(1): print(n) a2 1232 in g1.neighbors(1), 1 in g1.neighbors(2), \"a\" in g1.neighbors(2), \"a\" in g1.neighbors(1)(True, False, False, True) 만약 어떤 노드에서 출발하여 자기 자신으로 바로 돌아오는 간선이 있다면 셀프 루프(self loop)라고 한다. 다음 그래프에서는 노드 2에 셀프 루프가 있다. 12345678g2 = nx.Graph()g2.add_node(1)g2.add_node(2)g2.add_node(3)g2.add_edge(1, 2)g2.add_edge(2, 2)g2.add_edge(2, 3)np.random.seed(0) 셀프 루프가 있는 경우에는 graphviz로만 시각화 할 수 있다. 1234d2 = to_pydot(g2)d2.set_dpi(600)d2.set_rankdir(\"LR\")Image(d2.create_png(), width=600) 워크, 트레일, 패스어떤 노드를 출발해서 다른 노드로 도달하기 위한 인접한 노드의 순서열 워크(walk)라고 하고 워크 중에서 동일한 노드를 두 번 이상 지나지 않는 워크를 트레일(trail), 시작과 끝을 제외하고 다른 노드에 대해서만 동일한 노드를 두번이상 지나지 않는 패스(path)라고 한다. 패스 중에서 시작점과 끝점이 동일한 패스를 사이클(cycle)이라고 한다. 사이클이 없는 그래프를 어사이클릭 그래프(acyclic graph)라고 한다. 다음 그래프 g3에서 워크, 트레일, 패스, 사이클을 찾아보자 $a - c - d - c - e$는 $a$에서 $c$로 가는 워크이다. 하지만 트레일이나 패스는 아니다. $a - b - c - d- e$ 는 트레일이다. $a - b - c - d - e - c$ 는 패스지만 트레일은 아니다. $a - b - c - a$ 는 사이클이다. 123456789101112131415161718g3 = nx.DiGraph()g3.add_node(\"a\")g3.add_node(\"b\")g3.add_node(\"c\")g3.add_node(\"d\")g3.add_node(\"e\")g3.add_node(\"f\")g3.add_edge(\"a\", \"b\")g3.add_edge(\"a\", \"c\")g3.add_edge(\"b\", \"c\")g3.add_edge(\"c\", \"d\")g3.add_edge(\"d\", \"e\")g3.add_edge(\"c\", \"e\")d3 = to_pydot(g3)d3.set_dpi(600)d3.set_rankdir(\"LR\")Image(d3.create_png(), width=800) has_path 명령으로 두 노드간에 패스가 존재하는지 알 수 있다. 패스가 존재하면 shortest_path 명령으로 가장 짧은 패스를 구할 수 있다. 123nx.has_path(g3, \"a\", \"b\"), nx.has_path(g3, \"a\", \"e\"), nx.has_path(g3, \"a\", \"f\")(True, True, False) 123nx.shortest_path(g3, \"a\", \"e\")['a', 'c', 'e'] 클리크무방향성 그래프의 노드 집합 중에서 모든 노드끼리 간선이 존재하면 그 노드 집합을 클리크(clique)라고 한다. 만약 클리크에 포함된 노드에 인접한 다른 노드를 추가하면 클리크가 아니게 되는 것을 최대클리크(maximal clique)라고 한다. 다음 그래프 $g4$ 에서 클리크를 찾아보자 $\\{a, b\\}$ 는 클리크이다. 하지만 최대 클리크는 아니다. $\\{a, b, c\\}$ 는 클리크이며 최대 클리크이다. 1234567891011121314151617181920g4 = nx.Graph()g4.add_node(\"a\")g4.add_node(\"b\")g4.add_node(\"c\")g4.add_node(\"d\")g4.add_node(\"e\")g4.add_node(\"f\")g4.add_edge(\"a\", \"b\")g4.add_edge(\"a\", \"c\")g4.add_edge(\"b\", \"c\")g4.add_edge(\"b\", \"d\")g4.add_edge(\"c\", \"d\")g4.add_edge(\"d\", \"e\")g4.add_edge(\"d\", \"f\")g4.add_edge(\"e\", \"f\")d4 = to_pydot(g4)d4.set_dpi(600)d4.set_rankdir(\"LR\")Image(d4.create_png(), width=800) cliques_containing_node 명령은 특정 노드를 포함하는 클리크를 찾는다. 123nx.cliques_containing_node(g4, [\"a\"])&#123;'a': [['a', 'b', 'c']]&#125; 123nx.cliques_containing_node(g4, [\"a\", \"b\"])&#123;'a': [['a', 'b', 'c']], 'b': [['d', 'b', 'c'], ['a', 'b', 'c']]&#125; enumerate_all_cliques 명령은 모든 클리크를, find_cliques 는 모든 최대 클리크를 찾는다. 12345678910111213141516171819[c for c in nx.enumerate_all_cliques(g4)][['a'], ['b'], ['c'], ['d'], ['e'], ['f'], ['a', 'b'], ['a', 'c'], ['b', 'c'], ['b', 'd'], ['c', 'd'], ['d', 'e'], ['d', 'f'], ['e', 'f'], ['a', 'b', 'c'], ['b', 'c', 'd'], ['d', 'e', 'f']] 123[c for c in nx.find_cliques(g4)][['d', 'b', 'c'], ['d', 'e', 'f'], ['a', 'b', 'c']]","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Network","slug":"Math/Network","permalink":"https://p829911.github.io/categories/Math/Network/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","permalink":"https://p829911.github.io/tags/Unsupervised-Learning/"},{"name":"Network","slug":"Network","permalink":"https://p829911.github.io/tags/Network/"}]},{"title":"Affinity Propagation","slug":"Affinity-Propagation","date":"2019-01-11T01:17:57.000Z","updated":"2019-01-11T01:20:49.385Z","comments":true,"path":"2019/01/11/Affinity-Propagation/","link":"","permalink":"https://p829911.github.io/2019/01/11/Affinity-Propagation/","excerpt":"","text":"모든 데이터가 특정한 기준에 따라 자신을 대표할 대표 데이터를 선택한다. 만약 스스로가 자기 자신을 대표하게 되면 클러스터의 중심이 된다. responsibility $r(i,k)$ $k$ 번째 데이터가 $i$ 번째 데이터의 대표가 되어야 한다는 증거 availability $a(i, k)$ $i$ 번째 데이터가 $k$ 번째 데이터를 대표로 선택해야 한다는 증거 다음 수식을 수렴할 때까지 반복 r(i, k) \\leftarrow s(i,k) - \\text{max}_{k' \\neq k}(a(i,k') + s(i, k'))\\\\ a(i,k) \\leftarrow \\text{min}(0, r(k,k) + \\sum_{i' \\neq i,k}r(i',k)) \\\\여기에서 $s(i,k)$ 는 다음과 같이 음의 거리로 정의되는 유사도이다. s(i, k) = -||x_i - x_k||^2특히 $s(k,k)$ 는 특정한 음수 값으로 사용자가 정해 주게 되는데 이 값에 따라서 클러스터의 갯수가 달라지는 하이퍼 모수가 된다. $s(k,k)$ 가 크면 자기 자신에 대한 유사도가 커져서 클러스터의 수가 증가한다. 위 알고리즘으로 계산하는 $r, a$ 가 더 이상 변화하지 않고 수렴하면 계산이 종료되고 종료 시점에서 $r(k, k) + a(k, k) &gt; 0$ 이 데이터가 클러스터의 중심이 된다. 12345678910111213141516171819202122from sklearn.datasets.samples_generator import make_blobsfrom sklearn.cluster import AffinityPropagationfrom sklearn.metrics import *centers = [[1, 1], [-1, -1], [1, -1]]X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)model = AffinityPropagation(preference=-50).fit(X)cluster_centers_indices = model.cluster_centers_indices_labels = model.labels_n_clusters_ = len(cluster_centers_indices)print('Estimated number of clusters: %d' % n_clusters_)print(\"Adjusted Rand Index: %0.3f\" % adjusted_rand_score(labels_true, labels))print(\"Adjusted Mutual Information: %0.3f\" % adjusted_mutual_info_score(labels_true, labels))print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X, labels, metric='sqeuclidean'))Estimated number of clusters: 3Adjusted Rand Index: 0.912Adjusted Mutual Information: 0.871Silhouette Coefficient: 0.753 123456789101112from itertools import cyclecolors = cycle('rgb')for k, col in zip(range(n_clusters_), colors): class_members = labels == k cluster_center = X[cluster_centers_indices[k]] plt.plot(X[class_members, 0], X[class_members, 1], col + '.') for x in X[class_members]: plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col, alpha=0.25) plt.plot(cluster_center[0], cluster_center[1], 'o', mec='k', mew=3, markersize=7)plt.show()","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"},{"name":"Affinity Propagation","slug":"Affinity-Propagation","permalink":"https://p829911.github.io/tags/Affinity-Propagation/"}]},{"title":"계층적 클러스터링","slug":"계층적-클러스터링","date":"2019-01-11T01:17:40.000Z","updated":"2019-01-11T01:21:50.993Z","comments":true,"path":"2019/01/11/계층적-클러스터링/","link":"","permalink":"https://p829911.github.io/2019/01/11/계층적-클러스터링/","excerpt":"","text":"계층적 클러스터링은 하나의 데이터 샘플을 하나의 클러스터로 보고 가장 유사도가 높은 클러스터를 합치면서 클러스터 갯수를 줄여 가는 방법을 말한다. 클러스터간의 거리 측정클러스터간의 비유사도(dissimilarity) 혹은 거리(distance)를 측정하는 방법에는 다음과 같은 것이 있다. 비귀납적 방법centroid 두 클러스터의 중심점(centroid)를 정의한 다음 두 중심점의 거리를 클러스터간의 거리로 정의한다. d(u, v) = || c_u - c_v ||^2여기에서 $c_u$ 와 $c_v$ 각각 두 클러스터 $u$ 와 $v$ 의 중심점이다. single 클러스터 $u$ 의 모든 데이터 $i$ 와 클러스터 $v$ 의 모든 데이터 $j$ 의 모든 조합에 대해 거리를 측정해서 최소값을 구한다. 최소 거리(Nearest Point) 방법이라고도 한다. d(u,v) = \\text{min}(\\text{dist}(u[i], v[j]))complete 클러스터 $u$ 의 모든 데이터 $i$ 와 클러스터 $v$ 의 모든 데이터 $j$ 의 모든 조합에 대해 거리를 측정한 후 가장 큰 값을 구한다. Farthest Point Algorithm 또는 Voor Hees Algorithm 이라고 한다. d(u,v) = \\text{max}(\\text{dist}(u[i], v[j]))average 클러스터 $u$ 의 모든 데이터 $i$ 와 클러스터 $v$ 의 모든 데이터 $j$ 의 모든 조합에 대해 거리를 측정한 후 평균을 구한다. $|u|$ 와 $|v|$ 는 각각 두 클러스터의 원소의 갯수를 뜻한다. d(u,v) = \\sum_{ij} \\dfrac{d(u[i], v[j])}{|u||v|}귀납적 방법median 이 방법은 Agglomerative Clustering에서 사용할 수 있는 귀납적 방법으로 centroid 방법의 변형이다. 만약 클러스터 $u$가 클러스터 $s$ 와 클러스터 $t$ 가 결합하여 생겼다면 클러스터 $u$ 의 중심점은 새로 계산하지 않고 원래 클러스터의 두 클러스터의 중심점의 평균을 사용한다. weighted 이 방법은 Agglomeratice Clustering 에서 사용할 수 있는 귀납적 방법이다. 만약 클러스터 $u$ 가 클러스터 $s$ 와 클러스터 $t$ 가 결합하여 생겼다면 다음과 같이 원래 클러스터까지의 두 거리의 평균을 사용한다. d(u,v) = (dist(s,v) + dist(t,v))/2Ward 이 방법은 Agglomerative Clustering 에서 사용할 수 있는 귀납적 방법이다. 만약 클러스터 $u$ 가 클러스터 $s$ 와 클러스터 $t$ 가 결합하여 생겼다면 다음과 같이 두 클러스터 거리의 가중 평균에서 원래의 두 클러스터 사이의 거리를 보정한 값을 사용한다. d(u,v) = \\sqrt{\\frac{|v|+|s|}{|v|+|s|+|t|}d(v,s)^2 + \\frac{|v|+|t|}{|v|+|s|+|t|}d(v,t)^2 - \\frac{|v|}{|v|+|s|+|t|}d(s,t)^2}이 식에서 $|\\cdot|$ 기호는 클러스터의 원소의 갯수를 말한다. SciPy의 계층적 클러스터링파이썬으로 계층적 클러스터링을 하려면 SciPy 패키지의 linkage 명령을 사용하거나 scikit-learn 패키지의 AgglomerativeClustering 클래스를 사용한다. 각각 장단점이 있는데 SciPy 패키지는 tree 형태로 시각화해주는 dendrogram 명령도 지원한다. MNIST digit 이미지 중 20개의 이미지를 무작위로 골라 계층적 클러스터링을 적용해보자. 1234567891011121314151617from sklearn.datasets import load_digitsdigits = load_digits()n_image = 20np.random.seed(0)idx = np.random.choice(range(len(digits.images)), n_image)X = digits.data[idx]images = digits.images[idx]plt.figure(figsize=(12, 1))for i in range(n_image): plt.subplot(1, n_image, i + 1) plt.imshow(images[i], cmap=plt.cm.bone) plt.grid(False) plt.xticks(()) plt.yticks(()) plt.title(i) 1234from scipy.cluster.hierarchy import linkage, dendrogramZ = linkage(X, 'ward')Z 1234567891011121314151617181920212223from matplotlib.offsetbox import OffsetImage, AnnotationBboxplt.figure(figsize=(10, 4))ax = plt.subplot()ddata = dendrogram(Z)dcoord = np.array(ddata[\"dcoord\"])icoord = np.array(ddata[\"icoord\"])leaves = np.array(ddata[\"leaves\"])idx = np.argsort(dcoord[:, 2])dcoord = dcoord[idx, :]icoord = icoord[idx, :]idx = np.argsort(Z[:, :2].ravel())label_pos = icoord[:, 1:3].ravel()[idx][:20]for i in range(20): imagebox = OffsetImage(images[i], cmap=plt.cm.bone_r, interpolation=\"bilinear\", zoom=3) ab = AnnotationBbox(imagebox, (label_pos[i], 0), box_alignment=(0.5, -0.1), bboxprops=&#123;\"edgecolor\" : \"none\"&#125;) ax.add_artist(ab)plt.show()","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"},{"name":"계층적 클러스터링","slug":"계층적-클러스터링","permalink":"https://p829911.github.io/tags/계층적-클러스터링/"}]},{"title":"DBSCAN","slug":"DBSCAN","date":"2019-01-11T01:17:05.000Z","updated":"2019-09-18T11:39:51.663Z","comments":true,"path":"2019/01/11/DBSCAN/","link":"","permalink":"https://p829911.github.io/2019/01/11/DBSCAN/","excerpt":"","text":"K-Means 클러스터링 방법은 단순하고 강력한 방법이지만 클러스터의 모양이 원형이 아닌 경우에는 잘 동작하지 않으며 클러스터의 갯수를 사용자가 지정해주어야 한다는 단점이 있다. DBSCAN(Density-Based Spatial Clustering of Applications with Noise) 방법은 데이터가 밀집한 정도 즉 밀도를 이용하여 클러스터의 형태에 구애받지 않으며 클러스터의 갯수를 사용자가 지정할 필요가 없다. DBSCAN 방법에서는 초기 데이터로부터 근접한 데이터를 찾아나가는 방법으로 클러스터를 확장한다. 이 때 다음 사용자 인수를 사용한다. epsilon $\\epsilon$: 이웃(neighborhood)을 정의하기 위한 거리 최소 데이터 갯수(minimum points): 밀집 지역을 정의하기 위해 필요한 이웃의 갯수 만약 $\\epsilon$ 거리 안의 이웃 영역안에 최소 데이터 갯수 이상의 데이터가 있으면 그 데이터는 핵심(core) 데이터이다. 이렇게 핵심 데이터를 찾아낸 다음에는 이 핵심 데이터의 이웃영역 안에 있는 데이터를 이 핵심데이터와 연결된(reached) 고밀도 데이터로 정의한다. 고밀도 데이터의 이웃영역 안에 있는 데이터도 마찬가지로 연결된 고밀도 데이터가 된다. 만약 고밀도 데이터에 더이상 이웃이 없으면 이 데이터는 경계(border) 데이터라고 하며 연결은 끝난다. 핵심 데이터도 아니고 경계 데이터도 아닌 데이터를 outlier라고 한다. scikit-learn의 cluster 서브패키지는 DBSCAN 클러스터링을 위한 DBSCAN 클래스를 제공한다. 다음과 같은 인수를 받을 수 있다. eps: 이웃을 정의하기 위한 거리. epsilon. min_samples: 핵심 데이터를 정의하기 위해 필요한 이웃영역안의 데이터 갯수. 클러스터링이 끝나면 다음 속성을 가진다. labels_: 클러스터 번호 core_sample_indices_: 핵심 데이터의 인덱스 다음은 make_circles 명령과 make_moons 명령으로 만든 동심원, 초승달 데이터를 DBSCAN 방법으로 클러스터링한 결과를 나타낸 것이다. 마커(marker)의 모양은 클러스터를 나타내고 마커의 크기가 큰 데이터는 핵심데이터, x 표시된 데이터는 outlier이다. 1234567891011121314151617181920212223242526272829303132333435from sklearn.datasets import make_circles, make_moonsfrom sklearn.cluster import DBSCANn_samples = 1000np.random.seed(2)X1, y1 = make_circles(n_samples=n_samples, factor=.5, noise=.09)X2, y2 = make_moons(n_samples=n_samples, noise=.1)def plot_DBSCAN(title, X, eps, xlim, ylim): model = DBSCAN(eps=eps) y_pred = model.fit_predict(X) idx_outlier = np.logical_not((model.labels_ == 0) | (model.labels_ == 1)) plt.scatter(X[idx_outlier, 0], X[idx_outlier, 1], marker='x', lw=1, s=20) plt.scatter(X[model.labels_ == 0, 0], X[model.labels_ == 0, 1], marker='o', facecolor='g', s=5) plt.scatter(X[model.labels_ == 1, 0], X[model.labels_ == 1, 1], marker='s', facecolor='y', s=5) X_core = X[model.core_sample_indices_, :] idx_core_0 = np.array(list(set(np.where(model.labels_ == 0)[0]).intersection(model.core_sample_indices_))) idx_core_1 = np.array(list(set(np.where(model.labels_ == 1)[0]).intersection(model.core_sample_indices_))) plt.scatter(X[idx_core_0, 0], X[idx_core_0, 1], marker='o', facecolor='g', s=80, alpha=0.3) plt.scatter(X[idx_core_1, 0], X[idx_core_1, 1], marker='s', facecolor='y', s=80, alpha=0.3) plt.grid(False) plt.xlim(*xlim) plt.ylim(*ylim) plt.xticks(()) plt.yticks(()) plt.title(title) return y_predplt.figure(figsize=(10, 5))plt.subplot(121)y_pred1 = plot_DBSCAN(\"동심원 클러스터\", X1, 0.1, (-1.2, 1.2), (-1.2, 1.2))plt.subplot(122)y_pred2 = plot_DBSCAN(\"초승달 클러스터\", X2, 0.1, (-1.5, 2.5), (-0.8, 1.2))plt.tight_layout()plt.show() 이 클러스터링 결과의 adjusted Rand index와 adjusted mutual info 값은 다음과 같다. 1234567891011from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_scoreprint(\"Circle ARI:\", adjusted_rand_score(y1, y_pred1))print(\"Circle AMI:\", adjusted_mutual_info_score(y1, y_pred1))print(\"Moon ARI:\", adjusted_rand_score(y2, y_pred2))print(\"Moon AMI:\", adjusted_mutual_info_score(y2, y_pred2))Circle ARI: 0.9414262371038592Circle AMI: 0.8361564005781013Moon ARI: 0.9544844153926417Moon AMI: 0.8606657095694518","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"},{"name":"MATH","slug":"MATH","permalink":"https://p829911.github.io/tags/MATH/"},{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://p829911.github.io/tags/DBSCAN/"}]},{"title":"terraform","slug":"terraform","date":"2019-01-11T01:09:47.000Z","updated":"2019-01-11T01:12:57.873Z","comments":true,"path":"2019/01/11/terraform/","link":"","permalink":"https://p829911.github.io/2019/01/11/terraform/","excerpt":"","text":"테라폼 기초 튜토리얼 terraform 테라폼은 하시코프에서 오픈소스로 개발중인 인프라스트럭처 관리 도구이다. 서비스 실행에 필요한 환경을 구축하는 도구라는 점에서 셰프나 앤서블 같은 설정 관리 도구와 더불어 프로비저닝 도구로 분류된다. 테라폼은 코드로서의 인프라스트럭처를 지향하고 있는 도구로서, GUI나 웹 콘솔을 사용해 서비스 실행에 필요한 리소스를 관리하는 대신 필요한 리소스들을 선언적인 코드로 작성해 관리할 수 있도록 해준다. 테라폼을 이용한 웹 애플리케이션 인프라스트럭처 프로비저닝 간단한 웹 어플리케이션을 아마존 웹 서비스에 배포하는 상황을 가정하겠다. 이 애플리케이션은 EC2가상 머신과 RDS 데이터베이스를 사용한다. 테라폼으로 이 인프라스트럭처를 구축하려면 다음과 같은 단계를 거친다. 1단계 - 먼저 아마존 웹 서비스 계정을 준비하고, API 키를 설정한다. 2단계 스텝1 - 인프라스트럭처를 묘사하기 위해 사용하는 HCL언어로 필요한 리소스를 선언한다. 2단계 스텝2 - 선언된 리소스들이 생성가능한지 계획(Plan)을 확인한다. 2단계 스텝3 - 선언된 리소스들을 아마존 웹 서비스에 적용(Apply)한다. 3단계 - 웹 애플리케이션을 배포한다. 테라폼의 기본 개념들프로비저닝 (Provisioning) 어떤 프로세스나 서비스를 실행하기 위한 준비 단계를 프로비저닝이라고 이야기한다. 프로비저닝에는 크게 네트워크나 컴퓨팅 자원을 준비하는 작업과 준비된 컴퓨팅 자원에 사이트 패키지나 애플리케이션에 의존성을 준비하는 단계로 나뉘어진다. 명확한 경계는 불분명하지만 테라폼은 전자에 치우쳐있는 도구라고 할 수 있다. 프로바이더 (Provider) 테라폼과 외부 서비스를 연결해주는 기능을 하는 모듈이다. 예를 들면 테라폼으로 AWS 서비스의 컴퓨팅 자원을 생성하기 위해서는 aws 프로바이더를 먼저 셋업해야한다. 프로바이더로는 AWS, 구글 클라우드 플랫폼, 마이크로소프트 애저와 같은 범용 클라우드 서비스를 비롯해 깃허브, 데이터도그, DNSimple과 같은 특정 기능을 제공하는 서비스, MySQL, 레빗MQ, 도커와 같은 로컬 서비스를 지원한다. 리소스 (자원 Resource) 리소스란 특정 프로바이더가 제공해주는 조작 가능한 대상의 최소 단위이다. 예를 들어 AWS 프로바이더는 aws_instance 리소스 타입을 제공하고, 이 리소스 타입을 사용해 Amazon EC2의 가상 머신 리소스를 선언하고 조작하는 것이 가능하다. EC2 인스턴스, 시큐리티 그룹, 키 페어 모두 aws 프로바이더가 제공해주는 리소스 타입이다. HCL (Hashicorp Configuration Language) HCL은 테라폼에서 사용하는 설정 언어이다. 테라폼에서 모든 설정과 리소스 선언은 HCL을 사용해 이루어진다. 테라폼에서 HCL파일의 확장자는 .tf를 사용한다. 계획 (Plan) 테라폼 프로젝트 디렉터리 아래의 모든 .tf 파일의 내용을 실제로 적용 가능한지 확인하는 작업을 계획이라고 한다. 테라폼은 이를 terraform plan 명령어로 제공하며, 이 명령어를 실행하면 어떤 리소스가 생성되고, 수정되고, 파괴될지 계획을 보여준다. 적용 (Apply) 테라폼 프로젝트 디렉터리 아래의 모든 .tf 파일의 내용대로 리소스를 생성, 수정, 파괴하는 일을 적용이라고 한다. 테라폼은 이를 terraform apply 명령어로 제공한다. 이 명령어를 실행하기 전에 변경 예정 사항은 plan 명령어를 사용해 확인할 수 있다.","categories":[{"name":"Server","slug":"Server","permalink":"https://p829911.github.io/categories/Server/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Teffaform","slug":"Teffaform","permalink":"https://p829911.github.io/tags/Teffaform/"},{"name":"AWS","slug":"AWS","permalink":"https://p829911.github.io/tags/AWS/"},{"name":"Server","slug":"Server","permalink":"https://p829911.github.io/tags/Server/"}]},{"title":"K-means","slug":"K-means","date":"2019-01-10T11:31:21.000Z","updated":"2019-01-10T11:34:35.975Z","comments":true,"path":"2019/01/10/K-means/","link":"","permalink":"https://p829911.github.io/2019/01/10/K-means/","excerpt":"","text":"K-MeansK-Means 클러스터링 알고리즘은 가장 단순하고 빠른 클러스터링 알고리즘의 하나이다. 다음과 같은 목적함수 값이 최소화될 때까지 클러스터의 중심(centroid)의 위치와 각 데이터가 소속될 클러스터를 반복해서 찾는다. 이 값을 inertia라고도 한다. J = \\sum_{k=1}^K\\sum_{i \\in C_k} d(x_i, u_k)이 식에서 $K$ 는 클러스터의 갯수이고 $C_k$ 는 $k$ 번째 클러스터에 속하는 데이터의 집합, $u_k$ 는 $k$ 번째 클러스터의 중심위치, $d$ 는 $x_i, u_k$ 두 데이터 사이의 거리(distance) 혹은 비유사도(dissimilarity)로 정의한다. 만약 유클리드 거리를 사용한다면 다음과 같다. d(x_i, u_k) = || x_i - u_k || ^ 2세부 알고리즘은 다음과 같다. 임의의 중심값 $u_k$ 를 고른다. 보통 데이터 샘플 중에서 $K$개를 선택한다. 중심에서 각 데이터까지의 거리를 계산 각 데이터에서 가장 가까운 중심을 선택하여 클러스터 갱신 다시 만들어진 클러스터에 대해 중심을 다시 계산하고 1~4를 반복한다. scikit-learn의 cluster 서브패키지는 Means 클러스터링을 위한 KMeans 클래스를 제공한다. 다음과 같은 인수를 받을 수 있다. n_clusters: 클러스터의 갯수 init: 초기화 방법. &quot;random&quot; 이면 무작위, &quot;K-means++&quot; 이면 K-means++ 방법. 또는 각 데이터의 클러스터 라벨 n_init: 초기 중심값 시도 횟수. 디폴트는 10이고 10개의 무작위 중심값 목록 중 가장 좋은 값을 선택한다. max_iter: 최대 반복 횟수. random_state: 시드값. 다음은 make_blobs 명령으로 만든 데이터를 2개로 K-means 클러스터링하는 과정을 나타낸 것이다. 마커(marker)의 모양은 클러스터를 나타내고 크기가 큰 마커가 중심값 위치이다. 각 단계에서 중심값은 전단계의 클러스터의 평균으로 다시 계산된다. 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeansX, _ = make_blobs(n_samples=20, random_state=4)def plot_KMeans(n): model = KMeans(n_clusters=2, init=\"random\", n_init=1, max_iter=n, random_state=8).fit(X) c0, c1 = model.cluster_centers_ plt.scatter(X[model.labels_ == 0, 0], X[model.labels_ == 0, 1], marker='v', facecolor='r', edgecolors='k') plt.scatter(X[model.labels_ == 1, 0], X[model.labels_ == 1, 1], marker='^', facecolor='y', edgecolors='k') plt.scatter(c0[0], c0[1], marker='v', c=\"r\", s=200) plt.scatter(c1[0], c1[1], marker='^', c=\"y\", s=200) plt.grid(False) plt.title(\"iteration=&#123;&#125;, score=&#123;:5.2f&#125;\".format(n, model.score(X)))plt.figure(figsize=(8, 8))plt.subplot(321)plot_KMeans(1)plt.subplot(322)plot_KMeans(2)plt.subplot(323)plot_KMeans(3)plt.subplot(324)plot_KMeans(4)plt.tight_layout()plt.show() K-Means++K-Means++ 알고리즘은 초기 중심값을 설정하기 위한 알고리즘이다. 다음과 같은 방법을 통해 되도록 멀리 떨어진 중심값 집합을 나타낸다. 중심값을 저장할 집합 $M$ 준비 일단 하나의 중심 $\\mu_0$ 을 랜덤하게 선택하여 $M$ 에 넣는다. $M$ 에 속하지 않는 모든 샘플 $x_i$ 에 대해 거리 $d(M,x_i)$ 를 계산. $d(M, x_i)$ 는 $M$ 안의 모든 샘플 $\\mu_k$ 에 대해 $d(u_k, x_i)$ 를 계산하여 가장 작은 값 선택 $d(M, x_i)$ 에 비례한 확률로 다음 중심 $\\mu$ 를 선택. $K$ 개의 중심을 선택할 때까지 반복 K-Means 알고리즘 사용 다음은 KMean 방법을 사용하여 MNIST Digit 이미지 데이터를 클러스터링한 결과이다. 각 클러스터에서 10개씩의 데이터만 표시하였다. 1234567891011121314151617181920212223242526272829from sklearn.datasets import load_digitsdigits = load_digits()model = KMeans(init=\"k-means++\", n_clusters=10, random_state=0)model.fit(digits.data)y_pred = model.labels_def show_digits(images, labels): f = plt.figure(figsize=(8, 2)) i = 0 while (i &lt; 10 and i &lt; images.shape[0]): ax = f.add_subplot(1, 10, i + 1) ax.imshow(images[i], cmap=plt.cm.bone) ax.grid(False) ax.set_title(labels[i]) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks([]) plt.tight_layout() i += 1 def show_cluster(images, y_pred, cluster_number): images = images[y_pred == cluster_number] y_pred = y_pred[y_pred == cluster_number] show_digits(images, y_pred) for i in range(10): show_cluster(digits.images, y_pred, i) 이미지의 제목에 있는 숫자는 클러스터 번호에 지나지 않으므로 원래 숫자의 번호와 일치하지 않는다. 하지만 이를 예측 문제라고 가정하고 분류 결과 행렬을 만들면 다음과 같다. 1234567891011121314from sklearn.metrics import confusion_matrixconfusion_matrix(digits.target, y_pred)array([[ 1, 0, 0, 0, 0, 177, 0, 0, 0, 0], [ 0, 1, 1, 0, 0, 0, 55, 99, 24, 2], [ 0, 13, 0, 2, 3, 1, 2, 8, 148, 0], [ 0, 154, 2, 13, 7, 0, 0, 7, 0, 0], [163, 0, 0, 0, 7, 0, 7, 4, 0, 0], [ 2, 0, 136, 43, 0, 0, 0, 0, 0, 1], [ 0, 0, 0, 0, 0, 1, 1, 2, 0, 177], [ 0, 0, 0, 0, 177, 0, 0, 2, 0, 0], [ 0, 2, 4, 53, 5, 0, 5, 100, 3, 2], [ 0, 6, 6, 139, 7, 0, 20, 2, 0, 0]]) 이 클러스터링 결과의 adjusted Rand index와 adjusted mutual info값은 다음과 같다. 123456789101112131415from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_scoreprint(adjusted_rand_score(digits.target, y_pred))print(adjusted_mutual_info_score(digits.target, y_pred))array([[ 1, 0, 0, 0, 0, 177, 0, 0, 0, 0], [ 0, 1, 1, 0, 0, 0, 55, 99, 24, 2], [ 0, 13, 0, 2, 3, 1, 2, 8, 148, 0], [ 0, 154, 2, 13, 7, 0, 0, 7, 0, 0], [163, 0, 0, 0, 7, 0, 7, 4, 0, 0], [ 2, 0, 136, 43, 0, 0, 0, 0, 0, 1], [ 0, 0, 0, 0, 0, 1, 1, 2, 0, 177], [ 0, 0, 0, 0, 177, 0, 0, 2, 0, 0], [ 0, 2, 4, 53, 5, 0, 5, 100, 3, 2], [ 0, 6, 6, 139, 7, 0, 20, 2, 0, 0]])","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"}]},{"title":"clustering","slug":"clustering","date":"2019-01-05T11:30:02.000Z","updated":"2019-01-10T11:43:08.374Z","comments":true,"path":"2019/01/05/clustering/","link":"","permalink":"https://p829911.github.io/2019/01/05/clustering/","excerpt":"","text":"주어진 데이터 집합을 유사한 데이터들의 그룹으로 나누는 것을 클러스터링(clustering)이라고 하고 이렇게 나누어진 유사한 데이터의 그룹을 클러스터(cluster)라 한다. 클러스터링은 예측(prediction) 문제와 달리 특정한 독립변수와 종속변수의 구분도 없고 학습을 위한 목푯값(target value)도 필요로 하지 않는 비지도학습(unsupervised learning)의 일종이다. 클러스터링 방법대부분의 클러스터링 방법들도 예측모형처럼 특정한 목표함수의 값을 최소화 혹은 최대화하지만 예측모형과 달리 다만 목표함수가 명확히 주어지지 않았기 때문에 목표함수의 정의 및 최적화 방법들이 각기 다른 다양한 클러스터링 방법이 존재한다. 다음과 같은 클러스터링 방법이 많이 쓰인다. K-means DBSCAN Spectral Clustering Affinity Propagation 계층적 클러스터링(Hierarchical Clustering) 클러스터링 방법은 사용법과 모수 등이 서로 다르다. 예를 들어 K-means, Spectral Clustering 등은 클러스터의 갯수를 미리 지정해 주어야 하지만 DBSCAN이나 Affinity Propagation 등은 클러스터의 갯수를 지정할 필요가 없다. 다만 이 경우에는 다른 종류의 모수를 지정해주어야 하는데 이 모수의 값에 따라 클러스터링 갯수가 달라질 수 있다. 클러스터링 성능기준클러스터링 성능의 경우에는 분류문제와 달리 성능기준을 만들기 어렵다. 심지어는 원래 데이터가 어떻게 클러스터링되어 있었는지를 보여주는 정답(groundtruth)이 있는 경우도 마찬가지이다. 따라서 다양한 성능기준이 사용되고 있다. 다음은 클러스터링 성능기준의 예이다. Adjusted Rand Index Adjusted Mutual Information Silhouette Coefficient Incidence Matrix(adjusted) Rand index를 구하려면 데이터가 원래 어떻게 클러스터링되어 있어야 하는지를 알려주는 정답(groundtruth)이 있어야 한다. $N$개의 데이터 집합에서 $i, j$ 두 개의 데이터를 선택하였을 때 그 두 데이터가 같은 클러스터에 속하면 1 다른 데이터에 속하면 0이라고 하자. 이 값을 $N \\times N$ 행렬 $T$로 나타내자. T_{ij} = \\begin{cases} 1 &amp; \\text{$i$와 $j$가 같은 클러스터} \\\\ 0 &amp; \\text{$i$와 $j$가 다른 클러스터} \\end{cases}예를 들어 $\\{0, 1, 2, 3, 4\\}$라는 5개의 데이터 집합에서 $\\{0, 1, 2\\}$과 $\\{3, 4\\}$이 같은 클러스터라면 다음과 같아진다. 1234567groundtruth = np.array([ [1, 1, 1, 0, 0], [1, 1, 1, 0, 0], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1],]) 이제 클러스터링을 적용한 결과를 같은 방법으로 행렬 $C$로 표시하자. 만약 클러스터링이 정확하다면 이 행렬은 정답을 이용해서 만든 행렬과 거의 같은 값을 가져야 한다 만약 클러스터링 결과 $\\{0,1\\}$ 과 $\\{2, 3, 4\\}$ 이 같은 클러스터라면 다음과 같아진다. 1234567clustering = np.array([ [1, 1, 0, 0, 0], [1, 1, 0, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 1],]) 이 두 행렬의 모든 원소에 대해 값이 같으면 1 다르면 0으로 계산한 행렬을 incidence matrix라고 한다. 즉 데이터 집합에서 만들 수 있는 모든 데이터 쌍에 대해 정답과 클러스터링 결과에서 동일한 값을 나타내면 1, 다르면 0이 된다. R_{ij} = \\begin{cases} 1 &amp; \\text{if $T_{ij} = C_{ij}$} \\\\ 0 &amp; \\text{if $T_{ij} \\neq C_{ij}$} \\end{cases}즉, 원래 정답에서 1번 데이터와 2번 데이터가 다른 클러스터인데 클러스터링 결과에서도 다른 클러스터라고 하면 $R_{12} = 0$이다. 위 예제에서 incidence matrix를 구하면 다음과 같다. 1234567incidence = 1 * (groundtruth == clustering)incidencearray([[1, 1, 0, 1, 1], [1, 1, 0, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 1]]) Adjusted Rand IndexRand index는 가능한 모든 쌍의 경우에 대해 정답인 쌍의 갯수의 비율로 정의한다. 이 값은 groundtruth를 목표값, 클러스터링 결과를 예측값으로 하는 이진분류문제의 정확도(accuracy)에 해당한다. 123rand_index = np.sum(incidence) / np.prod(incidence.shape)rand_index0.68 Rand index는 0부터 1까지의 값을 가지고 1이 가장 좋은 성능을 뜻한다. Rand index의 문제점은 무작위로 클러스터링을 한 경우에도 어느 정도 좋은 값이 나올 가능성이 높다는 점이다. 즉 무작위 클러스터링 에서 생기는 Rand index의 기댓값이 너무 크다. 이를 해결하기 위해 무작위 클러스터링에서 생기는 Rand index의 기댓값을 원래의 값에서 빼서 기댓값과 분산을 재조정한 것이 adjusted Rand index다. adjusted Rand index는 무작위 클러스터링의 경우에 0이 나올 확률이 높다. 하지만 경우에 따라서는 음수가 나올 수도 있다. adjusted Rand index를 계산하려면 다음과 같은 contigency table을 만들어야 한다. contingency table은 정답과 클러스터링 결과에서 각각 같은 클러스터에 속하는 데이터의 갯수를 나타낸 것이다. 정답은 T = \\{T_1, T_2, \\dots, T_r \\}의 $r$ 개의 클러스터이고 클러스터링 결과는 C = \\{C_1, C_2, \\cdots, C_3 \\}의 $s$ 개의 클러스터라고 가정한다. \\begin{array}{c|cccc|c} T \\; \\backslash \\; C &amp; C_1&amp; C_2&amp; \\ldots&amp; C_s&amp; \\text{소계} \\\\ \\hline T_1&amp; n_{11}&amp; n_{12}&amp; \\ldots&amp; n_{1s}&amp; a_1 \\\\ T_2&amp; n_{21}&amp; n_{22}&amp; \\ldots&amp; n_{2s}&amp; a_2 \\\\ \\vdots&amp; \\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots&amp; \\vdots \\\\ T_r&amp; n_{r1}&amp; n_{r2}&amp; \\ldots&amp; n_{rs}&amp; a_r \\\\ \\hline \\text{소계}&amp; b_1&amp; b_2&amp; \\ldots&amp; b_s&amp; \\end{array} $n_{ij}$: 정답에서는 클러스터 $T_i$ 에 속하고 클러스터링 결과에서는 클러스터 $C_j$ 에 속하는 데이터의 수 $a_i = \\sum_{j=1}^s n_{ij}$ $b_j = \\sum_{i=1}^r n_{ij}$ 무작위 클러스터링의 rand index 기댓값을 구하는 공식을 적용하면 adjusted Rand index값이 다음처럼 정의된다. \\text{ARI} = \\frac{ \\overbrace{\\sum_{ij} \\binom{n_{ij}}{2}}^\\text{Index} - \\overbrace{[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}^\\text{기댓값} }{ \\underbrace{\\frac{1}{2} [\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}]}_\\text{최댓값} - \\underbrace{[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}_\\text{기댓값} }위에서 예로 들었던 타원형 데이터 예제에 대해 여러가지 클러스터링 방법을 적용하였을때 adjusted Rand index 값을 계산해보면 DBSCAN과 Spectral Clustering의 값이 높게 나오는 것을 확인할 수 있다. scikit-learn 패키지의 metrics.cluster 서브패키지는 adjusted_rand_score 명령을 제공한다. 123456789101112131415161718from sklearn.metrics.cluster import adjusted_rand_scoreX, y_true = anisotropicX = StandardScaler().fit_transform(X)for name, algorithm in clustering_algorithms: with ignore_warnings(category=UserWarning): algorithm.fit(X) if hasattr(algorithm, 'labels_'): y_pred = algorithm.labels_.astype(np.int) else: y_pred = algorithm.predict(X) print(\"&#123;:25s&#125;: ARI=&#123;:5.3f&#125;\".format(name, adjusted_rand_score(y_true, y_pred))) K-Means : ARI=0.607DBSCAN : ARI=0.975Spectral Clustering : ARI=0.959Hierarchical Clustering : ARI=0.685Affinity Propagation : ARI=0.617 Adjusted Mutual Informationmutual information은 두 확률변수간의 상호 의존성을 측정한 값이다. 클러스터링 결과를 이산확률변수라고 가정한다. 정답은 T = \\{T_1, T_2, \\cdots, T_r \\}의 $r$ 개의 값을 가질 수 있는 이산확률변수이고 클러스터링 결과는 C = \\{C_1, C_2, \\cdots, C_s \\}의 $s$ 개의 값을 가질 수 있는 이산확률변수라고 하자. 전체 데이터의 갯수를 $N$ 이라고 하면 이산확률변수 $T$ 의 분포는 P(i) = \\dfrac{|T_i|}{N}로 추정할 수 있다. 이 식에서 $|T_i|$ 는 클러스터 $T_i$ 에 속하는 데이터의 갯수를 나타낸다. 비슷하게 이산확률변수 $C$ 의 분포는 P&#39;(j) = \\dfrac{|C_i|}{N}라고 추정하고 $T$ 와 $C$ 의 결합확률분포는 P(i, j) = \\dfrac{|\\;T_i \\;\\cap\\; C_j\\;|}{N}라고 추정한다. 여기에서 $|T_i \\cap C_j|$ 는 클러스터 $T_i$ 에도 속하고 클러스터 $C_j$ 에도 속하는 데이터의 갯수를 나타낸다. 확률변수 $T, C$ 의 mutual information은 MI(T, C) = \\sum_{i=1}^r\\sum_{j=1}^s P(i,j)로 정의한다. 만약 두 확률변수가 서로 독립이면 mutual information의 값은 0이며 이 값이 mutual information이 가질 수 있는 최소값이다. 두 확률변수가 의존성이 강할수록 mutual information은 증가한다. 또한 클러스터의 갯수가 많아질수록 mutual information이 증가하므로 올바른 비교가 어렵다. 따라서 adjusted Rand index의 경우와 마찬가지로 각 경우에 따른 mutual information의 기댓값을 빼서 재조정한 것이 adjusted mutual information이다. 다음은 위에서 예로 들었던 타원형 데이터 예제에 대해 여러가지 클러스터링 방법을 적용하였을때 adjusted mutual information 값을 계산한 결과이다. scikit-learn 패키지의 metrics.cluster 서브패키지는 adjusted_mutual_info_score 명령을 제공한다. 123456789101112131415161718from sklearn.metrics.cluster import adjusted_mutual_info_scoreX, y_true = anisotropicX = StandardScaler().fit_transform(X)for name, algorithm in clustering_algorithms: with ignore_warnings(category=UserWarning): algorithm.fit(X) if hasattr(algorithm, 'labels_'): y_pred = algorithm.labels_.astype(np.int) else: y_pred = algorithm.predict(X) print(\"&#123;:25s&#125;: ARI=&#123;:5.3f&#125;\".format(name, adjusted_mutual_info_score(y_true, y_pred)))array([[1, 1, 0, 1, 1], [1, 1, 0, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 1]]) 실루엣 계수지금까지는 각각의 데이터가 원래 어떤 클러스터에 속해있었는지 정답(groudtruth)를 알고 있는 경우를 다루었다. 하지만 이러한 정답 정보가 없다면 어떻게 클러스터링이 잘되었는지 판단할 수 있을까? 실루엣 계수(Silhouette coeffient)는 이러한 경우에 클러스터링의 성능을 판단하기 위한 기준의 하나이다. 우선 모든 데이터 쌍 $(i, j)$ 에 대해 거리(distance) 혹은 비유사도(dissimilarity)을 구한다. 이 결과를 이용하여 모든 데이터 $i$에 대해 다음 값을 구한다. $a_i$: $i$ 와 같은 클러스터에 속한 원소들의 평균 거리 $b_i$: $i$ 와 다른 클러스터 중 가장 가까운 클러스터까지의 평균 거리 실루엣 계수는 s = \\dfrac{b - a}{\\text{max}(a,b)}로 정의한다. 만약 데이터 $i$ 에 대해 같은 클러스터의 데이터가 다른 클러스터의 데이터보다 더 가깝다면 실루엣 계수는 양수가 된다. 하지만 만약 다른 클러스터의 데이터가 같은 클러스터의 데이터보다 더 가깝다면 클러스터링이 잘못된 경우라고 볼 수 있는데 이 때는 실루엣 계수가 음수가 된다. 실루엣 계수가 클수록 보다 좋은 클러스터링이라고 이야기 할 수 있다. 클러스터링 방법 중에는 클러스터의 갯수를 사용자가 정해주어야 하는 것들이 있는데 실루엣 계수는 이 경우 클러스터의 갯수를 정하는데 큰 도움이 된다. 앞에서 예로 들었던 3개의 원형 데이터에 대해 KMean 방법으로 클러스터 갯수를 바꾸어가면서 클러스터링 결과를 살펴보자. scikit-learn 패키지의 metrics 서브패키지는 실루엣 계수를 계산하는 silhouette_samples 명령을 제공한다. 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.metrics import silhouette_samplesX = StandardScaler().fit_transform(blobs[0])colors = plt.cm.tab10(np.arange(20, dtype=int))plt.figure(figsize=(6, 8))for i in range(4): model = KMeans(n_clusters=i + 2, random_state=0) cluster_labels = model.fit_predict(X) sample_silhouette_values = silhouette_samples(X, cluster_labels) silhouette_avg = sample_silhouette_values.mean() plt.subplot(4, 2, 2 * i + 1) y_lower = 10 for j in range(i + 2): jth_cluster_silhouette_values = sample_silhouette_values[cluster_labels == j] jth_cluster_silhouette_values.sort() size_cluster_j = jth_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_j plt.fill_betweenx(np.arange(y_lower, y_upper), 0, jth_cluster_silhouette_values, facecolor=colors[j], edgecolor=colors[j]) plt.text(-0.05, y_lower + 0.5 * size_cluster_j, str(j + 1)) plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") plt.xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]) plt.yticks([]) plt.title(\"실루엣 계수 평균: &#123;:5.2f&#125;\".format(silhouette_avg)) y_lower = y_upper + 10 plt.subplot(4, 2, 2 * i + 2) plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[cluster_labels]) plt.xlim(-2.5, 2.5) plt.ylim(-2.5, 2.5) plt.xticks(()) plt.yticks(()) plt.title(\"클러스터 수: &#123;&#125;\".format(i + 2))plt.tight_layout()plt.show()","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Clustering","slug":"Math/Clustering","permalink":"https://p829911.github.io/categories/Math/Clustering/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Clustering","slug":"Clustering","permalink":"https://p829911.github.io/tags/Clustering/"}]},{"title":"dark sky 사이트를 이용한 날씨 크롤링","slug":"dark-sky-사이트를-이용한-날씨-크롤링","date":"2019-01-01T09:45:00.000Z","updated":"2019-01-01T09:46:24.182Z","comments":true,"path":"2019/01/01/dark-sky-사이트를-이용한-날씨-크롤링/","link":"","permalink":"https://p829911.github.io/2019/01/01/dark-sky-사이트를-이용한-날씨-크롤링/","excerpt":"","text":"https://darksky.net/dev 다음 사이트를 이용해 api로 날씨 데이터를 크롤링 해 볼 것이다. dark sky api는 전 세계 현재/과거/미래에 관련된 날씨와 관련된 많은 데이터들을 제공한다. 사이트에 가입 후 console로 들어가면 다음과 같은 창이 나타난다. Your Secret Key에 있는 key를 사용하여 dark sky로 쿼리를 날려야 한다. dark sky api 는 하루에 1000건의 무료 call을 제공하고 있으며 1건이 추가 될 때마다 $0.0001를 받고 있다. 1000건이 넘는 요청을 해야 할 경우 account setting에서 카드 등록을 하고 하루 최대 call수를 늘려주면 요청이 정상적으로 된다. 이 포스트 에서는 구글 맵을 이용하여 지역 명으로 위도 경도를 얻은 후 그 위도 경도를 이용하여 dark sky에 요청하여 날씨 정보를 받는 실습을 진행해 보았다. 자세한 코드는 아래 링크에서 확인 가능하다. https://github.com/p829911/python_study/blob/master/crawling/dark_sky_weather_crawling.ipynb","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"},{"name":"Crawling","slug":"Crawling","permalink":"https://p829911.github.io/tags/Crawling/"}]},{"title":"모형 최적화","slug":"모형-최적화","date":"2019-01-01T08:13:29.000Z","updated":"2019-01-01T08:21:50.409Z","comments":true,"path":"2019/01/01/모형-최적화/","link":"","permalink":"https://p829911.github.io/2019/01/01/모형-최적화/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 머신 러닝 모형이 완성된 후에는 최적화 과정을 통해 예측 성능을 향상시킨다. Scikit-Learn 의 모형 하이퍼 파라미터 튜닝 도구Scikit-Learn에서는 다음과 같은 모형 최적화 도구를 지원한다. validation_curve 단일 하이퍼 파라미터 최적화 GridSearchCV 그리드를 사용한 복수 하이퍼 파라미터 최적화 ParameterGrid 복수 파라미터 최적화용 그리드 validation_curvevalidation_curve 함수는 최적화할 파라미터 이름과 범위, 그리고 성능 기준을 param_name, param_range, scoring 인수로 받아 파라미터 범위의 모든 경우에 대해 성능 기준을 계산한다. 12345678from sklearn.datasets import load_digitsfrom sklearn.svm import SVCfrom sklearn.model_selection import validation_curvedigits = load_digits()X, y = digits.data, digits.targetparam_range = np.logspace(-6, -1, 10) 12345%%timetrain_scores, test_scores = \\ validation_curve(SVC(), X, y, param_name=\"gamma\", param_range=param_range, cv=10, scoring=\"accuracy\", n_jobs=1) 123456789101112131415161718train_scores_mean = np.mean(train_scores, axis=1)train_scores_std = np.std(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1)test_scores_std = np.std(test_scores, axis=1)mpl.rcParams[\"font.family\"] = 'DejaVu Sans'plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")plt.legend(loc=\"best\")plt.title(\"Validation Curve with SVM\")plt.xlabel(\"$\\gamma$\")plt.ylabel(\"Score\")plt.ylim(0.0, 1.1)plt.show() GridSearchCVGridSearchCV 클래스는 validation_curve 함수와 달리 모형 래퍼(Wrapper) 성격의 클래스이다. 클래스 객체에 fit 메서드를 호출하면 grid search를 사용하여 자동으로 복수개의 내부 모형을 생성하고 이를 모두 실행시켜서 최적 파라미터를 찾아준다. 생성된 복수개와 내부 모형과 실행 결과는 다음 속성에 저장된다. grid_scores_ param_grid 의 모든 파라미터 조합에 대한 성능 결과. 각각의 원소는 다음 요소로 이루어진 튜플이다. parameters: 사용된 파라미터 mean_validation_score: 교차 검증(cross-validation) 결과의 평균값 cv_validation_scores: 모든 교차 검증(cross-validation) 결과 best_score_ 최고 점수 best_params_ 최고 점수를 낸 파라미터 best_estimator_ 최고 점수를 낸 파라미터를 가진 모형 \u0002\u0002 1234567891011121314from sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import SVCpipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]param_grid = [ &#123;'clf__C': param_range, 'clf__kernel': ['linear']&#125;, &#123;'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']&#125;]gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=1)%time gs = gs.fit(X, y) 1gs.cv_results_[\"params\"] 1gs.cv_results_[\"mean_test_score\"] 12print(gs.best_score_)print(gs.best_params_) ParameterGrid때로는 scikit-learn 이 제공하는 GridSearchCV 이외의 방법으로 그리드 탐색을 해야하는 경우도 있다. 이 경우 파라미터를 조합하여 탐색 그리드를 생성해 주는 명령어가 ParameterGrid 이다. ParameterGrid 는 탐색을 위한 iterator 역할을 한다. 1from sklearn.model_selection import ParameterGrid 12param_grid = &#123;'a': [1, 2], 'b': [True, False]&#125;list(ParameterGrid(param_grid)) 12param_grid = [&#123;'kernel': ['linear']&#125;, &#123;'kernel': ['rbf'], 'gamma': [1, 10]&#125;]list(ParameterGrid(param_grid)) 병렬 처리GridSearchCV 명령에는 n_jobs 라는 인수가 있다. 디폴트 값은 1인데 이 값을 증가시키면 내부적으로 멀티 프로세서를 사용하여 그리드서치를 수행한다. 만약 CPU 코어의 수가 충분하다면 n_jobs 를 늘릴수록 속도가 증가한다. 123param_grid = &#123;\"gamma\": np.logspace(-6, -1, 10)&#125;gs1 = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=1)gs2 = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=2) 12%%timegs1.fit(X, y) 12%%timegs2.fit(X, y)","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://p829911.github.io/categories/Math/Classification/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Classification","slug":"Classification","permalink":"https://p829911.github.io/tags/Classification/"}]},{"title":"hexo 테마 변경하기","slug":"hexo-테마-변경하기","date":"2018-12-13T14:05:43.000Z","updated":"2018-12-13T15:23:42.000Z","comments":true,"path":"2018/12/13/hexo-테마-변경하기/","link":"","permalink":"https://p829911.github.io/2018/12/13/hexo-테마-변경하기/","excerpt":"","text":"hexo 폰트 변경 나눔스퀘어라운드 (NanumSquareRound) https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css head.ejs 수정하기1&lt;%- css(&apos;https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css&apos;) %&gt; 1vi blog/themes/hueman/layout/common/head.ejs head 부분에 추가 가운데 보면 위의 코드를 넣어 준 것을 볼 수 있다. css 적용1vi blog/themes/hueman/source/css/_variables.styl font-sans 본문 폰트 : NanumSquareRound 추가 font-mono 코드 폰트 : D2Coding을 추가 hexo theme color 변경 위와 같이 hueman 테마의 색상을 변경할 수 있다. 1vi blog/themes/hueman/source/css/_variables.styl color-default: hexo 폴더 안에 있는 _config.yml 파일에서 설정해 줄 수 있는 색상이다. 위에 사진에 보이는 follow 부분에 나타나는 색상을 적용해 줄 수 있다, 여기서 적용하지 말고 _config.yml파일에서 바꿔준다. color-header-background: 위에 보이는 파란색 영역에 색상을 지정해 줄 수 있다. color-border: 위에 보이는 노란색 영역에 색상을 지정해 줄 수 있다. color-nav-background: 검은색 영역에 색상 지정 color-footer-background: 맨 밑의 영역에 색상 지정 color-sidebar- background: sidebar의 배경색은 모바일에서는 바뀌지만, 데스크탑 사이트에서는 바뀌지 않는다 이 부분의 문제점은 좀 더 찾아봐야할 것 같다. 아래의 링크에서 색상 선택에 도움을 받을 수 있을 것이다.Hex Color Code","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Blog","slug":"Blog","permalink":"https://p829911.github.io/tags/Blog/"}]},{"title":"entropy","slug":"entropy","date":"2018-12-13T11:18:47.000Z","updated":"2018-12-13T11:29:43.000Z","comments":true,"path":"2018/12/13/entropy/","link":"","permalink":"https://p829911.github.io/2018/12/13/entropy/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 엔트로피의 정의$Y = 0$ 또는 $Y=1$인 두 가지 값을 가지는 확률 분포가 다음과 같이 세 종류가 있다고 하자 확률 분포 $Y_1$: $P(Y=0) = 0.5, P(Y=1) = 0.5$ 확률 분포 $Y_2$: $P(Y=0) = 0.8, P(Y=1) = 0.2$ 확률 분포 $Y_3$ : $P(Y=0) = 0.8, P(Y=1) = 0.2$ 이 확률 값이 베이지안 확률이라면 확률 분포 $Y_1$은 $y$값에 대해 아무것도 모르는 상태, $Y_3$은 $y$값이 0이라고 100% 확신하는 상태, $Y_2$은 $y$값이 0이라고 믿지만 아닐 수도 있다는 것을 아는 상태를 나타내고 있을 것이다. 확률 분포들이 가지는 확신의 정도를 수치로 표현하는 것을 엔트로피(entropy)라고 한다. 확률 변수의 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 높아진다. 반대로 특정한 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피가 작아진다. 물리학에서는 상태가 분산되어 있는 정도를 엔트로피로 정의한다. 여러가지로 고루 분산되어 있을 수 있으면 엔트로피가 높고 특정한 하나의 상태로 몰려있으면 엔트로피가 낮다. 확률분포의 엔트로피는 물리학의 엔트로피 용어를 빌려온 것이다. 엔트로피는 수학적으로 다음과 같이 정의한다 확률변수 $Y$가 베르누이나 카테고리 분포와 같은 이산 확률변수이면 H[Y] = -\\sum_{k=1}^K P(y_k) log_2 P(y_k)이 식에서 $K$는 $X$가 가질 수 있는 클래스의 수이고 $P(y)$는 확률질량함수이다. 확률변수 $Y$가 연속 확률변수이면 H[Y] = - \\int_{-\\infty}^{\\infty} p(y) log_2p(y)\\;dy이 식에서 $P(y)$는 확률밀도함수이다. 로그의 밑(base)이 2로 정의된 것은 정보통신과 관련을 가지는 역사적인 이유 때문이다. 위에서 예를 든 $Y_1$, $Y_2$, $Y_3$에 대해 엔트로피를 구하면 다음과 같다. \\begin{eqnarray} H[Y_1] &=& -\\frac{1}{2}log_2\\frac{1}{2} - \\frac{1}{2}log_2\\frac{1}{2} &=& 1 \\\\\\\\ H[Y_2] &=& -\\frac{8}{10}log_2\\frac{8}{10} - \\frac{2}{10}log_2\\frac{2}{10} &=& 0.72 \\\\\\\\ H[Y_3] &=& -1log_21 - 0log_20 &=& 0 \\end{eqnarray}엔트로피 계산에서 $p(y) = 0$인 경우에는 다음과 같은 극한값을 사용한다. 이 값은 로피탈의 정리에서 구할 수 있다. 엔트로피의 성질확률변수가 결정론적이면 확률분포에서 특정한 하나의 값이 나올 확률이 1이다. 이 때 엔트로피는 0이 되고 이 값은 엔트로피가 가질 수 있는 최솟값이다. 반대로 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산 확률변수가 가질 수 있는 클래스가 $2^K$개이고 이산 확률변수가 가질 수 있는 엔트로피의 최대값은 각 클래스가 모두 같은 확률을 가지는 때이다. 이 때 엔트로피의 값은 H = -\\frac{2^K}{2^K}log_2\\frac{1}{2^K} = K이다. 엔트로피와 정보량엔트로피는 확률변수가 담을 수 있는 정보의 양을 의미한다고 볼 수도 있다. 확률변수가 담을 수 있는 정보량(information)이란 확률변수의 표본값을 관측해서 얻을 수 있는 추가적인 정보의 종류를 말한다. 엔트로피가 0이면 확률변수는 결정론적이므로 확률 변수의 표본값은 항상 같다. 따라서 확률 변수의 표본값을 관측한다고 해도 우리가 얻을 수 있는 추가 정보는 없다. 반대로 엔트로피가 크다면 확률변수의 표본값이 가질 수 있는 경우의 수가 증가하므로 표본값을 실제로 관측하기 전까지는 아는 것이 거의 없다. 반대로 말하면 확률변수의 표본값이 우리에게 가져다 줄 수 있는 정보의 양이 많다. 엔트로피와 무손실 인코딩엔트로피는 원래 통신 분야에서 데이터가 가지고 있는 정보량을 계산하기 위해 고안되었다. 예를 들어 4개의 알파벳 A, B, C, D로 씌어진 문서가 있다고 하자. 이 문서를 0과 1로 이루어진 이진수로 변환할 때 일반적인 경우라면 다음과 같이 인코딩을 할 것이다. A = “00” B = “01” C = “10” D = “11” 이렇게 인코딩을 하면 1,000 글자로 이루어진 문서는 이진수 2,000개가 된다. 만약 문서에서 각 알파벳이 나올 확률이 동일하지 않고 다음과 같다고 가정하다. \\Big\\{ \\dfrac{1}{2}, \\dfrac{1}{4}, \\dfrac{1}{8}, \\dfrac{1}{8} \\Big\\}이 때는 다음과 같이 가변길이 인코딩(variable length encoding)을 하면 인코딩된 이진수의 수를 줄일 수 있다. A = “0” B = “10” C = “110” D = “111” 인코딩된 이진수의 숫자는 다음 계산에서 약 1,750개가 됨을 알 수 있다. \\left(1000 \\times \\frac{1}{2}\\right) \\cdot 1 + \\left(1000 \\times \\frac{1}{4}\\right) \\cdot 2 + \\left(1000 \\times \\frac{1}{8}\\right) \\cdot 3 + \\left(1000 \\times \\frac{1}{8}\\right) \\cdot 3 = 17501.75는 알파벳 한 글자를 인코딩하는데 필요한 평균 비트(bit)수이며 확률변수의 엔트로피 값과 같다. H = -\\frac{1}{2}log_2\\frac{1}{2}-\\frac{1}{4}log_2\\frac{1}{4}-\\frac{2}{8}log_2\\frac{1}{8} = 1.75엔트로피의 추정확률 변수 모형, 즉 이론적인 확률 밀도(질량) 함수가 아닌 실제 데이터가 주어진 경우에는 확률질량함수를 추정하여 엔트로피를 계산한다. 예를 들어 데이터가 모두 80개가 있고 그 중 $Y=0$인 데이터가 40개, $Y=1$인 데이터가 40개 있는 경우는 엔트로피가 1이다. P(y=0) = \\frac{40}{80} = \\frac{1}{2} \\\\ \\\\ P(y=1) = \\frac{40}{80} = \\frac{1}{2} \\\\ \\\\ H[Y] = -\\frac{1}{2}log_2\\left(\\frac{1}{2}\\right) -\\frac{1}{2}log_2\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1만약 데이터가 모두 60개가 있고 그 중 $Y=0$인 데이터가 20개, $Y=1$인 데이터가 40개 있는 경우는 엔트로피가 약 0.92이다. P(y=0) = \\frac{20}{60} = \\frac{1}{3}\\\\ P(y=1) = \\frac{40}{60} = \\frac{2}{3}\\\\ H[Y] = -\\frac{1}{3}log_2\\left(\\frac{1}{3}\\right)-\\frac{2}{3}log_2\\left(\\frac{2}{3}\\right) = 0.92지니불순도엔트로피와 유사한 개념으로 지니불순도(Gini Impurity)라는 것이 있다. 지니불순도는 엔트로피처럼 확률분포가 어느쪽에 치우쳐있는가를 재는 척도지만 로그를 사용하지 않으므로 계산량이 더 적어 엔트로피 대용으로 많이 사용된다. G[Y] = \\sum_{k=1}^K P(y_k)(1-P(y_k))1234567891011P0 = np.linspace(0.001, 1 - 0.001, 1000)P1 = 1 - P0H = - P0 * np.log2(P0) - P1 * np.log2(P1)G = (P0 * (1 - P0) + P1 * (1 - P1)) # 엔트로피와 높이를 맞춰주기 위해 스케일링 하는 경우도 있다. 여기서는 G 앞에 2를 곱해준다.plt.plot(P0, H, \"-\", label=\"엔트로피\")plt.plot(P0, G, \"--\", label=\"지니불순도\")plt.legend()plt.xlabel(\"P(0)\")plt.show() 결합 엔트로피두 이산확률변수 $X, Y$에 대해 결합 엔트로피(joint entropy)는 다음처럼 정의한다. H[X,Y] = -\\sum_{i=1}^{K_x}\\sum_{j=1}^{K_y}P(x_i,y_j)log_2P(x_i,y_j)연속확률변수의 경우에는 다음처럼 정의한다. H[X,Y] = - \\int_x\\int_y p(x,y)log_2p(x,y)\\; dxdy조건부 엔트로피조건부 엔트로피는 상관관계가 있는 두 확률변수 $X,Y$가 있고 $X$의 값을 안다면 $Y$의 확률변수가 가질 수 있는 정보의 양을 뜻한다. 수학적으로는 다음과 같이 정의한다. H[Y \\mid X] = - \\sum_{i=1}^{K_x} \\sum_{j=1}^{K_y} P(x_i,y_j)\\,log2\\, P(y_j \\mid x_i)연속확률변수의 경우에는 다음처럼 정의한다. H[Y \\mid X] = - \\int_x \\int_y p(x,y) \\, log_2\\,p(y \\mid x)\\; dxdy조건부 엔트로피는 조건부 확률 분포의 정의를 사용하여 다음과 같이 고칠 수 있다. H[Y \\mid X] = - \\sum_{i=1}^{K_x} \\,P(x_i)\\,H[Y \\mid X=x_i]연속확률변수의 경우에는 다음과 같다. H[Y \\mid X] = - \\int_{x} \\,p(x) \\,H[Y \\mid X=x] \\; dx크로스 엔트로피두 확률분포 $p(x), p(y)$의 크로스 엔트로피(cross entropy) $H[p,q]$는 다음과 같이 정의한다. 크로스 엔트로피의 경우에는 같은 확률변수에 대한 두 개의 추정 확률분포를 비교하는데 주로 쓰이기 때문에 표기를 할 땐 결합 엔트로피처럼 확률변수를 인수로 사용하지 않고 확률분포를 인수로 사용한다는 점에 주의해라 H[P,Q] = -\\sum_{k=1}^K P(y_k)\\, \\log_2\\, Q(y_k)또는 H[p,q] = - \\int_yp(y)\\log_2\\,q(y)\\,dy크로스 엔트로피는 확률분포의 차이를 정량화한 값이지만 기준이 되는 분포가 $p$로 고정되어 있다. 즉 $p$와 $q$가 바뀌면 값이 달라진다. H[p,q] \\neq H[q,p]크로스 엔트로피는 분류 모형의 성능을 측정하는데 사용된다. $Y$가 0 또는 1이라는 값만 가지는 이진 분류 문제를 예로 들어보자. $P_Y$는 $X$가 정해졌을 때 실제 $Y$가 가지는 분포를 뜻한다. $X$가 정해지면 $Y$는 확실히 0이거나 확실히 1이다. 즉, $P_Y$는 (0,1) 또는 (1,0)이 된다. 하지만 예측값 $\\hat{Y}$의 분포 $Q_{\\hat{Y}}$는 모수가 $\\mu$인 베르누이 분포이다. 즉 $Q_{\\hat{Y}}$는 $(1-\\mu, \\mu)$이다. 특정한 $X$에 대해 $P$와 $Q$의 크로스 엔트로피는 H[P,Q] = \\begin{cases} & -\\log_2 (1-\\mu) & Y=0 \\text{일 때} \\\\ & -\\log_2 \\mu & Y=1 \\text{일 때} \\end{cases}가 된다. $Y = 0$일 때 $\\mu$가 커질수록 즉, 예측이 틀릴수록 $-\\log_2 (1-\\mu)$의 값도 커진다. $Y = 1$일 때, $\\mu$가 작아질수록 즉, 예측이 틀릴수록 $-\\log_2(\\mu)$의 값도 커진다. 따라서 위 값은 예측의 틀린정도를 나타내는 오차 함수의 역할을 할 수 있다. 모든 데이터에 대해 이 값의 평균을 구하면 다음 식으로 표현할 수 있다. H[P,Q] = - \\frac{1}{N} \\sum_{i=1}^N(y_i \\log_2 \\mu_i + (1-y_i)\\log_2 (1-\\mu))같은 방법으로 이진 분류가 아닌 다중 분류에서도 크로스 엔트로피를 오차 함수로 사용할 수 있다. 쿨백-라이블러 발산쿨백-라이블러 발산(Kullback-Leibler divergence)은 두 확률분포 $p(y), q(y)$의 차이를 정량화하는 방법의 하나이다. 다음과 같이 정의한다. \\begin{eqnarray} KL(P || Q) &=& H[P,Q] - H[P] &=& \\sum_{i=1}^{K} P(y_i) \\log_2 \\left(\\dfrac{P(y_i)}{Q(y_i)}\\right) \\end{eqnarray}또는 \\begin{eqnarray} KL(p || q) &=& H[p, q] - H[p] &=& \\int p(y) \\log_2 \\left(\\dfrac{p(y)}{q(y)}\\right) dy \\end{eqnarray}쿨백-라이블러 발산은 크로스 엔트로피에서 기준이 되는 분포의 엔트로피 값을 뺀 값이므로 상대 엔트로피(relative entropy)라고도 한다. 그 값은 항상 양수이며 두 확률분포 $p(x), q(x)$가 완전히 같을 경우에만 0이 된다.","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://p829911.github.io/categories/Math/Classification/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Classification","slug":"Classification","permalink":"https://p829911.github.io/tags/Classification/"}]},{"title":"mysql install","slug":"mysql-install","date":"2018-12-06T12:52:38.000Z","updated":"2018-12-06T12:54:22.000Z","comments":true,"path":"2018/12/06/mysql-install/","link":"","permalink":"https://p829911.github.io/2018/12/06/mysql-install/","excerpt":"","text":"mysql install1sudo apt install -y mysql-server 1sudo mysql_secure_installaion press y|Y for Yes, any other key for No: n remove anonymous users? y disallow root login remotely? n remove test database and access to it? n reload privilege tables now? y success! 1sudo mysql passward 설정1234SELECT user,authentication_string,plugin,host FROM mysql.user;ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_passwordBY &apos;dss&apos;;FLUSH PRIVILEGES; root 사용자로 접속12mysql -u root -p# 패스워드 입력 상태확인bash 창에서 1sudo systemctl status mysql active 확인 외부접속 설정1sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf bind-address = 0.0.0.0으로 수정 외부접속이 허용되도록 mysql 설정 1mysql -u root -p 1grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;&lt;password&gt;&apos;; 포트 활성화AWS서버에서 3306포트 활성화 재시작으로 설정 적용 1sudo systemctl restart mysql.service mysql에서 접속 import sql file바로 데이터 베이스 넣기 1mysql -u root -p world &lt; world.sql mysql shell에서 넣기 1234create database world;use world;source world.sql;show tables; Mysql 삭제12345sudo apt remove --purge mysql-server mysql-clientsudo rm -rf /etc/mysql/var/lib/mysqlsudo apt autoremovesudo apt autocleansudo apt purge mysql*","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://p829911.github.io/categories/Mysql/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"},{"name":"Mysql","slug":"Mysql","permalink":"https://p829911.github.io/tags/Mysql/"}]},{"title":"다중 선형 회귀","slug":"regression","date":"2018-12-06T12:33:47.000Z","updated":"2018-12-13T11:22:47.000Z","comments":true,"path":"2018/12/06/regression/","link":"","permalink":"https://p829911.github.io/2018/12/06/regression/","excerpt":"","text":"다중 선형 회귀 설명변수들 $X_1, X_2, \\cdots, X_p$ 중 적어도 하나는 반응변수를 예측하는데 유용한가? $Y$를 설명하는 데 모든 설명변수들이 도움이 되는가? 또는 설명변수들의 일부만이 유용한가? 모델은 데이터에 얼마나 잘 맞는가? 주어진 설명변수 값들에 대해 어떤 반응변수 값을 예측해야 하고 그 예측은 얼마나 정확한가? 1. 반응변수와 설명변수 사이에 상관관계가 있는가? 단순선형회귀에서 반응변수와 설명변수 사이에 상관관계가 있는지는 단순히 $\\beta_1 = 0$인지 검사하면 결정할 수 있다. $p$개 설명변수가 있는 다중회귀에서는 모든 회귀계수들이 영인지, 즉 $\\beta_1 = \\beta_2 = \\cdots = \\beta_p =0$인지를 검사해야 한다. 단순선형회귀에서와 같이 이 질문에 답하기 위해 가설검정을 사용한다. 귀무가설 H_0 : \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0대립가설 H_a: \\text{적어도 하나의 }\\beta_j\\text{는 영이 아니다.}이러한 가설 검정은 $F$-통계량을 계산함으로써 이루어진다. F = \\dfrac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n-p-1)}단순선형회귀에서와 같이 $\\text{TSS} = \\sum(y_i-\\bar{y})^2$ 이고 $\\text{RSS} = \\sum(y_i - \\hat{y_i})^2$이다. 만약 선형 모델의 가정이 같다면 다음이 성립함을 보여줄 것이다. E[\\,\\text{RSS}\\, / \\,(n-p-1)\\,] = \\sigma^2또한 귀무가설 $H_0$이 참이면 다음이 성립함을 보여줄 수도 있다. E[\\,(\\text{TSS} - \\text{RSS})\\,/\\,p\\,] = \\sigma^2그러므로, 반응변수와 설명변수들 사이에 상관관계가 없는 경우(RSS가 커질때 TSS와 거의 가까움) $F$- 통계량이 1에 매우 가까운 값이라고 기대할 수 있을 것이다. 반면에 만약 대립가설 $H_a$가 참이면 $E[\\,(\\text{TSS}-\\text{RSS})\\,/\\, p\\,] &gt; \\sigma^2$이고 그래서 $F$의 기대값은 1보다 크다. $H_0$을 기각하고 상관관계가 있다고 결론을 내릴 수 있으면 $F$-통계량이 얼마나 커야될까? $n\\text{과} p$값에 따라 다르다. $n$이 큰 경우에는 $F$-통계량이 1보다 약간만 크면 $H_0$에 반하는 증거가 된다. $n$이 작은 경우 $H_0$를 기각하려면 더 큰 $F$-통계량이 필요하다. 2. 중요 변수의 결정 모든 설명변수가 반응변수와 상관성이 있을 수도 있다. 하지만 대부분의 경우 설명변수들의 일부(서브셋)만이 반응변수와 상관관계가 있다. 상관성이 있는 설명변수만으로 모델 적합을 수행하기 위해 어느 설명변수가 반응변수와 상관성이 있는지 결정하는 것을 변수선택이라고 한다. 모델의 질 평가 맬로우즈(Mallows) $C_p$ AIC (Akaike information criterion) BIC (Bayesian information criterion) 수정된 $R^2$ $p$개 변수들의 일부를 포함하는 총 모델의 경우의 수는 $2^p$개에 이른다. 심지어 $p$가 크지 않더라도 모든 가능한 설명변수들의 부분집합을 다 시험해 보는 것은 현실적으로 어렵다. 예를 들어 $p = 2$인 경우 $2^2 = 4$모델을 고려하면 된다. 그러나 $p=30$이면 고려해야 하는 모델 수는 $2^{30} = 1,073,741,824$개로 늘어나 현실적으로 불가능에 가깝다. 그러므로 $p$가 아주 작은 경우가 아니면 $2^p$개 모델 모두를 고려할 수는 없고, 대신에 더 작은 수의 고려할 모델 집합을 선택하는 자동화되고 효과적인 기법이 필요하다. 이 목적을 위한 3가지 고전적인 기법은 아래와 같다. 전진선택: 이 방법은 절편만 포함하고 설명변수는 없는 영모델(null model)을 가지고 시작한다. $p$개의 단순 선형 회귀를 적합하여 가장 낮은 $RSS$가 발생되는 변수를 영모델에 추가한다. 그런 다음 새로운 새로운 2-변수 모델에 대해 가장 낮은 $RSS$가 생기는 변수를 모델에 추가한다. 이런 방식으로 어떤 정지규칙(stopping rule)을 만족할 때까지 계속된다. 후진선택(Backward selection): 이 방법은 모델의 모든 변수를 가지고 시작하여 가장 큰 $p$-값을 가지는 변수, 즉 통계적으로 중요도가 가장 낮은 변수를 제외한다. 그 다음에 새로운 (p-1)-변수의 모델을 적합하고 $p$-값이 가장 큰 변수를 제외한다. 이 과정을 정지 규칙이 만족될 때까지 계속한다. 예를 들어 모든 남아있는 변수들의 $p$-값이 어떤 임계치보다 작으면 이 과정을 중지한다. 혼합선택(Mixed selection): 이것은 전진선택과 후진선택을 결합한 것이다. 전진선택처럼 변수가 없는 모델로 시작하여 최상의 적합을 제공하는 변수를 하나씩 추가한다. 새로운 설명변수들이 모델에 추가됨에 따라 변수들에 대한 $p$-값이 커질 수 있다. 그러므로 모델의 변수들 중 어느 하나에 대한 $p$-값이 어떤 임계치보다 커지면 그 변수를 모델에서 제외한다. 이러한 전진선택 및 후진선택 단계를 계속하여 모델에 포함되는 모든 변수들은 충분히 작은 $p$-값을 가지고 모델에서 제외된 변수들은 만약 모델에 추가될 경우 $p$-값이 크게 될 때 중지한다. 후진 선택법은 만약 p &gt; n 이면 사용할 수 없지만 전진선택법은 항상 사용할 수 있다. 전진선택법은 그리디(greedy)방식이다. 그래서 초기에 포함된 변수들이 나중에는 유효하지 않을 수 있다. 이 문제는 혼합선택법으로 선택할 수 있다. 3. 모델 적합 모델 적합의 수치적 측도로 가장 흔히 사용되는 두가지는 $RSE$와 $R^2$(설명되는 분산의 비율)이다. 이 값들은 단순선형회귀에서와 같은 방식으로 계산되고 해석된다. 단순회귀에서 $R^2$은 반응변수와 설명변수의 상관계수의 곱이다. 다중 선형회귀에서 이것은 반응변수와 적합된 선형모델 사이의 상관계수의 제곱인 $Cor(Y,\\hat{Y})^2$과 동일하다. 사실 적합된 선형모델은 모든 가능한 선형모델 중에서 이 상관계수가 최대로 되는 것이다. 1에 가까운 $R^2$값은 모델이 반응변수 내 분산의 많은 부분을 설명한다는 것을 나타낸다. 모델에 더 많은 변수가 추가되면 비록 추가된 변수와 반응변수의 상관관계가 아주 약하더라도 $R^2$은 항상 증가할 것이다. 이것은 최소제곱 방정식에 변수를 추가하면 훈련 데이터(반드시 검정 데이터일 필요는 없다.)를 더 정확하게 적합할 수 있다는 사실 때문이다. 특정 독립 변수를 추가했을 때 $R^2$이 약간만 증가한다는 사실은 그 독립 변수가 모델에서 제외될 수 있다는 추가적인 증거가 된다. 모델에 그 독렵변수를 포함하는 것은 독립적인 검정표본에 대한 과적합으로 인해 좋지 않은 결과를 초래할 가능성이 높을 것이다. 4. 예측 다중선형모델 적합을 수행하였으면 $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\cdots + \\hat{\\beta_p}x_p$을 적용하여 설명변수 $X_1, X_2, \\cdots, X_p$의 값에 기초하여 반응 변수 $Y$를 예측하는 것은 어렵지 않다. 하지만, 이러한 예측에는 세 가지 명확하지 않은 것이 연관되어 있다. 계수추정 $\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_p}$는 $\\beta_0,\\beta_1,\\cdots, \\beta_p$에 대한 추정값이다. 즉 아래 최소제곱평면은 \\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X_1 + \\cdots + \\hat{\\beta_p}X_p다음의 실제 모회귀평면에 대한 추정값이다. f(X) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p계수추정의 부정확도는 2장의 축소가능 오차(reducible error)와 관련된다. 신뢰구간을 계산하여 $\\hat{Y}$가 $f(x)$에 얼마나 가까운지 결정할 수 있다. 물론, 실제로 $f(x)$에 대해 선형 모델을 가정하는 것은 거의 항상 현실에 대한 근사이다. 따라서 모델 편향(model bias)이라고 하는 잠재적으로 축소가능한 오차의 또 다른 출처가 있다. 그러므로 선형모델을 사용할 때 실제 표면에 대한 최상의 선형 근사를 추정하는 것이다. 하지만, 여기서는 이러한 차이를 무시하고 마치 선형 모델이 올바른 것으로 간주한다. 심지어 $f(x)$를 알아도 - $\\beta_0, \\beta_1, \\cdots, \\beta_p$에 대한 실제 값을 알아도 - 모델의 랜덤오차 때문에 반응변수 값을 완벽하게 예측할 수는 없다. 2장에서 이 오차를 축소불가능 오차(irreducible error)라고 하였다. 선형 모델의 확장표준선형회귀모델 $Y = \\beta_0 + \\beta_1X1 + \\beta_2X2 + \\cdots + \\beta_pX_p + \\epsilon$ 는 해석이 가능한 결과를 제공하며 많은 현실적인 문제에 대해서도 잘 동작한다. 하지만 이것은 실제로는 성립되지 않는 몇 가지 아주 제한적인 가정을 사용한다. 가장 중요한 가정 중 두 가지는 설명변수와 반응변수 사이의 관계는 가산적(additive)이고 선형적이라는 것이다. 가산성 가정: 설명변수 $X_j$의 변화가 반응변수 $Y$에 미치는 영향은 다른 설명변수 값에 독립적이다. 선형성 가정: $X_j$의 한 유닛 변화로 인한 $Y$의 변화는 $X_j$의 값에 관계없이 상수이다. 가산성 가정의 제거 Advertising 자료 분석에서 TV와 radio 둘 다 sales와 상관관계가 있다고 결론지었다. 이러한 결론의 근거가 되는 선형모델들은 한 광고매체의 지출 증가가 sales에 미치는 영향은 다른 매체에 대한 지출과 무관하다고(독립적이라고) 가정한다. 하지만, 이런 단순한 모델은 맞지 않을 수 있다. 라디오 광고 지출이 실제로 TV 광고의 효과를 증가시켜 TV에 대한 기울기 항이 라디오 지출이 늘어남에 따라 증가해야 한다고 해보자. 이러한 경우, 주어진 10만 달러의 고정 광고예산을 라디오와 TV에 절반씩 지출하는 것이 전체 예산을 TV 또는 라디오 어느 한쪽에 모두 사용하는 것보다 판매량 증가가 더 클 수 있다. 이것을 마케팅에서는 시너지 효과 라 하고 통계학에서는 상호작용 효과 라 한다. 두 개의 변수를 가지는 표준 선형회귀모델을 고려해보자 Y = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\epsilon이 모델에 따르면 $X_1$이 한 유닛 증가하면 $Y$는 평균 $\\beta_1$유닛만큼 증가할 것이다. $X_2$의 존재는 이 사실을 변경하지 않는다. 즉, $X_2$의 값에 관계없이 $X_1$이 한 유닛 증가하면 $Y$는 $\\beta_1$ 유닛 증가할 것이다. 상호작용 효과를 포함하도록 이 모델을 확장하는 한 가지 방법은 상호작용 항이라 불리는 세 번째 설명변수를 포함하는 것이다. 상호작용 항은 $X_1$과 $X_2$의 곱으로 구성된다. 그러면 결과 모델은 다음과 같이 표현 된다. Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon위의 식은 아래와 같이 다시 쓸 수 있다. \\begin{eqnarray} Y &=& \\beta_0 + (\\beta_1 + \\beta_3X_2)X_1 + \\beta_2X_2 + \\epsilon\\\\ &=& \\beta_0 + \\tilde{\\beta_1}X_1 + \\beta_2X_2 + \\epsilon \\end{eqnarray}여기서 $\\tilde{\\beta_1} = \\beta_1 + \\beta_3X_2$이다. $\\tilde{\\beta_1}$은 $X_2$에 따라 변하므로 $Y$에 대한 $X_1$의 효과는 더이상 상수가 아니다. $X_2$를 조정하면 $Y$에 대한 $X_1$의 효과가 변할 것이다. 이것은 실제 상관관계는 가산적이지 않다는 것이 명백하다. 가끔씩 상호작용 항은 매우 작은 $p$-값을 가지지만 관련된 주효과는 그렇지 않은 경우도 있다. 계층적 원리 에 의하면, 만약 모델에 상호작용을 포함하면 주효과는 그 계수와 연관된 $p$-값이 유의하지 않더라도 모델에 포함해야 한다. 다시 말해, 만약 $X_1$과 $X_2$사이의 상호작용이 중요한 것 같으면 $X_1$과 $X_2$의 계수 추정치가 큰 $p$-값을 가져도 모델에 $X_1$과 $X_2$를 포함해야 한다. 이유는 만약 $X_1 \\times X_2 $가 반응변수와 상관관계가 있으면 $X_1$또는 $X_2$의 계수가 영인지는 관심이 없다. 또한 $X_1 \\times X_2$는 보통 $X_1$ 및 $X_2$와 상관되어 있어 이들을 제외하는 것은 상호작용의 의미를 바꾸는 경향이 있다. 상호작용의 개념은 질적 변수 또는 양적 변수와 질적 변수의 조합에도 적용된다. 사실, 질적 변수와 양적 변수 사이의 상호작용을 해석하기는 특히 쉽다. 상호작용 항이 없을 경우 모델은 다음 형태를 가진다. \\begin{eqnarray} \\text{balance}_i &\\approx& \\beta_0 + \\beta_1 \\times \\text{income}_i + \\begin{cases}\\beta_2 & i\\text{번째 사람이 학생인 경우} \\\\ 0 & i\\text{번째 사람이 학생이 아닌 경우} \\end{cases} \\\\\\\\ &=& \\beta_1 \\times \\text{income}_i + \\begin{cases}\\beta_0 + \\beta_2 & i\\text{번째 사람이 학생인 경우} \\\\ \\beta_0 & i\\text{번째 사람이 학생이 아닌 경우}\\end{cases} \\end{eqnarray}이것은 두 개의 평행한 직선을 데이터에 적합하는 것이다. 학생과 학생이 아닌 사람에 대한 두 직선은 다른 절편 $\\beta_0 + \\beta_2$와 $\\beta_0$을 가지지만 동일한 기울기 $\\beta_1$을 가진다. 두 직선이 평행하다는 사실이 의미하는 것은 income의 한 유닛 증가가 balance에 미치는 평균 효과는 그 사람이 학생인지 아닌지에 의존적이지 않다는 것을 의미한다. 이것은 모델이 잠재적으로 심각한 한계가 있음을 나타낸다. 왜냐하면 소득의 변화는 학생과 학생이 아닌 사람의 신용카드 대금에 아주 다른 효과를 줄 수 있기 때문이다. 이 한계는 income을 student에 대한 가변수와 곱하여 얻은 상호작용 변수를 추가함으로써 해결할 수 있다. 그러면 모델은다음과 같이 표현된다. \\begin{eqnarray} \\text{balance}_i &\\approx& \\beta_0 + \\beta_1 \\times \\text{income}_i + \\begin{cases}\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{학생인 경우}\\\\ 0 & \\text{학생이 아닌 경우}\\end{cases}\\\\ \\\\ &=& \\begin{cases}(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & 학생인 경우 \\\\ \\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{학생이 아닌 경우}\\end{cases} \\end{eqnarray}이 경우에도 학생과 학생이 아닌 사람에 대한 회귀 직선이 다르다. 그러나 이번에는 두 직선의 절편 뿐만 아니라 기울기도 다르다. 학생인 경우, 회귀직선의 절편은 $\\beta_0 + \\beta_2$, 기울기는 $\\beta_1 + \\beta_3$이다. 학생이 아닌 경우에는 절편은 $\\beta_0$, 기울기는 $\\beta_1$이다. 이것은 소득 변화가 신용카드 대금에 미치는 영향이 학생인지의 여부에 따라 다를 수 있게 한다. 학생에 대한 기울의 기울기가 학생이 아닌 경우에 대한 것보다 작은데, 이것은 소득 증가에 따른 카드 대금의 증가가 학생인 경우 학생이 아닌 사람보다 낮다는 것을 시사한다. 비선형 상관관계","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Regression","slug":"Math/Regression","permalink":"https://p829911.github.io/categories/Math/Regression/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Regression","slug":"Regression","permalink":"https://p829911.github.io/tags/Regression/"}]},{"title":"git 설치 & 사용법","slug":"git-install-and-how-to-use","date":"2018-12-03T12:58:52.000Z","updated":"2018-12-19T05:45:28.000Z","comments":true,"path":"2018/12/03/git-install-and-how-to-use/","link":"","permalink":"https://p829911.github.io/2018/12/03/git-install-and-how-to-use/","excerpt":"","text":"git은 터미널에서 다음과 같은 명렁어로 설치 할 수 있다.1sudo apt-get install git 설치가 완료 되었으면 1git --version git config 설정 1234git config --global user.name \"username\"git config --global user.email \"github email address\"git config --global core.editor \"vim\"git config --list local에 있는 폴더와 git 저장소 연동시키기 폴더 생성 후 git init 해주기 123git initgit add.git commit -m \"contents\" git repository 생성 해 준 후 12git remote add origin https://github.com/username/repo.gitgit push origin master 위의 코드에서 origin이라는 부분은 사용자가 지정할 수 있지만, 보통 origin이라고 써줌 git 에서 저장소를 clone 할 때는 자동적으로 origin이라고 지정 된다. git 저장소 local 폴더로 복제하기1git clone https://github.com/username/repo.git 개인적으로는 git remote 보다 git clone해서 사용하는 것이 더 편하다. branch소프트웨어를 개발할 때 개발자들은 동일한 소스코드를 함께 공유한다. 동일한 소스코드 위에서 서로 다른 작업을 할 때는 각각 서로 다른 버전의 코드가 만들어 질 수 밖에 없다.이럴 때, 여러 개발자들이 동시에 다양한 작업을 할 수 있게 만들어 주는 기능이 바로 ‘브랜치(Branch)’이다.이렇게 분리된 작업 영역에서 변경된 내용은 나중에 원래의 버전과 비교해서 하나의 새로운 버전으로 만들어 낼 수 있다. 브랜치란 독립적으로 어떤 작업을 진행하기 위한 개념이다. 필요에 의해 만들어지는 각각의 브랜치는 다른 브랜치의 영향을 받지 않기 때문에, 여러 작업을 동시에 진행할 수 있다. 또한 이렇게 만들어진 브랜치는 다른 브랜치와 병합(Merge)함으로써, 작업한 내용을 다시 새로운 하나의 브랜치로 모을 수 있다. 저장소를 처음 만들면, Git은 바로 ‘master’라는 이름의 브랜치를 만들어 준다. 이 새로운 저장소에 새로운 파일을 추가한다거나 추가한 파일의 내용을 변경하여 그 내용을 저장(커밋, commit)하는 것은 모두 ‘master’라는 이름의 브랜치를 통해 처리할 수 있다. 12345678git branch &lt;branchname&gt; # 브랜치 생성git branch # 브랜치 확인git branch -r # remote 브랜치 확인git branch -a # 모든 사용가능한 브랜치 확인git checkout &lt;branchname&gt; # 브랜치 지정git checkout -b &lt;branchname&gt; # 브랜치 생성, 체크아웃git merge &lt;branchname&gt; # 브랜치 병합git branch -d &lt;branchname&gt; # 브랜치 삭제 참고 https://backlog.com/git-tutorial/kr/stepup/stepup1_1.html https://github.com/ulgoon/dss-linux-git","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"},{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"}]},{"title":"ubuntu permission","slug":"ubuntu-permission","date":"2018-11-27T08:44:04.000Z","updated":"2018-11-28T09:35:45.000Z","comments":true,"path":"2018/11/27/ubuntu-permission/","link":"","permalink":"https://p829911.github.io/2018/11/27/ubuntu-permission/","excerpt":"","text":"파일 정보 보기 ls -al: 현재 위치에 있는 파일들을 자세히 보여주는 명령 파일 Type 퍼미션정보 링크수 소유자 소유그룹 용량 생성날짜 파일이름 d rwxr-xr-x 28 p829911 p829911 4096 11월 27 20:30 .ipython 파일 Type: d (디렉토리), ㅣ (링크파일), - (일반파일) 퍼미션정보: 해당 파일에 어떠한 퍼미션이 부여되어 있는지 링크수: 해당 파일이 링크된 수, 윈도우의 바로가기와 같다. in [대상파일][링크파일]명령으로 만든다. 소유자: 해당 파일의 소유자 이름 소유그룹: 해당 파일을 소유한 그룹이름 용량: 파일의 용량 생성날짜: 파일이 생성된 날짜 파일이름: 파일의 이름 퍼미션 정보 앞에서 두번째부터 아홉번째까지 퍼미션 정보이다. rwxr-xr-x 퍼미션 종류 r : 파일의 읽기 권한 w : 파일의 쓰기 권한 x : 파일의 실행 권한 퍼미션의 사용자지정 소유자: 소유자에 대한 퍼미션 지정 rwx 그룹: 소유그룹에 대한 퍼미션 지정 r-x 공개: 모든 사용자들에 대한 퍼미션 지정 r-x -: 그 퍼미션은 없다 소유자는 읽기, 쓰기, 실행을 허용하고 파일의 소유그룹에 속하고 있는 사용자들은 읽기 실행만 허용하고 이외에 나머지 모든 사용자들도 읽기, 실행만 허용한다. 퍼미션 변경하기chmod [변경될 퍼미션값][변경할 파일] 각 퍼미션 기호를 숫자로 변환한다. ( r = 4, w = 2, x = 1)ex) r - x = 4 0 1 변환한 숫자를 합산한다.ex) 4 0 1 = 5 rwxr-xr-x = 755 chmod 775 [변경할 파일] : 변경할 파일이 755에 해당되는 퍼미션으로 변경된다. 디렉토리의 경우 -R 옵션을 사용하면 하위 디렉토리의 모든 디렉토리 및 파일의 퍼미션이 변경된다.ex) chmod -R 777 [변경할 디렉토리] 소유자 변경하기chown [변경할 소유자][변경할 파일] 이 명령으로 소유자뿐만 아니라 소유그룹도 변경할 수 있다. [변경할 소유자]에 .그룹이름 형식으로 입력하면 된다. ex) p829911.text 파일의 소유자를 p829911 소유그룹을 p829911로 동시에 변경할 경우chown p829911.p829911 p829911.text","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"ubuntu apt 명령어","slug":"ubuntu-apt-명령어","date":"2018-11-27T07:50:51.000Z","updated":"2018-11-27T08:05:17.000Z","comments":true,"path":"2018/11/27/ubuntu-apt-명령어/","link":"","permalink":"https://p829911.github.io/2018/11/27/ubuntu-apt-명령어/","excerpt":"","text":"우분투에서 패키지를 관리하는 명령어가 몇 가지 있다. 그 중 apt-get과 apt-cache를 결합한 apt에 관한 명령어를 알아보겠다. 패키지 목록 갱신 1apt update 모든 패키지를 최신 버전으로 업그레이드 12apt install upgradeapt full-upgrade # 의존성 고려한 패키지 업그레이드 패키지 설치 1apt install package_name 패키지 삭제 1apt remove package_name 패키지 삭제(설정 파일 포함) 1apt purge package_name 불필요한 패키지 제거 1apt autoremove 패키지 검색 1apt search package_name 패키지 상세 정보 출력 1apt show package_name apt 명령어 사용법 &amp; 옵션 1apt -h 패키지 리스트 출력 1apt list 권한 문제가 발생할 경우 sudo 명령을 붙여 root로 실행할 수 있다.","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"교차 검증","slug":"교차-검증","date":"2018-11-27T06:32:01.000Z","updated":"2018-12-13T11:23:22.000Z","comments":true,"path":"2018/11/27/교차-검증/","link":"","permalink":"https://p829911.github.io/2018/11/27/교차-검증/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 표본 내 성능과 표본 외 성능회귀분석 모형을 만들기 위해서는 모수 추정 즉 학습을 위한 데이터 집합이 필요하다. 보통 회귀분석 성능을 이야기할 때는 이 학습 데이터 집합의 종속 변수값을 얼마나 잘 예측하였는지를 결정 계수(codefficient of determination) 등을 이용하여 따진다. 이러한 성능을 표본 내 성능 검증(in-sample testing)이라고 한다. 그런데 회귀분석 모형을 만드는 목적 중 하나는 종속 변수의 값을 아직 알지 못하고 따라서 학습에 사용하지 않은 표본에 대해 종속 변수의 값을 알아내고자 하는 것 즉 예측(prediction)이다. 이렇게 학습에 쓰이지 않는 표본 데이터 집합의 종속 변수 값을 얼마나 잘 예측하는가를 검사하는 것을 표본 외 성능 검증(out-of-sample testing) 혹은 교차 검증(cross validation)이라고 한다. 과최적화일반적으로 표본 내 성능과 표본 외 성능은 비슷한 수준을 보이지만 경우에 따라서는 표본 내 성능은 좋으면서 표본 외 성능이 상대적으로 많이 떨어지는 수도 있다. 이러한 경우를 과최적화(overfitting)라고 한다. 과최적화가 발생하면 학습에 쓰였던 표본 데이터에 대해서는 종속변수의 값을 잘 추정하지만 새로운 데이터를 주었을 때 전혀 예측하지 못하기 때문에 예측 목적으로는 쓸모없는 모형이 된다. 검증용 데이터 집합교차 검증을 하려면 두 종류의 데이터 집합이 필요하다. 모형 추정 즉 학습을 위한 데이터 집합 (training data set) 성능 검증을 위한 데이터 집합 (test data set) 두 데이터 집합 모두 종속 변수값이 있어야 한다. 따라서 보통은 가지고 있는 데이터 집합을 학습용과 검증용으로 나누어 학습용 데이터만을 사용하여 회귀분석 모형을 만들고 검증용 데이터로 성능을 계산하는 학습/검증 데이터 분리(train-test split) 방법을 사용한다. statsmodels 패키지에서의 교차 검증사실 소수의 입력 변수와 소규모 데이터를 사용하는 전통적인 회귀분석에서는 다항 회귀 등의 방법으로 모형 차수를 증가시키지 않는 한 과최적화가 잘 발생하지 않는다. 따라서 statsmodels 패키지에는 교차 검증을 위한 기능이 별도로 준비되어 있지 않고 사용자가 직접 코드를 작성해야 한다. scikit-learn의 교차 검증 기능독립 변수의 개수가 많은 빅데이터에서는 과최적화가 쉽게 발생한다. 따라서 scikit-learn 의 model_selection 서브 패키지는 교차 검증을 위한 다양한 명령을 제공한다. 단순 데이터 분리train_test_split 명령은 데이터를 학습용 데이터와 검증용 데이터로 분리한다. 1train_test_split(data, data2, test_size, train_size, random_state) data: 독립 변수 데이터 배열 또는 pandas 데이터 프레임 data2: 종속 변수 데이터. data인수에 종속 변수 데이터가 같이 있으면 생략할 수 있다. test_size: 검증용 데이터 개수. 1보다 작은 실수이면 비율을 나타낸다. train_size: 학습용 데이터의 개수. 1보다 작은 실수이면 비율을 나타낸다. test_size와 train_size중 하나만 있어도 된다. random_state: 난수 시드 1234from sklearn.model_selection import train_test_splitdf_train, df_test = train_test_split(df, test_size=0.3, random_state=0)df_train.shape, df_test.shape 12dfx_train, dfx_test, dfy_train, dfy_test = train_test_split(dfx, dfy, test_size=0.3, random_state=0)dfx_train.shape, dfy_train.shape, dfx_test.shape, dfy_test.shape K- 폴드 교차 검증데이터의 수가 적은 경우에는 이 데이터 중의 일부인 검증 데이터의 수도 적기 때문에 검증 성능의 신뢰도가 떨어진다. 그렇다고 검증 데이터의 수를 증가시키면 학습용 데이터의 수가 적어지므로 정상적인 학습이 되지 않는다. 이러한 딜레마를 해결하기 위한 검증 방법이 K-폴드(K-fold) 교차 검증 방법이다. K-폴드 교차 검증에서는 전체 데이터를 K개의 부분집합($\\{1, 2, \\cdots , K\\}$)로 나눈 뒤 다음과 같이 학습과 검증을 반복한다. 데이터 $\\{1, 2, \\cdots, K - 1\\}$를 학습용 데이터로 사용하여 회귀분석 모형을 만들고 데이터 $\\{K\\}$ 로 교차 검증을 한다. 데이터 $\\{1, 2, \\cdots, K - 2, K\\}$를 학습용 데이터로 사용하여 회귀분석 모형을 만들고 데이터 $\\{K-1\\}$로 교차 검증을 한다. $\\vdots$ 데이터 $\\{2, \\cdots, K\\}$를 학습용 데이터로 사용하여 회귀분석 모형을 만들고 데이터 $\\{1\\}$로 교차 검증을 한다. 이렇게 하면 총 K개의 모형과 K개의 교차 검증 성능이 나온다. 이 K개의 교차 검증 성능을 평균하여 최종 교차 검증 성능을 계산한다. scikit-learn 패키지의 model_selection 서브 패키지는 KFold 클래스를 비롯한 다양한 교차 검증 생성기를 제공한다. 이 생성기의 split 메서드는 학습용과 검증용의 데이터 인덱스를 출력하는 파이썬 반복자(iterator)를 반환한다. 123456789101112131415161718from sklearn.model_selection import KFoldscores = np.zeros(5)cv = KFold(5, shuffle=True, random_state=0)for i, (idx_train, idx_test) in enumerate(cv.split(df)): df_train = df.iloc[idx_train] df_test = df.iloc[idx_test] model = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train) result = model.fit() pred = result.predict(df_test) rss = ((df_test.MEDV - pred) ** 2).sum() tss = ((df_test.MEDV - df_test.MEDV.mean()) ** 2).sum() rsquared = 1 - rss/tss scores[i] = rsquared print(\"train R2 = &#123;:.8f&#125;, test R2 = &#123;:.8f&#125;\".format(result.rsquared, rsquared))","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Regression","slug":"Math/Regression","permalink":"https://p829911.github.io/categories/Math/Regression/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Regression","slug":"Regression","permalink":"https://p829911.github.io/tags/Regression/"}]},{"title":"hexo mathjax","slug":"hexo-mathjax","date":"2018-11-26T14:36:53.000Z","updated":"2018-11-26T14:53:50.000Z","comments":true,"path":"2018/11/26/hexo-mathjax/","link":"","permalink":"https://p829911.github.io/2018/11/26/hexo-mathjax/","excerpt":"","text":"rendering engine changeHexo의 기본 renderer인 hexo-renderer-marked는 mathjax 문법을 지원하지 않는다. 따라서 mathjax를 지원하는 rendering engine으로 교체해준다. 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save &lt;blog dir&gt;/node_modules/hexo-renderer-kramed/lib/renderer.js를 열어 return값을 text로 수정한다. 12345function formatText(text) &#123; // Fit kramed's rule: $$ + \\1 + $$ // return text.replace(/`\\$(.*?)\\$`/g, '$$$$$1$$$$'); return text;&#125; install mathjaxmathjax plugin 설치 1npm install hexo-renderer-mathjax --save &lt;blog dir&gt;/node_modules/hexo-renderer-kramed/node_modules/hexo-renderer-mathjax/mathjax.html 을 열고 URL을 수정해준다. 12&lt;!-- &lt;script src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"&gt;&lt;/script&gt; --&gt;&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'&gt;&lt;/script&gt; LaTex와 markdown 문법 충돌 해결하기&lt;blog dir&gt;/node_modules/kramed/lib/rules/inline.js를 열고 다음과 같이 수정한다. 12escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, Mathjax 사용하기사용하고 있는 theme의 _config.yml파일을 열고 다음과 같이 수정한다. 12mathjax: enable: true markdown post 작성post 작성시 header 부분에 mathjax: true를 넣어주면 블로그에서 수식이 보이게 된다.","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Blog","slug":"Blog","permalink":"https://p829911.github.io/tags/Blog/"},{"name":"Mathjax","slug":"Mathjax","permalink":"https://p829911.github.io/tags/Mathjax/"}]},{"title":"분산 분석","slug":"분산-분석","date":"2018-11-26T14:21:57.000Z","updated":"2018-12-13T11:23:10.000Z","comments":true,"path":"2018/11/26/분산-분석/","link":"","permalink":"https://p829911.github.io/2018/11/26/분산-분석/","excerpt":"","text":"이 포스트는 fastcampus에서 강의를 하고 계시는 김도형 박사님의 강의록을 따라 쓰며 연습한 포스트입니다. 데이터 사이언스 스쿨 선형회귀분석의 결과가 얼마나 좋은지는 단순히 잔차제곱합(RSS: Residual sum of square)으로 평가할 수 없다. 변수의 단위 즉, 스케일이 달라지면 회귀분석과 상관없이 잔차제곱합도 달라지기 때문이다. 분산 분석(ANOVA: Analysis of Variance)은 종속변수의 분산과 독립변수의 분산간의 관계를 사용하여 선형회귀분석의 성능을 평가하고자 하는 방법이다. 분산 분석은 서로 다른 두 개의 선형회귀분석의 성능 비교에 응용할 수 있으며 독립변수가 카테고리 변수인 경우 각 카테고리 값에 따른 영향을 정량적으로 분석하는데도 사용된다. $\\hat{y}$를 종속변수 $y$의 샘플 평균이라고 하자. \\hat{y} = \\frac{1}{N} \\sum_{i=1}^N y_iTSS (total sum of squares) \\text{TSS} = \\sum_{i=1}^N (y_i-\\bar{y})^2 = (y-\\bar{y})^T(y-\\bar{y})종속변수값의 움직임의 범위를 나타낸다. ESS(explained sum of squares) \\text{ESS} = \\sum_{i=1}^N (\\hat{y_i}-\\bar{\\hat{y}})^2 = (\\hat{y}-\\bar{\\hat{y}})^T(\\hat{y}-\\bar{\\hat{y}})회귀 분석에 의해 예측한 값 $\\hat{y}$의 분산을 나타낸다. 모형에서 나온 예측값의 움직임의 범위를 뜻한다. RSS (residual sum of squares) \\text{RSS} = \\sum_{i=1}^N(y_i-\\hat{y_i})^2 = e^Te잔차 $e$의 분산을 나타낸다. 잔차의 움직임의 범위, 즉 오차의 크기를 뜻한다. 만약 회귀모형이 상수항을 포함하여 올바르게 정의되었다면 잔차의 평균이 0이 된다. 즉 종속변수의 평균과 모형 예측값의 평균이 같아진다. \\bar{e} = \\bar{y} - \\bar{\\hat{y}} = 0 \\bar{y} = \\bar{\\hat{y}}그리고 이 분산값들 간에는 다음과 같은 관계가 성립한다. \\text{TSS} = \\text{ESS} + \\text{RSS}위 식이 말하는 바는 다음과 같다. 모형 예측치의 움직임의 크기(분산)은 종속변수의 움직임의 크기(분산)보다 클 수 없다. 모형의 성능이 좋을 수록 모형 예측치의 움직임의 크기는 종속변수의 움직임의 크기와 비슷해진다. example 123456789101112import numpyimport pandasfrom sklearn.datasets import make_regressionx0, y, coef = make_regression(n_samples=100, n_features=1, noise=30, coef=True, random_state=0)dfx0 = pd.DataFrame(x0, columns=[\"X\"])dfx = sm.add_constant(dfx0)dfy = pd.DataFrame(y, columns=[\"Y\"])df = pd.concat([dfx, dfy], axis=1)model = sm.OLS.from_formula(\"Y ~ X\", data=df)result = model.fit() 12345print(\"TSS = \", result.uncentered_tss)print(\"ESS = \", result.mse_model)print(\"RSS = \", result.ssr)print(\"ESS + RSS = \", result.mse_model + result.ssr)print(\"R squared = \", result.rsquared) 결정 계수(Coefficient of Determination) 위의 분산 관계식에서 모형의 성능을 나타내는 결정계수(Coefficient of Determination) $R^2$를 정의할 수 있다. R^2 \\equiv 1- \\dfrac{\\text{RSS}}{\\text{TSS}} = \\dfrac{\\text{ESS}}{\\text{TSS}}분산 관계식과 모든 분산값이 0보다 크다는 점을 이용하면 $R^2$의 값은 다음과 같은 조건을 만족한다. 0 \\leq R^2 \\leq 1여기에서 $R^2$가 0 이라는 것은 오차의 분산 RSS가 최대이고 회귀분석 예측값의 분산 ESS가 0인 경우이므로 회귀분석 결과가 아무런 의미가 없다는 뜻이다. 반대로 $R^2$가 1이라는 것은 오차의 분산 RSS가 0이고 회귀분석 예측의 분산 ESS가 TSS와 같은 경우이므로 회귀분석 결과가 완벽하다는 뜻이다. 따라서 결정계수값은 회귀분석의 성능을 나타내는 수치라고 할 수 있다. 분산 분석표 분산 분석의 결과는 보통 다음과 같은 분산 분석표를 사용하여 표시한다. 아래의 표에서 $N$은 데이터의 갯수, $K$는 모수(독립변수의 갯수, 상수항 포함)의 갯수를 뜻한다. source degree of freedom sum of square mean square F test-statistics p-value Regression $K -1$ $\\text{ESS}$ $s_\\hat{y}^2 = \\dfrac{\\text{ESS}}{K-1}$ $F = \\dfrac{s_\\hat{y}^2}{s_e^2}$ p-value Residual $N - K$ $\\text{RSS}$ $s_e^2 = \\dfrac{\\text{RSS}}{N-K}$ Total $N - 1$ $\\text{TSS}$ $s_y^2 = \\dfrac{\\text{TSS}}{N-1}$ $R^2$ $\\dfrac{\\text{ESS}}{\\text{TSS}}$ 결정 계수와 상관 계수 $y$와 $\\hat{y}$의 샘플 상관계수 $r$의 제곱은 결정 계수 $R^2$와 같다. 상수항이 없는 모형의 경우 모형에서 상수항을 지정하지 않은 경우에는 결정계수의 정의에 사용되는 TSS의 정의가 다음과 같이 달라진다. \\text{TSS} = \\sum_{i=1}^{N}y_i^2 = y^Ty즉, 실제 샘플평균과 상관없이 $\\bar{y} = 0$ 이라는 가정하에 TSS를 계산한다. 이렇게 정의하지 않으면 TSS = RSS + ESS 관계식이 성립하지 않아서 결정계수의 값이 1보다 커지게 된다. 따라서 모형의 결정계수를 비교할 때 상수항이 없는 모형과 상수항이 있는 모형은 직접 비교하면 안된다. F 검정을 이용한 모형 비교 F 검정을 이용하면 다음과 같이 포함관계에 있는 두 모형의 성능을 비교할 수 있다. 전체 모형(Full Model): y = w_0 + w_1x_1 + w_2x_2 + w_3x_3 축소 모형(Reduced Model): y = w_0 + w_1x_1 다음과 같은 귀무가설을 검정하는 것은 위의 두 모형이 실질적으로 같은 모형이라는 가설을 검정하는 것과 같다. H_0 : w_2 = w_3 = 0이 검정도 F검정을 사용하여 할 수 있다. StatsModels에서는 anova_lm명령에 두 모형의 result 객체를 인수로 넣어주면 이러한 검정을 할 수 있다. 인수를 넣어줄 때는 축소 모형(reduced model), 전체 모형(full model)의 순서로 넣어준다.","categories":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/categories/Math/"},{"name":"Regression","slug":"Math/Regression","permalink":"https://p829911.github.io/categories/Math/Regression/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://p829911.github.io/tags/Math/"},{"name":"Regression","slug":"Regression","permalink":"https://p829911.github.io/tags/Regression/"}]},{"title":"Jupyber notebook matplotlib 한글 설정","slug":"Jupyber-notebook-matplotlib-한글-설정","date":"2018-11-26T13:25:40.000Z","updated":"2018-11-26T14:52:32.000Z","comments":true,"path":"2018/11/26/Jupyber-notebook-matplotlib-한글-설정/","link":"","permalink":"https://p829911.github.io/2018/11/26/Jupyber-notebook-matplotlib-한글-설정/","excerpt":"","text":"우분투 폰트 경로 /usr/share/fonts/ 나눔 글꼴 또는 다른 폰트도 /usr/share/fonts/ 폴더에 복사에서 사용가능하다. 나눔글꼴 설치 12sudo apt-get install fonts-nanum*sudo fc-cache -fv apt-get 명령으로 나눔글꼴 설치 후, fc-cache 명령으로 폰트 캐시 삭제 다른 ttf 폰트 12sudo cp new_font.ttf / usr/share/fontssudo fc-cache -fv 우분투 폰트 경로로 ttf폰트 복사 후, fc-cache 명령으로 폰트 캐시 삭제 matplotlib 폴더에 글꼴 추가 123sudo cp /usr/share/fonts/truetype/D2Coding/D2* /home/p829911/.local/lib/python3.6/site-packages/matplotlib/mpl-data/rm -rf /home/ubuntu/.cache/matplotlib/* matplotlib 폴더에 글꼴을 복사 한 후 matplotlib의 폰트 캐시를 삭제 12# 캐쉬 디렉토리matplotlib.get_cachedir() 내 컴퓨터에 저장되어 있는 폰트 리스트 가져오기 123456789import matplotlib.font_manager as fmfont_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')# 전체개수print(len(font_list)) # 처음 10개만 출력font_list[:10] 사용가능한 시스템의 TTF 폰트 목록 123456import matplotlib.font_manager as fmfont_list = [(f.name, f.fname) for f in fm.fontManager.ttflist]print(len(font_list))font_list[:10] 내가 원하는 D2Coding 폰트의 저장 위치를 불러오기 123for font in font_list: if \"D2\" in font[0]: print(font) rcParams 를 설정 파일에 직접 적어주기 - 모든 노트북에 공통 적용 font.family: D2Coding 이곳에 폰트를 지정해 주면 노트북을 실행 할 때 바로 로드되도록 설정할 수 있다. 1print(matplotlib.matplotlib_fname()) 1vi /home/p829911/.local/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc matplotlibrc파일에서 font.family를 D2Coding으로 설정해준다. 1234567891011import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinedata = np.random.randint(-100, 100, 50).cumsum()dataplt.plot(range(50), data, 'r')plt.title('가격변동 추이')plt.ylabel('가격')plt.show()","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Jupyter","slug":"Jupyter","permalink":"https://p829911.github.io/tags/Jupyter/"},{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"}]},{"title":"Jupyter notebook 글꼴 설정","slug":"Jupyter-notebook-글꼴-설정","date":"2018-11-26T10:51:23.000Z","updated":"2018-11-26T14:52:46.000Z","comments":true,"path":"2018/11/26/Jupyter-notebook-글꼴-설정/","link":"","permalink":"https://p829911.github.io/2018/11/26/Jupyter-notebook-글꼴-설정/","excerpt":"","text":"D2Coding 설치 12345cd /home/username/.jupytermkdir customcd customtouch custom.cssvi custom.css 만약 custom.css에 쓰기 권한이 없으면 chmod명령으로 파일 권한을 바꿔준다. 1sudo chmod 777 custom.css vi 편집기로 custom.css 파일을 연 후 다음과 같이 설정 해 준다. 1.CodeMirror pre &#123;font-family: D2Coding; font-size: 12pt; line-height: 120%;&#125; jupyter notebook을 실행하면 글꼴이 D2Coding으로 바뀐 것을 볼 수 있다.","categories":[{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/categories/Python/"}],"tags":[{"name":"Jupyter","slug":"Jupyter","permalink":"https://p829911.github.io/tags/Jupyter/"},{"name":"Python","slug":"Python","permalink":"https://p829911.github.io/tags/Python/"}]},{"title":"git blog 관리하기","slug":"git-blog-관리하기","date":"2018-11-26T10:50:56.000Z","updated":"2018-12-13T14:11:23.000Z","comments":true,"path":"2018/11/26/git-blog-관리하기/","link":"","permalink":"https://p829911.github.io/2018/11/26/git-blog-관리하기/","excerpt":"","text":"새 저장소(repository) 만들기Github에서 새 저장소(repository)를 만든다. 이 때 저장소의 이름을 자신의 username뒤에 .github.io가 붙은 이름으로 만든다. 이렇게 만들어 줘야 username.github.io의 도메인으로 접속할 수 있는 블로그가 된다. Hexo 설치하기git과 node.js는 설치돼 있어야 한다. 12sudo apt install npmnpm install -g hexo-cli # 오류 날 시 앞에 sudo를 붙여준다 1hexo -v 위의 명령어로 hexo가 제대로 설치 되었는지 확인 한다. Hexo 설치가 완료되었으면 다음과 같은 명령어를 입력해서 Hexo 디렉토리를 초기화한다. 1hexo init &lt;디렉토리명&gt; 설치가 모두 잘 되었다면 다음 명령어를 입력해서 내장 서버를 돌릴 수 있다. 123hexo server# 서버 실행 후 창 오픈hexo s -o 브라우저에서 http://0.0.0.0:4000/ 으로 접속해서 확인 할 수 있다. deployHexo가 설치된 디렉토리로 가서 _config.yml 파일을 열어 Site, URL, Deployment 항목을 수정해준다. 그리고 정적 파일을 생성한다. 1hexo generate 디플로이를 하기 위해서는 hexo-deployer-git 플러그인이 필요하다. 아래의 명령어를 사용해서 설치한다. 1npm install --save hexo-deployer-git 생성이 잘 되었다면 디플로이 명령어를 사용한다. 1hexo deploy Hueman 테마 적용하기블로그 루트 폴더에서 명령어로 테마를 받는다. 1git clone https://github.com/ppoffice/hexo-theme-hueman.git themes/hueman blog 폴더에 있는 _config.yml에서 Theme부분을 landscape에서 hueman으로 바꿔준다. themes/hueman 폴더에 있는 _config.yml.example을 _config.yml로 바꾼다. 12cd blog/themes/huemanmv _config.yml.example _config.yml 최신 버전을 다운받기 위해 pull해준다. 12cd themes/huemangit pull Hueman 테마의 Insight Search 검색엔진을 사용하기 위해 npm으로 hexo-generator-json-content을 설치한다. 1npm install -S hexo-generator-json-content hueman의 테마는 hueman폴더 안에 있는 _config.yml에서 설정할 수 있다. 포스트 작성하기1hexo new post \"post name\" 그러면 [blogFolder]/source/_posts에 새로운 마크다운 파일이 생성된다.자동으로 제목과 생성날짜가 들어간다. 글을 마크다운 파일로 작성 한 후 12hexo generatehexo deploy or 123hexo generate --deploy# 단축키hexo g -d 테마가 적용 안되는 경우12hexo cleanhexo generate --deploy","categories":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://p829911.github.io/tags/Git/"},{"name":"Blog","slug":"Blog","permalink":"https://p829911.github.io/tags/Blog/"}]},{"title":"우분투 Root 비밀번호 설정","slug":"우분투-Root-비밀번호-설정","date":"2018-11-26T10:46:43.000Z","updated":"2018-11-26T11:07:04.000Z","comments":true,"path":"2018/11/26/우분투-Root-비밀번호-설정/","link":"","permalink":"https://p829911.github.io/2018/11/26/우분투-Root-비밀번호-설정/","excerpt":"","text":"우분투를 설치하면 기본으로 Root 비밀번호가 없는 상태이다. 아래와 같은 방법으로 root 비밀번호를 설정해본다. root 비밀번호 설정 1sudo passwd 위와 같이 비밀번호를 설정 했다면 1su 명령을 통해 root에 로그인 할 수 있다. passwd 1passwd 현재 로그인한 사용자 계정의 비밀번호를 변경할 수 있는 명령어이다. root에서 user 비밀번호 변경 1passwd p829911 root에서 p829911이라는 사용자의 비밀번호를 변경하고 싶을 때 사용하는 명렁어이다.","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"우분투 한글 입력기 설치","slug":"우분투-한글-입력기-설치","date":"2018-11-24T18:14:50.000Z","updated":"2018-11-24T18:51:55.000Z","comments":true,"path":"2018/11/25/우분투-한글-입력기-설치/","link":"","permalink":"https://p829911.github.io/2018/11/25/우분투-한글-입력기-설치/","excerpt":"","text":"한글 설치 fcitx-hangul 설치 1sudo apt-get install fcitx-hangul System Setting (설정) &gt; Language Support(언어 지원) 을 실행해서 설치되지 않은 언어팩 모두 설치한다. 키보드 입력기를 ibus에서 fcitx로 변경한다. 재부팅 시 오른쪽 위에 아래의 첫번째에서 보는 것과 같은 아이콘이 생성된 것을 볼 수 있다. 한영 변환 설정 &gt; 장치 &gt; 키보드 로 들어간 뒤 입력 중의 다음 입력소스로 전환, 이전 입력소스로 전환을 사용 않음으로 바꿔준다. 사용 않음으로 바꿔주기 위해선 클릭 후 backspace를 누르면 된다. 상단 메뉴바 오른쪽의 입력기 선택(위 그림에서 세번째) 후 현재 입력기 설정 클릭 Keyboard-English(US)가 있다면 + 를 눌러 Hangul을 추가한다. (uncheck “Only Show Current Language”). Korean이 아닌 Hangul을 선택한다. 전역 설정 &gt; 단축키 &gt; 입력기 전환에 ‘Shift + Space’를 추가한다. 전역 설정 &gt; 프로그램 윈도우 사이에 상태 공유를 ‘모두’로 바꿔준다","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]},{"title":"우분투 데이터분석 환경설정","slug":"우분투-데이터분석-환경설정","date":"2018-11-24T17:25:25.000Z","updated":"2018-11-24T18:12:21.000Z","comments":true,"path":"2018/11/25/우분투-데이터분석-환경설정/","link":"","permalink":"https://p829911.github.io/2018/11/25/우분투-데이터분석-환경설정/","excerpt":"","text":"우분투 버전: Ubuntu 18.04.1 LTS 이 글은 데이터 분석을 공부하면서 window 사용자가 우분투에 데이터 분석 환경 설정 하며 겪은 시행착오와 그 단계들을 모아둔 글입니다. 오류와 path충돌 때문에 눈물을 머금고 우분투를 4번 정도 다시 깔면서 다음에 다시 설치해야 할 상황이 왔을 때 참고하기 위한 글이고, 우분투로 처음 데이터 분석 환경 설정을 하려고 하는 분들을 위해 정리하는 글입니다. 이 포스트와 다음과 같은 내용이 포함되어 있습니다. 한글 입력기 설치 root 비밀번호 설정 패키지 관리 툴 apt 사용법 슬랙 설치 vim 설치 git 설치 python3 설치 및 기본 설정 python3 데이터 분석 관련 패키지 설치 markdown 편집기 typora 설치 Atom &amp; Atom package 설치 mysql &amp; mysql workbench 설치 AWS(Amazon Web Services) 가입 및 접속 AWS와 파일 주고 받기를 할 수 있는 FileZilla 설치 및 사용법","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://p829911.github.io/tags/Ubuntu/"}]}]}